{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e409d638",
   "metadata": {},
   "source": [
    "# **Preprocessing Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b25a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11975, 1491), (11975, 594), (11975, 856))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from 3 files\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"datasets/Permission Dataset.csv\", delimiter=\";\")\n",
    "df2 = pd.read_csv(\"datasets/Recievers Dataset.csv\", delimiter=\";\")\n",
    "df3 = pd.read_csv(\"datasets/Services Dataset.csv\", delimiter=\";\")\n",
    "\n",
    "df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07d122d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/Permission Dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/Permission Dataset.csv\", delimiter=\";\")\n",
    "\n",
    "# Check the balance of the 'class' column\n",
    "print(\"\\nNumber of instances in each class:\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Plot the balance of the 'class' column\n",
    "plt.figure(figsize=(6,4))\n",
    "df['class'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08404ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9073068893528183\n",
      "Decision Tree Accuracy: 0.9708768267223382\n",
      "Random Forest Accuracy: 0.9708768267223382\n",
      "XGBoost Accuracy: 0.9409185803757829\n",
      "KNN Accuracy: 0.9210855949895616\n",
      "SVM Accuracy: 0.9226513569937369\n",
      "Gradient Boosting Accuracy: 0.9058455114822547\n",
      "AdaBoost Accuracy: 0.8982254697286013\n",
      "LinearSVC Accuracy: 0.9146137787056368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"LinearSVC\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_models(models, X, y):\n",
    "    for name, model in models.items():\n",
    "        model.fit(X, y)\n",
    "        accuracy = model.score(X, y)\n",
    "        print(f\"{name} Accuracy: {accuracy}\")\n",
    "\n",
    "# Call the function\n",
    "evaluate_models(models, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdfa4a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11975, 1491)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f538693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permission.GetuiService.com.glodon.ynjtapp    0\n",
       "permission.COLLECT_METRICS                    0\n",
       "permission.sec.MDM_PHONE_RESTRICTION          0\n",
       "permission.MEDIA_MOUNTED                      0\n",
       "permission.USAGE_ACCESS_SETTINGS              0\n",
       "                                             ..\n",
       "permission.GetuiService.com.hooray.snm        0\n",
       "permission.READ_USAGESTATS                    0\n",
       "permission.USE_IFAA_MANAGER                   0\n",
       "permission.READ_MEDIA_STORAGE                 0\n",
       "class                                         0\n",
       "Length: 1491, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb0dc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Separate the features and the target variable\n",
    "Xx = df.drop('class', axis=1)\n",
    "yy = df['class']\n",
    "\n",
    "\n",
    "\n",
    "# Perform feature importance\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(Xx, yy)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Get the top 50% important features\n",
    "num_features = int(len(Xx.columns) * 0.5)\n",
    "important_features = Xx.columns[importances.argsort()[-num_features:][::-1]]\n",
    "\n",
    "# Select important features from the original dataset\n",
    "df_selected = df[important_features.tolist() + ['class']]\n",
    "\n",
    "# Save selected features to a new CSV file\n",
    "df_selected.to_csv('selected_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed2da716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Feature  Importance\n",
      "0             permission.READ_PHONE_STATE    0.076616\n",
      "1    permission.MOUNT_UNMOUNT_FILESYSTEMS    0.063153\n",
      "2                      permission.RECEIVE    0.056047\n",
      "3          permission.SYSTEM_ALERT_WINDOW    0.052092\n",
      "4                    permission.GET_TASKS    0.047943\n",
      "..                                    ...         ...\n",
      "740      permission..ACCESS_FINE_LOCATION    0.000005\n",
      "741             permission.ENTERPRISE_API    0.000005\n",
      "742       permission.CHECK_NUMBER_BLOCKED    0.000005\n",
      "743              permission.READ_PROVIDER    0.000005\n",
      "744      permission.RRAD_EXTERNAL_STORAGE    0.000005\n",
      "\n",
      "[745 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the top 50% important features\n",
    "num_features = int(len(X.columns) * 0.5)\n",
    "important_features = X.columns[importances.argsort()[-num_features:][::-1]]\n",
    "important_features_importance = importances[importances.argsort()[-num_features:][::-1]]\n",
    "\n",
    "# Create a DataFrame of selected features and their importance\n",
    "df_importance = pd.DataFrame({'Feature': important_features, 'Importance': important_features_importance})\n",
    "# Save the DataFrame to a CSV file\n",
    "df_importance.to_csv('feature_importance.csv', index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7070b",
   "metadata": {},
   "source": [
    "## **Normalizing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5baec659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "dfsel = pd.read_csv(\"datasets/selected_features.csv\")\n",
    "# Separate the features and the target variable\n",
    "X = dfsel.drop('class', axis=1)\n",
    "y = dfsel['class']\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Fit and transform the features using MinMaxScaler\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "# If necessary, reshape the features for 3D input\n",
    "X_reshaped = np.reshape(X_scaled,(X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Convert the target variable into numpy array\n",
    "y_array = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded4b24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11975, 1, 745), (11975,), dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshaped.shape , y_array.shape , X_reshaped.dtype , y_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fd51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_reshaped.shape[1], X_reshaped.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b8c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 745)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshaped.shape[1], X_reshaped.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d347c",
   "metadata": {},
   "source": [
    "# **Arsitektur Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d388427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 09:43:48.120165: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD\n",
    "from keras_radam import RAdam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas as pd\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(512, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(GRU(512, return_sequences=True, dropout=0.2))\n",
    "    model.add(GRU(256, return_sequences=True, dropout=0.2))\n",
    "    model.add(GRU(128, return_sequences=False, dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3047d",
   "metadata": {},
   "source": [
    "# **Training Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36319e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070b5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 09:43:50.472828: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-15 09:43:50.838197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7910 MB memory:  -> device: 0, name: A100-SXM4-40GB MIG 2g.10gb, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Define optimizers\n",
    "from keras_radam import RAdam\n",
    "\n",
    "# Define optimizers\n",
    "optimizers = [\n",
    "    ('radam', RAdam(learning_rate=0.0001)),\n",
    "    ('adamax', tf.keras.optimizers.Adamax(learning_rate=0.0001)),\n",
    "    ('adam', tf.keras.optimizers.Adam(learning_rate=0.0001)),\n",
    "    ('sgd', tf.keras.optimizers.SGD(learning_rate=0.001))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a82f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "# Define optimizers\n",
    "optimizers = [\n",
    "    ('radam', RAdamOptimizer(learning_rate=0.0001)),\n",
    "    ('adamax', tf.keras.optimizers.Adamax(learning_rate=0.0001)),\n",
    "    ('adam', tf.keras.optimizers.Adam(learning_rate=0.0001)),\n",
    "    ('sgd', tf.keras.optimizers.SGD(learning_rate=0.001))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36fab90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Jumlah fold yang diinginkan\n",
    "num_folds = 10\n",
    "\n",
    "# Inisialisasi KFold dengan jumlah fold yang diinginkan\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db84077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e55fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "# Define DataFrame to store performance of each model\n",
    "performance_df = pd.DataFrame(columns=['optimizer', 'fold', 'val_accuracy', 'val_loss', 'precision', 'recall', 'f1_score', 'specificity', 'tp', 'tn', 'fp', 'fn', 'training_time', 'training_accuracy', 'training_loss'])\n",
    "\n",
    "histories = []\n",
    "models = []\n",
    "\n",
    "\n",
    "# Define variables to store average values\n",
    "avg_tp = 0\n",
    "avg_tn = 0\n",
    "avg_fp = 0\n",
    "avg_fn = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acab7674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 09:46:36.239819: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-15 09:46:36.411808: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - ETA: 0s - loss: 0.6649 - accuracy: 0.7370\n",
      "Epoch 1: val_loss improved from inf to 0.54867, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 8s 13ms/step - loss: 0.6649 - accuracy: 0.7370 - val_loss: 0.5487 - val_accuracy: 0.8280\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3772 - accuracy: 0.8673\n",
      "Epoch 2: val_loss improved from 0.54867 to 0.26244, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.3772 - accuracy: 0.8673 - val_loss: 0.2624 - val_accuracy: 0.9032\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8940\n",
      "Epoch 3: val_loss improved from 0.26244 to 0.23777, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2779 - accuracy: 0.8940 - val_loss: 0.2378 - val_accuracy: 0.9182\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9000\n",
      "Epoch 4: val_loss improved from 0.23777 to 0.23103, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2630 - accuracy: 0.9000 - val_loss: 0.2310 - val_accuracy: 0.9232\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9019\n",
      "Epoch 5: val_loss improved from 0.23103 to 0.22432, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2579 - accuracy: 0.9019 - val_loss: 0.2243 - val_accuracy: 0.9207\n",
      "Epoch 6/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2531 - accuracy: 0.9031\n",
      "Epoch 6: val_loss improved from 0.22432 to 0.22013, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2528 - accuracy: 0.9031 - val_loss: 0.2201 - val_accuracy: 0.9232\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9043\n",
      "Epoch 7: val_loss improved from 0.22013 to 0.21483, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2498 - accuracy: 0.9043 - val_loss: 0.2148 - val_accuracy: 0.9224\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9044\n",
      "Epoch 8: val_loss did not improve from 0.21483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2471 - accuracy: 0.9044 - val_loss: 0.2198 - val_accuracy: 0.9190\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9057\n",
      "Epoch 9: val_loss improved from 0.21483 to 0.21068, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2459 - accuracy: 0.9057 - val_loss: 0.2107 - val_accuracy: 0.9257\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9051\n",
      "Epoch 10: val_loss did not improve from 0.21068\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2421 - accuracy: 0.9051 - val_loss: 0.2114 - val_accuracy: 0.9232\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9073\n",
      "Epoch 11: val_loss improved from 0.21068 to 0.20877, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2410 - accuracy: 0.9073 - val_loss: 0.2088 - val_accuracy: 0.9224\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9085\n",
      "Epoch 12: val_loss improved from 0.20877 to 0.20796, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2387 - accuracy: 0.9082 - val_loss: 0.2080 - val_accuracy: 0.9257\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.9083\n",
      "Epoch 13: val_loss improved from 0.20796 to 0.20560, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2370 - accuracy: 0.9083 - val_loss: 0.2056 - val_accuracy: 0.9265\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9095\n",
      "Epoch 14: val_loss improved from 0.20560 to 0.20483, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2344 - accuracy: 0.9095 - val_loss: 0.2048 - val_accuracy: 0.9265\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9094\n",
      "Epoch 15: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2341 - accuracy: 0.9094 - val_loss: 0.2051 - val_accuracy: 0.9290\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9097\n",
      "Epoch 16: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2316 - accuracy: 0.9097 - val_loss: 0.2064 - val_accuracy: 0.9240\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9119\n",
      "Epoch 17: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2283 - accuracy: 0.9119 - val_loss: 0.2091 - val_accuracy: 0.9224\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9113\n",
      "Epoch 18: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2283 - accuracy: 0.9113 - val_loss: 0.2074 - val_accuracy: 0.9265\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9127\n",
      "Epoch 19: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2265 - accuracy: 0.9127 - val_loss: 0.2065 - val_accuracy: 0.9265\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9123\n",
      "Epoch 20: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2236 - accuracy: 0.9123 - val_loss: 0.2070 - val_accuracy: 0.9265\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9117\n",
      "Epoch 21: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2237 - accuracy: 0.9117 - val_loss: 0.2124 - val_accuracy: 0.9190\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9134\n",
      "Epoch 22: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2202 - accuracy: 0.9134 - val_loss: 0.2058 - val_accuracy: 0.9290\n",
      "Epoch 23/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2196 - accuracy: 0.9148\n",
      "Epoch 23: val_loss did not improve from 0.20483\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2191 - accuracy: 0.9146 - val_loss: 0.2059 - val_accuracy: 0.9257\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9164\n",
      "Epoch 24: val_loss improved from 0.20483 to 0.20472, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2172 - accuracy: 0.9164 - val_loss: 0.2047 - val_accuracy: 0.9274\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9157\n",
      "Epoch 25: val_loss did not improve from 0.20472\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2173 - accuracy: 0.9157 - val_loss: 0.2112 - val_accuracy: 0.9215\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9170\n",
      "Epoch 26: val_loss did not improve from 0.20472\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2135 - accuracy: 0.9170 - val_loss: 0.2054 - val_accuracy: 0.9265\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9161\n",
      "Epoch 27: val_loss did not improve from 0.20472\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2149 - accuracy: 0.9161 - val_loss: 0.2071 - val_accuracy: 0.9299\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9162\n",
      "Epoch 28: val_loss did not improve from 0.20472\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2130 - accuracy: 0.9162 - val_loss: 0.2179 - val_accuracy: 0.9182\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9153\n",
      "Epoch 29: val_loss did not improve from 0.20472\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2123 - accuracy: 0.9153 - val_loss: 0.2068 - val_accuracy: 0.9249\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.9190\n",
      "Epoch 30: val_loss improved from 0.20472 to 0.20391, saving model to model_radam_fold1.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2105 - accuracy: 0.9190 - val_loss: 0.2039 - val_accuracy: 0.9274\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 1 - TP: 576, TN: 535, FP: 26, FN: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8746\n",
      "Epoch 1: val_loss improved from inf to 0.23688, saving model to model_radam_fold2.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 14ms/step - loss: 0.3083 - accuracy: 0.8745 - val_loss: 0.2369 - val_accuracy: 0.9165\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9037\n",
      "Epoch 2: val_loss improved from 0.23688 to 0.23675, saving model to model_radam_fold2.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2524 - accuracy: 0.9037 - val_loss: 0.2367 - val_accuracy: 0.9157\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9043\n",
      "Epoch 3: val_loss improved from 0.23675 to 0.23481, saving model to model_radam_fold2.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2447 - accuracy: 0.9043 - val_loss: 0.2348 - val_accuracy: 0.9124\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9073\n",
      "Epoch 4: val_loss did not improve from 0.23481\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2406 - accuracy: 0.9073 - val_loss: 0.2372 - val_accuracy: 0.9132\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9094\n",
      "Epoch 5: val_loss did not improve from 0.23481\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2369 - accuracy: 0.9094 - val_loss: 0.2352 - val_accuracy: 0.9149\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9096\n",
      "Epoch 6: val_loss did not improve from 0.23481\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2335 - accuracy: 0.9096 - val_loss: 0.2357 - val_accuracy: 0.9132\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9107\n",
      "Epoch 7: val_loss improved from 0.23481 to 0.23399, saving model to model_radam_fold2.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2303 - accuracy: 0.9107 - val_loss: 0.2340 - val_accuracy: 0.9157\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9118\n",
      "Epoch 8: val_loss did not improve from 0.23399\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2276 - accuracy: 0.9118 - val_loss: 0.2358 - val_accuracy: 0.9098\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9138\n",
      "Epoch 9: val_loss improved from 0.23399 to 0.23334, saving model to model_radam_fold2.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2256 - accuracy: 0.9138 - val_loss: 0.2333 - val_accuracy: 0.9149\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9135\n",
      "Epoch 10: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2224 - accuracy: 0.9135 - val_loss: 0.2338 - val_accuracy: 0.9190\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9158\n",
      "Epoch 11: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2201 - accuracy: 0.9158 - val_loss: 0.2337 - val_accuracy: 0.9174\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9163\n",
      "Epoch 12: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2183 - accuracy: 0.9163 - val_loss: 0.2370 - val_accuracy: 0.9115\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9157\n",
      "Epoch 13: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2158 - accuracy: 0.9157 - val_loss: 0.2350 - val_accuracy: 0.9174\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9161\n",
      "Epoch 14: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2140 - accuracy: 0.9161 - val_loss: 0.2403 - val_accuracy: 0.9132\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9175\n",
      "Epoch 15: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2132 - accuracy: 0.9175 - val_loss: 0.2346 - val_accuracy: 0.9157\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9174\n",
      "Epoch 16: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2091 - accuracy: 0.9174 - val_loss: 0.2353 - val_accuracy: 0.9149\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2096 - accuracy: 0.9175\n",
      "Epoch 17: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2096 - accuracy: 0.9175 - val_loss: 0.2383 - val_accuracy: 0.9165\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9195\n",
      "Epoch 18: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2067 - accuracy: 0.9195 - val_loss: 0.2529 - val_accuracy: 0.9098\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9185\n",
      "Epoch 19: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2053 - accuracy: 0.9185 - val_loss: 0.2400 - val_accuracy: 0.9165\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9188\n",
      "Epoch 20: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2056 - accuracy: 0.9188 - val_loss: 0.2400 - val_accuracy: 0.9140\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9189\n",
      "Epoch 21: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2031 - accuracy: 0.9189 - val_loss: 0.2447 - val_accuracy: 0.9149\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9205\n",
      "Epoch 22: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2017 - accuracy: 0.9205 - val_loss: 0.2457 - val_accuracy: 0.9149\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1988 - accuracy: 0.9229\n",
      "Epoch 23: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1988 - accuracy: 0.9229 - val_loss: 0.2436 - val_accuracy: 0.9190\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9220\n",
      "Epoch 24: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1990 - accuracy: 0.9220 - val_loss: 0.2481 - val_accuracy: 0.9115\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9219\n",
      "Epoch 25: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.1982 - accuracy: 0.9219 - val_loss: 0.2518 - val_accuracy: 0.9149\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9218\n",
      "Epoch 26: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1976 - accuracy: 0.9218 - val_loss: 0.2531 - val_accuracy: 0.9165\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9229\n",
      "Epoch 27: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1959 - accuracy: 0.9229 - val_loss: 0.2509 - val_accuracy: 0.9174\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9229\n",
      "Epoch 28: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1955 - accuracy: 0.9229 - val_loss: 0.2529 - val_accuracy: 0.9165\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9247\n",
      "Epoch 29: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1919 - accuracy: 0.9247 - val_loss: 0.2528 - val_accuracy: 0.9132\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9240\n",
      "Epoch 30: val_loss did not improve from 0.23334\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1934 - accuracy: 0.9240 - val_loss: 0.2564 - val_accuracy: 0.9157\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 2 - TP: 556, TN: 541, FP: 41, FN: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.8711\n",
      "Epoch 1: val_loss improved from inf to 0.26263, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3021 - accuracy: 0.8711 - val_loss: 0.2626 - val_accuracy: 0.8948\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9047\n",
      "Epoch 2: val_loss improved from 0.26263 to 0.25501, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2493 - accuracy: 0.9047 - val_loss: 0.2550 - val_accuracy: 0.8957\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9073\n",
      "Epoch 3: val_loss improved from 0.25501 to 0.25361, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2435 - accuracy: 0.9073 - val_loss: 0.2536 - val_accuracy: 0.8965\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9081\n",
      "Epoch 4: val_loss improved from 0.25361 to 0.25288, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2378 - accuracy: 0.9081 - val_loss: 0.2529 - val_accuracy: 0.9015\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2350 - accuracy: 0.9108\n",
      "Epoch 5: val_loss improved from 0.25288 to 0.25197, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2350 - accuracy: 0.9108 - val_loss: 0.2520 - val_accuracy: 0.8998\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2319 - accuracy: 0.9116\n",
      "Epoch 6: val_loss improved from 0.25197 to 0.25013, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2319 - accuracy: 0.9116 - val_loss: 0.2501 - val_accuracy: 0.9023\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9133\n",
      "Epoch 7: val_loss did not improve from 0.25013\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2290 - accuracy: 0.9133 - val_loss: 0.2508 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9128\n",
      "Epoch 8: val_loss improved from 0.25013 to 0.24726, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2267 - accuracy: 0.9128 - val_loss: 0.2473 - val_accuracy: 0.9015\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9140\n",
      "Epoch 9: val_loss improved from 0.24726 to 0.24618, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2224 - accuracy: 0.9140 - val_loss: 0.2462 - val_accuracy: 0.9023\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9159\n",
      "Epoch 10: val_loss improved from 0.24618 to 0.24613, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2200 - accuracy: 0.9159 - val_loss: 0.2461 - val_accuracy: 0.8982\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9140\n",
      "Epoch 11: val_loss improved from 0.24613 to 0.24497, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2186 - accuracy: 0.9140 - val_loss: 0.2450 - val_accuracy: 0.9007\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9167\n",
      "Epoch 12: val_loss did not improve from 0.24497\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2153 - accuracy: 0.9167 - val_loss: 0.2459 - val_accuracy: 0.9015\n",
      "Epoch 13/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.2143 - accuracy: 0.9167\n",
      "Epoch 13: val_loss improved from 0.24497 to 0.24175, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 2s 9ms/step - loss: 0.2144 - accuracy: 0.9165 - val_loss: 0.2418 - val_accuracy: 0.9007\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9172\n",
      "Epoch 14: val_loss did not improve from 0.24175\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2124 - accuracy: 0.9172 - val_loss: 0.2475 - val_accuracy: 0.8982\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9189\n",
      "Epoch 15: val_loss did not improve from 0.24175\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2125 - accuracy: 0.9189 - val_loss: 0.2462 - val_accuracy: 0.8990\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9190\n",
      "Epoch 16: val_loss did not improve from 0.24175\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2089 - accuracy: 0.9190 - val_loss: 0.2418 - val_accuracy: 0.9057\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9206\n",
      "Epoch 17: val_loss did not improve from 0.24175\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2076 - accuracy: 0.9206 - val_loss: 0.2447 - val_accuracy: 0.9032\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9198\n",
      "Epoch 18: val_loss improved from 0.24175 to 0.23930, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2070 - accuracy: 0.9198 - val_loss: 0.2393 - val_accuracy: 0.9048\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9215\n",
      "Epoch 19: val_loss did not improve from 0.23930\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2033 - accuracy: 0.9215 - val_loss: 0.2397 - val_accuracy: 0.9032\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9216\n",
      "Epoch 20: val_loss did not improve from 0.23930\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2029 - accuracy: 0.9216 - val_loss: 0.2396 - val_accuracy: 0.9040\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9226\n",
      "Epoch 21: val_loss did not improve from 0.23930\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2008 - accuracy: 0.9226 - val_loss: 0.2443 - val_accuracy: 0.9023\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9214\n",
      "Epoch 22: val_loss did not improve from 0.23930\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2005 - accuracy: 0.9214 - val_loss: 0.2402 - val_accuracy: 0.9007\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9243\n",
      "Epoch 23: val_loss improved from 0.23930 to 0.23841, saving model to model_radam_fold3.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1984 - accuracy: 0.9243 - val_loss: 0.2384 - val_accuracy: 0.9048\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9233\n",
      "Epoch 24: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1982 - accuracy: 0.9233 - val_loss: 0.2410 - val_accuracy: 0.9040\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9252\n",
      "Epoch 25: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1953 - accuracy: 0.9252 - val_loss: 0.2457 - val_accuracy: 0.9032\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.9235\n",
      "Epoch 26: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1969 - accuracy: 0.9235 - val_loss: 0.2400 - val_accuracy: 0.9040\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9244\n",
      "Epoch 27: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1943 - accuracy: 0.9244 - val_loss: 0.2402 - val_accuracy: 0.9040\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9237\n",
      "Epoch 28: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1935 - accuracy: 0.9237 - val_loss: 0.2472 - val_accuracy: 0.8998\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9239\n",
      "Epoch 29: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1942 - accuracy: 0.9239 - val_loss: 0.2432 - val_accuracy: 0.9082\n",
      "Epoch 30/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.9246\n",
      "Epoch 30: val_loss did not improve from 0.23841\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1927 - accuracy: 0.9242 - val_loss: 0.2461 - val_accuracy: 0.8990\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 3 - TP: 531, TN: 546, FP: 35, FN: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.8655\n",
      "Epoch 1: val_loss improved from inf to 0.24728, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3081 - accuracy: 0.8655 - val_loss: 0.2473 - val_accuracy: 0.9124\n",
      "Epoch 2/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9036\n",
      "Epoch 2: val_loss improved from 0.24728 to 0.24073, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2527 - accuracy: 0.9036 - val_loss: 0.2407 - val_accuracy: 0.9140\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9025\n",
      "Epoch 3: val_loss improved from 0.24073 to 0.23894, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2474 - accuracy: 0.9025 - val_loss: 0.2389 - val_accuracy: 0.9157\n",
      "Epoch 4/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2401 - accuracy: 0.9061\n",
      "Epoch 4: val_loss did not improve from 0.23894\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2408 - accuracy: 0.9057 - val_loss: 0.2390 - val_accuracy: 0.9165\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.9084\n",
      "Epoch 5: val_loss did not improve from 0.23894\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2374 - accuracy: 0.9084 - val_loss: 0.2398 - val_accuracy: 0.9132\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9083\n",
      "Epoch 6: val_loss improved from 0.23894 to 0.23605, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2345 - accuracy: 0.9083 - val_loss: 0.2361 - val_accuracy: 0.9215\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9094\n",
      "Epoch 7: val_loss improved from 0.23605 to 0.23487, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2315 - accuracy: 0.9094 - val_loss: 0.2349 - val_accuracy: 0.9157\n",
      "Epoch 8/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2268 - accuracy: 0.9127\n",
      "Epoch 8: val_loss improved from 0.23487 to 0.23155, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2298 - accuracy: 0.9113 - val_loss: 0.2315 - val_accuracy: 0.9215\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9109\n",
      "Epoch 9: val_loss improved from 0.23155 to 0.22917, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2265 - accuracy: 0.9109 - val_loss: 0.2292 - val_accuracy: 0.9207\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9131\n",
      "Epoch 10: val_loss did not improve from 0.22917\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2250 - accuracy: 0.9131 - val_loss: 0.2300 - val_accuracy: 0.9215\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9120\n",
      "Epoch 11: val_loss improved from 0.22917 to 0.22896, saving model to model_radam_fold4.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2230 - accuracy: 0.9120 - val_loss: 0.2290 - val_accuracy: 0.9199\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9156\n",
      "Epoch 12: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2204 - accuracy: 0.9156 - val_loss: 0.2320 - val_accuracy: 0.9182\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9160\n",
      "Epoch 13: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2167 - accuracy: 0.9160 - val_loss: 0.2327 - val_accuracy: 0.9224\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9167\n",
      "Epoch 14: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2146 - accuracy: 0.9167 - val_loss: 0.2311 - val_accuracy: 0.9207\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9168\n",
      "Epoch 15: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2136 - accuracy: 0.9168 - val_loss: 0.2311 - val_accuracy: 0.9240\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9175\n",
      "Epoch 16: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2126 - accuracy: 0.9175 - val_loss: 0.2310 - val_accuracy: 0.9224\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9169\n",
      "Epoch 17: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2116 - accuracy: 0.9169 - val_loss: 0.2329 - val_accuracy: 0.9207\n",
      "Epoch 18/30\n",
      "162/169 [===========================>..] - ETA: 0s - loss: 0.2082 - accuracy: 0.9178\n",
      "Epoch 18: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2088 - accuracy: 0.9176 - val_loss: 0.2342 - val_accuracy: 0.9224\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9184\n",
      "Epoch 19: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2085 - accuracy: 0.9184 - val_loss: 0.2389 - val_accuracy: 0.9199\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9176\n",
      "Epoch 20: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2080 - accuracy: 0.9176 - val_loss: 0.2303 - val_accuracy: 0.9240\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.9198\n",
      "Epoch 21: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2034 - accuracy: 0.9198 - val_loss: 0.2349 - val_accuracy: 0.9257\n",
      "Epoch 22/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9194\n",
      "Epoch 22: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2036 - accuracy: 0.9193 - val_loss: 0.2324 - val_accuracy: 0.9257\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9190\n",
      "Epoch 23: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2022 - accuracy: 0.9190 - val_loss: 0.2351 - val_accuracy: 0.9232\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9217\n",
      "Epoch 24: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2006 - accuracy: 0.9217 - val_loss: 0.2422 - val_accuracy: 0.9199\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9196\n",
      "Epoch 25: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2012 - accuracy: 0.9196 - val_loss: 0.2309 - val_accuracy: 0.9274\n",
      "Epoch 26/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1978 - accuracy: 0.9231\n",
      "Epoch 26: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.1986 - accuracy: 0.9228 - val_loss: 0.2311 - val_accuracy: 0.9249\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9215\n",
      "Epoch 27: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1983 - accuracy: 0.9215 - val_loss: 0.2388 - val_accuracy: 0.9232\n",
      "Epoch 28/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1992 - accuracy: 0.9203\n",
      "Epoch 28: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1979 - accuracy: 0.9214 - val_loss: 0.2367 - val_accuracy: 0.9240\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9219\n",
      "Epoch 29: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1953 - accuracy: 0.9219 - val_loss: 0.2347 - val_accuracy: 0.9207\n",
      "Epoch 30/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1959 - accuracy: 0.9205\n",
      "Epoch 30: val_loss did not improve from 0.22896\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1956 - accuracy: 0.9211 - val_loss: 0.2383 - val_accuracy: 0.9207\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 4 - TP: 510, TN: 593, FP: 22, FN: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8813\n",
      "Epoch 1: val_loss improved from inf to 0.26630, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3002 - accuracy: 0.8813 - val_loss: 0.2663 - val_accuracy: 0.8982\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9043\n",
      "Epoch 2: val_loss improved from 0.26630 to 0.26112, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2500 - accuracy: 0.9043 - val_loss: 0.2611 - val_accuracy: 0.8982\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.9063\n",
      "Epoch 3: val_loss did not improve from 0.26112\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2420 - accuracy: 0.9063 - val_loss: 0.2624 - val_accuracy: 0.8998\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9093\n",
      "Epoch 4: val_loss improved from 0.26112 to 0.26025, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2384 - accuracy: 0.9093 - val_loss: 0.2602 - val_accuracy: 0.8990\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9098\n",
      "Epoch 5: val_loss improved from 0.26025 to 0.25748, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2341 - accuracy: 0.9098 - val_loss: 0.2575 - val_accuracy: 0.8973\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9112\n",
      "Epoch 6: val_loss did not improve from 0.25748\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2314 - accuracy: 0.9112 - val_loss: 0.2575 - val_accuracy: 0.8982\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9130\n",
      "Epoch 7: val_loss did not improve from 0.25748\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2289 - accuracy: 0.9130 - val_loss: 0.2604 - val_accuracy: 0.8982\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9141\n",
      "Epoch 8: val_loss did not improve from 0.25748\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2255 - accuracy: 0.9141 - val_loss: 0.2593 - val_accuracy: 0.8957\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9152\n",
      "Epoch 9: val_loss did not improve from 0.25748\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2220 - accuracy: 0.9152 - val_loss: 0.2581 - val_accuracy: 0.8990\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9134\n",
      "Epoch 10: val_loss did not improve from 0.25748\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2216 - accuracy: 0.9134 - val_loss: 0.2600 - val_accuracy: 0.8965\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2187 - accuracy: 0.9155\n",
      "Epoch 11: val_loss improved from 0.25748 to 0.25493, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2187 - accuracy: 0.9155 - val_loss: 0.2549 - val_accuracy: 0.9007\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9171\n",
      "Epoch 12: val_loss did not improve from 0.25493\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2158 - accuracy: 0.9171 - val_loss: 0.2551 - val_accuracy: 0.9015\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9175\n",
      "Epoch 13: val_loss improved from 0.25493 to 0.25300, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2124 - accuracy: 0.9175 - val_loss: 0.2530 - val_accuracy: 0.9040\n",
      "Epoch 14/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9196\n",
      "Epoch 14: val_loss did not improve from 0.25300\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2112 - accuracy: 0.9196 - val_loss: 0.2572 - val_accuracy: 0.8973\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9186\n",
      "Epoch 15: val_loss did not improve from 0.25300\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2126 - accuracy: 0.9186 - val_loss: 0.2579 - val_accuracy: 0.9023\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9203\n",
      "Epoch 16: val_loss did not improve from 0.25300\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2089 - accuracy: 0.9203 - val_loss: 0.2540 - val_accuracy: 0.9007\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9196\n",
      "Epoch 17: val_loss did not improve from 0.25300\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2058 - accuracy: 0.9196 - val_loss: 0.2570 - val_accuracy: 0.8998\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9200\n",
      "Epoch 18: val_loss improved from 0.25300 to 0.25280, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2070 - accuracy: 0.9200 - val_loss: 0.2528 - val_accuracy: 0.9015\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9218\n",
      "Epoch 19: val_loss did not improve from 0.25280\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2047 - accuracy: 0.9218 - val_loss: 0.2547 - val_accuracy: 0.9015\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9216\n",
      "Epoch 20: val_loss improved from 0.25280 to 0.24902, saving model to model_radam_fold5.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2022 - accuracy: 0.9216 - val_loss: 0.2490 - val_accuracy: 0.9023\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9229\n",
      "Epoch 21: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1992 - accuracy: 0.9229 - val_loss: 0.2529 - val_accuracy: 0.9023\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9221\n",
      "Epoch 22: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2007 - accuracy: 0.9221 - val_loss: 0.2568 - val_accuracy: 0.9023\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9221\n",
      "Epoch 23: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1997 - accuracy: 0.9221 - val_loss: 0.2581 - val_accuracy: 0.9015\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9236\n",
      "Epoch 24: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1974 - accuracy: 0.9236 - val_loss: 0.2539 - val_accuracy: 0.9032\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9246\n",
      "Epoch 25: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1970 - accuracy: 0.9246 - val_loss: 0.2554 - val_accuracy: 0.9040\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9232\n",
      "Epoch 26: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1951 - accuracy: 0.9232 - val_loss: 0.2572 - val_accuracy: 0.9015\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9247\n",
      "Epoch 27: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1957 - accuracy: 0.9247 - val_loss: 0.2653 - val_accuracy: 0.9032\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9235\n",
      "Epoch 28: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9235 - val_loss: 0.2652 - val_accuracy: 0.9032\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9246\n",
      "Epoch 29: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1939 - accuracy: 0.9246 - val_loss: 0.2805 - val_accuracy: 0.8982\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9257\n",
      "Epoch 30: val_loss did not improve from 0.24902\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1920 - accuracy: 0.9257 - val_loss: 0.2679 - val_accuracy: 0.8990\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 5 - TP: 509, TN: 568, FP: 54, FN: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.8738\n",
      "Epoch 1: val_loss improved from inf to 0.31804, saving model to model_radam_fold6.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3020 - accuracy: 0.8738 - val_loss: 0.3180 - val_accuracy: 0.8822\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9054\n",
      "Epoch 2: val_loss improved from 0.31804 to 0.30983, saving model to model_radam_fold6.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2448 - accuracy: 0.9054 - val_loss: 0.3098 - val_accuracy: 0.8839\n",
      "Epoch 3/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2386 - accuracy: 0.9099\n",
      "Epoch 3: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2388 - accuracy: 0.9096 - val_loss: 0.3169 - val_accuracy: 0.8839\n",
      "Epoch 4/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2330 - accuracy: 0.9109\n",
      "Epoch 4: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2327 - accuracy: 0.9107 - val_loss: 0.3156 - val_accuracy: 0.8847\n",
      "Epoch 5/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2311 - accuracy: 0.9105\n",
      "Epoch 5: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2297 - accuracy: 0.9108 - val_loss: 0.3166 - val_accuracy: 0.8805\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9128\n",
      "Epoch 6: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2261 - accuracy: 0.9128 - val_loss: 0.3128 - val_accuracy: 0.8830\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9131\n",
      "Epoch 7: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2232 - accuracy: 0.9131 - val_loss: 0.3101 - val_accuracy: 0.8881\n",
      "Epoch 8/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2200 - accuracy: 0.9131\n",
      "Epoch 8: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2217 - accuracy: 0.9127 - val_loss: 0.3111 - val_accuracy: 0.8822\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9171\n",
      "Epoch 9: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2175 - accuracy: 0.9171 - val_loss: 0.3160 - val_accuracy: 0.8814\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9181\n",
      "Epoch 10: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2148 - accuracy: 0.9181 - val_loss: 0.3123 - val_accuracy: 0.8922\n",
      "Epoch 11/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2145 - accuracy: 0.9163\n",
      "Epoch 11: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2140 - accuracy: 0.9165 - val_loss: 0.3188 - val_accuracy: 0.8830\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9178\n",
      "Epoch 12: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2117 - accuracy: 0.9178 - val_loss: 0.3220 - val_accuracy: 0.8864\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9171\n",
      "Epoch 13: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2086 - accuracy: 0.9171 - val_loss: 0.3146 - val_accuracy: 0.8881\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2083 - accuracy: 0.9171\n",
      "Epoch 14: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2083 - accuracy: 0.9171 - val_loss: 0.3245 - val_accuracy: 0.8889\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9198\n",
      "Epoch 15: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2048 - accuracy: 0.9198 - val_loss: 0.3309 - val_accuracy: 0.8872\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2038 - accuracy: 0.9197\n",
      "Epoch 16: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2038 - accuracy: 0.9197 - val_loss: 0.3362 - val_accuracy: 0.8830\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9206\n",
      "Epoch 17: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2015 - accuracy: 0.9206 - val_loss: 0.3248 - val_accuracy: 0.8881\n",
      "Epoch 18/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2000 - accuracy: 0.9196\n",
      "Epoch 18: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2015 - accuracy: 0.9192 - val_loss: 0.3302 - val_accuracy: 0.8881\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9202\n",
      "Epoch 19: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2007 - accuracy: 0.9202 - val_loss: 0.3296 - val_accuracy: 0.8922\n",
      "Epoch 20/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1964 - accuracy: 0.9218\n",
      "Epoch 20: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1978 - accuracy: 0.9212 - val_loss: 0.3312 - val_accuracy: 0.8889\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9201\n",
      "Epoch 21: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1961 - accuracy: 0.9201 - val_loss: 0.3368 - val_accuracy: 0.8939\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9225\n",
      "Epoch 22: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1938 - accuracy: 0.9225 - val_loss: 0.3347 - val_accuracy: 0.8939\n",
      "Epoch 23/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1943 - accuracy: 0.9230\n",
      "Epoch 23: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1949 - accuracy: 0.9224 - val_loss: 0.3339 - val_accuracy: 0.8931\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9248\n",
      "Epoch 24: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1909 - accuracy: 0.9248 - val_loss: 0.3295 - val_accuracy: 0.8939\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9239\n",
      "Epoch 25: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1905 - accuracy: 0.9239 - val_loss: 0.3428 - val_accuracy: 0.8897\n",
      "Epoch 26/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.1887 - accuracy: 0.9243\n",
      "Epoch 26: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1900 - accuracy: 0.9237 - val_loss: 0.3455 - val_accuracy: 0.8897\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9251\n",
      "Epoch 27: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1916 - accuracy: 0.9251 - val_loss: 0.3462 - val_accuracy: 0.8947\n",
      "Epoch 28/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9249\n",
      "Epoch 28: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.1877 - accuracy: 0.9247 - val_loss: 0.3654 - val_accuracy: 0.8872\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9261\n",
      "Epoch 29: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1870 - accuracy: 0.9261 - val_loss: 0.3456 - val_accuracy: 0.8906\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9245\n",
      "Epoch 30: val_loss did not improve from 0.30983\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1878 - accuracy: 0.9245 - val_loss: 0.3567 - val_accuracy: 0.8922\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 6 - TP: 490, TN: 578, FP: 49, FN: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.8773\n",
      "Epoch 1: val_loss improved from inf to 0.26674, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3032 - accuracy: 0.8773 - val_loss: 0.2667 - val_accuracy: 0.8964\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9057\n",
      "Epoch 2: val_loss improved from 0.26674 to 0.25945, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2503 - accuracy: 0.9057 - val_loss: 0.2595 - val_accuracy: 0.8981\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9079\n",
      "Epoch 3: val_loss improved from 0.25945 to 0.25889, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2441 - accuracy: 0.9079 - val_loss: 0.2589 - val_accuracy: 0.8981\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9085\n",
      "Epoch 4: val_loss improved from 0.25889 to 0.25570, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2405 - accuracy: 0.9085 - val_loss: 0.2557 - val_accuracy: 0.8939\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2361 - accuracy: 0.9106\n",
      "Epoch 5: val_loss improved from 0.25570 to 0.25451, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2361 - accuracy: 0.9106 - val_loss: 0.2545 - val_accuracy: 0.8989\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9114\n",
      "Epoch 6: val_loss improved from 0.25451 to 0.24855, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2317 - accuracy: 0.9114 - val_loss: 0.2486 - val_accuracy: 0.8997\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9124\n",
      "Epoch 7: val_loss improved from 0.24855 to 0.24723, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2300 - accuracy: 0.9124 - val_loss: 0.2472 - val_accuracy: 0.9014\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9131\n",
      "Epoch 8: val_loss did not improve from 0.24723\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2269 - accuracy: 0.9131 - val_loss: 0.2476 - val_accuracy: 0.9039\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9132\n",
      "Epoch 9: val_loss improved from 0.24723 to 0.24722, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2244 - accuracy: 0.9132 - val_loss: 0.2472 - val_accuracy: 0.9006\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9134\n",
      "Epoch 10: val_loss improved from 0.24722 to 0.24556, saving model to model_radam_fold7.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2217 - accuracy: 0.9134 - val_loss: 0.2456 - val_accuracy: 0.9039\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9160\n",
      "Epoch 11: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2192 - accuracy: 0.9160 - val_loss: 0.2468 - val_accuracy: 0.8981\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9154\n",
      "Epoch 12: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2182 - accuracy: 0.9154 - val_loss: 0.2552 - val_accuracy: 0.9031\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9186\n",
      "Epoch 13: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2148 - accuracy: 0.9186 - val_loss: 0.2650 - val_accuracy: 0.8981\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9182\n",
      "Epoch 14: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2138 - accuracy: 0.9182 - val_loss: 0.2468 - val_accuracy: 0.9039\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9189\n",
      "Epoch 15: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2109 - accuracy: 0.9189 - val_loss: 0.2470 - val_accuracy: 0.9031\n",
      "Epoch 16/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9190\n",
      "Epoch 16: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2090 - accuracy: 0.9191 - val_loss: 0.2500 - val_accuracy: 0.9031\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9183\n",
      "Epoch 17: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2090 - accuracy: 0.9183 - val_loss: 0.2496 - val_accuracy: 0.9014\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9205\n",
      "Epoch 18: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2064 - accuracy: 0.9205 - val_loss: 0.2493 - val_accuracy: 0.9064\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9190\n",
      "Epoch 19: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2061 - accuracy: 0.9190 - val_loss: 0.2522 - val_accuracy: 0.9031\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9189\n",
      "Epoch 20: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2064 - accuracy: 0.9189 - val_loss: 0.2504 - val_accuracy: 0.9006\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2023 - accuracy: 0.9222\n",
      "Epoch 21: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2023 - accuracy: 0.9222 - val_loss: 0.2525 - val_accuracy: 0.9039\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9208\n",
      "Epoch 22: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2009 - accuracy: 0.9208 - val_loss: 0.2532 - val_accuracy: 0.9006\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9228\n",
      "Epoch 23: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1986 - accuracy: 0.9228 - val_loss: 0.2535 - val_accuracy: 0.9014\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9231\n",
      "Epoch 24: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1983 - accuracy: 0.9231 - val_loss: 0.2533 - val_accuracy: 0.8997\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9208\n",
      "Epoch 25: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1986 - accuracy: 0.9208 - val_loss: 0.2499 - val_accuracy: 0.9023\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9210\n",
      "Epoch 26: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1974 - accuracy: 0.9210 - val_loss: 0.2521 - val_accuracy: 0.8997\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9224\n",
      "Epoch 27: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1968 - accuracy: 0.9224 - val_loss: 0.2594 - val_accuracy: 0.9014\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9216\n",
      "Epoch 28: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1958 - accuracy: 0.9216 - val_loss: 0.2575 - val_accuracy: 0.9006\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9231\n",
      "Epoch 29: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1933 - accuracy: 0.9231 - val_loss: 0.2599 - val_accuracy: 0.9031\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9230\n",
      "Epoch 30: val_loss did not improve from 0.24556\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1920 - accuracy: 0.9230 - val_loss: 0.2621 - val_accuracy: 0.9073\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 7 - TP: 523, TN: 563, FP: 34, FN: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.8743\n",
      "Epoch 1: val_loss improved from inf to 0.26294, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3025 - accuracy: 0.8743 - val_loss: 0.2629 - val_accuracy: 0.8972\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9034\n",
      "Epoch 2: val_loss did not improve from 0.26294\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2491 - accuracy: 0.9034 - val_loss: 0.2644 - val_accuracy: 0.9014\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9057\n",
      "Epoch 3: val_loss improved from 0.26294 to 0.25432, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2433 - accuracy: 0.9057 - val_loss: 0.2543 - val_accuracy: 0.9081\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9078\n",
      "Epoch 4: val_loss improved from 0.25432 to 0.25262, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2387 - accuracy: 0.9078 - val_loss: 0.2526 - val_accuracy: 0.9048\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9085\n",
      "Epoch 5: val_loss improved from 0.25262 to 0.25151, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2362 - accuracy: 0.9085 - val_loss: 0.2515 - val_accuracy: 0.9031\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9130\n",
      "Epoch 6: val_loss did not improve from 0.25151\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2323 - accuracy: 0.9130 - val_loss: 0.2581 - val_accuracy: 0.9039\n",
      "Epoch 7/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9113\n",
      "Epoch 7: val_loss improved from 0.25151 to 0.25124, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2300 - accuracy: 0.9114 - val_loss: 0.2512 - val_accuracy: 0.9014\n",
      "Epoch 8/30\n",
      "164/169 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9135\n",
      "Epoch 8: val_loss did not improve from 0.25124\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2258 - accuracy: 0.9133 - val_loss: 0.2551 - val_accuracy: 0.9014\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.9146\n",
      "Epoch 9: val_loss improved from 0.25124 to 0.24885, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2225 - accuracy: 0.9146 - val_loss: 0.2489 - val_accuracy: 0.9064\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.9155\n",
      "Epoch 10: val_loss improved from 0.24885 to 0.24877, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2203 - accuracy: 0.9155 - val_loss: 0.2488 - val_accuracy: 0.9031\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2178 - accuracy: 0.9158\n",
      "Epoch 11: val_loss did not improve from 0.24877\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2178 - accuracy: 0.9158 - val_loss: 0.2536 - val_accuracy: 0.9039\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9168\n",
      "Epoch 12: val_loss improved from 0.24877 to 0.24875, saving model to model_radam_fold8.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2167 - accuracy: 0.9168 - val_loss: 0.2488 - val_accuracy: 0.9039\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9167\n",
      "Epoch 13: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2137 - accuracy: 0.9167 - val_loss: 0.2500 - val_accuracy: 0.9023\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9160\n",
      "Epoch 14: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2120 - accuracy: 0.9160 - val_loss: 0.2519 - val_accuracy: 0.9039\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9181\n",
      "Epoch 15: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2109 - accuracy: 0.9181 - val_loss: 0.2506 - val_accuracy: 0.9064\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9204\n",
      "Epoch 16: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2087 - accuracy: 0.9204 - val_loss: 0.2510 - val_accuracy: 0.9048\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9184\n",
      "Epoch 17: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2077 - accuracy: 0.9184 - val_loss: 0.2561 - val_accuracy: 0.9031\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9197\n",
      "Epoch 18: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2049 - accuracy: 0.9197 - val_loss: 0.2573 - val_accuracy: 0.9031\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9211\n",
      "Epoch 19: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2046 - accuracy: 0.9211 - val_loss: 0.2536 - val_accuracy: 0.9031\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9229\n",
      "Epoch 20: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2029 - accuracy: 0.9229 - val_loss: 0.2598 - val_accuracy: 0.9039\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2042 - accuracy: 0.9206\n",
      "Epoch 21: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2042 - accuracy: 0.9206 - val_loss: 0.2509 - val_accuracy: 0.9081\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9226\n",
      "Epoch 22: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1993 - accuracy: 0.9226 - val_loss: 0.2552 - val_accuracy: 0.9056\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9221\n",
      "Epoch 23: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1995 - accuracy: 0.9221 - val_loss: 0.2587 - val_accuracy: 0.9064\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9226\n",
      "Epoch 24: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1980 - accuracy: 0.9226 - val_loss: 0.2555 - val_accuracy: 0.9073\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9252\n",
      "Epoch 25: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1963 - accuracy: 0.9252 - val_loss: 0.2676 - val_accuracy: 0.9006\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9237\n",
      "Epoch 26: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1977 - accuracy: 0.9237 - val_loss: 0.2530 - val_accuracy: 0.9098\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1964 - accuracy: 0.9236\n",
      "Epoch 27: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1964 - accuracy: 0.9236 - val_loss: 0.2567 - val_accuracy: 0.9081\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9244\n",
      "Epoch 28: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1927 - accuracy: 0.9244 - val_loss: 0.2525 - val_accuracy: 0.9089\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9241\n",
      "Epoch 29: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1929 - accuracy: 0.9241 - val_loss: 0.2570 - val_accuracy: 0.9039\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9236\n",
      "Epoch 30: val_loss did not improve from 0.24875\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1930 - accuracy: 0.9236 - val_loss: 0.2531 - val_accuracy: 0.9081\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 8 - TP: 539, TN: 548, FP: 36, FN: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.8828\n",
      "Epoch 1: val_loss improved from inf to 0.28746, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 8s 13ms/step - loss: 0.3020 - accuracy: 0.8828 - val_loss: 0.2875 - val_accuracy: 0.8881\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9056\n",
      "Epoch 2: val_loss improved from 0.28746 to 0.28583, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2483 - accuracy: 0.9056 - val_loss: 0.2858 - val_accuracy: 0.8872\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9061\n",
      "Epoch 3: val_loss improved from 0.28583 to 0.27683, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2435 - accuracy: 0.9061 - val_loss: 0.2768 - val_accuracy: 0.8914\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9085\n",
      "Epoch 4: val_loss did not improve from 0.27683\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2369 - accuracy: 0.9085 - val_loss: 0.2781 - val_accuracy: 0.8922\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9105\n",
      "Epoch 5: val_loss improved from 0.27683 to 0.27212, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2332 - accuracy: 0.9105 - val_loss: 0.2721 - val_accuracy: 0.8931\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9109\n",
      "Epoch 6: val_loss did not improve from 0.27212\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2304 - accuracy: 0.9109 - val_loss: 0.2739 - val_accuracy: 0.8922\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9126\n",
      "Epoch 7: val_loss did not improve from 0.27212\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2265 - accuracy: 0.9126 - val_loss: 0.2743 - val_accuracy: 0.8922\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9144\n",
      "Epoch 8: val_loss improved from 0.27212 to 0.27139, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2235 - accuracy: 0.9144 - val_loss: 0.2714 - val_accuracy: 0.8981\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.9139\n",
      "Epoch 9: val_loss improved from 0.27139 to 0.26544, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2210 - accuracy: 0.9139 - val_loss: 0.2654 - val_accuracy: 0.8997\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9157\n",
      "Epoch 10: val_loss did not improve from 0.26544\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2183 - accuracy: 0.9157 - val_loss: 0.2666 - val_accuracy: 0.8956\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9159\n",
      "Epoch 11: val_loss did not improve from 0.26544\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2171 - accuracy: 0.9159 - val_loss: 0.2703 - val_accuracy: 0.8964\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9175\n",
      "Epoch 12: val_loss improved from 0.26544 to 0.26459, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2137 - accuracy: 0.9175 - val_loss: 0.2646 - val_accuracy: 0.8964\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9171\n",
      "Epoch 13: val_loss did not improve from 0.26459\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2114 - accuracy: 0.9171 - val_loss: 0.2690 - val_accuracy: 0.8964\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9172\n",
      "Epoch 14: val_loss improved from 0.26459 to 0.26416, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2115 - accuracy: 0.9172 - val_loss: 0.2642 - val_accuracy: 0.8964\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9183\n",
      "Epoch 15: val_loss improved from 0.26416 to 0.26099, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2091 - accuracy: 0.9183 - val_loss: 0.2610 - val_accuracy: 0.8981\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9188\n",
      "Epoch 16: val_loss did not improve from 0.26099\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2075 - accuracy: 0.9188 - val_loss: 0.2687 - val_accuracy: 0.8947\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9195\n",
      "Epoch 17: val_loss did not improve from 0.26099\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2062 - accuracy: 0.9195 - val_loss: 0.2618 - val_accuracy: 0.8989\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2038 - accuracy: 0.9205\n",
      "Epoch 18: val_loss improved from 0.26099 to 0.25868, saving model to model_radam_fold9.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2038 - accuracy: 0.9205 - val_loss: 0.2587 - val_accuracy: 0.8989\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9198\n",
      "Epoch 19: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2031 - accuracy: 0.9198 - val_loss: 0.2651 - val_accuracy: 0.9006\n",
      "Epoch 20/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9213\n",
      "Epoch 20: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2026 - accuracy: 0.9210 - val_loss: 0.2610 - val_accuracy: 0.8956\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9222\n",
      "Epoch 21: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1987 - accuracy: 0.9222 - val_loss: 0.2668 - val_accuracy: 0.8906\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9216\n",
      "Epoch 22: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2001 - accuracy: 0.9216 - val_loss: 0.2603 - val_accuracy: 0.8997\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.9227\n",
      "Epoch 23: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1979 - accuracy: 0.9227 - val_loss: 0.2655 - val_accuracy: 0.8989\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9230\n",
      "Epoch 24: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1973 - accuracy: 0.9230 - val_loss: 0.2605 - val_accuracy: 0.8981\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9223\n",
      "Epoch 25: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1958 - accuracy: 0.9223 - val_loss: 0.2618 - val_accuracy: 0.9014\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9230\n",
      "Epoch 26: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1961 - accuracy: 0.9230 - val_loss: 0.2688 - val_accuracy: 0.8989\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9241\n",
      "Epoch 27: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1924 - accuracy: 0.9241 - val_loss: 0.2681 - val_accuracy: 0.8997\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9245\n",
      "Epoch 28: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1930 - accuracy: 0.9245 - val_loss: 0.2677 - val_accuracy: 0.8914\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1914 - accuracy: 0.9252\n",
      "Epoch 29: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1914 - accuracy: 0.9252 - val_loss: 0.2599 - val_accuracy: 0.9023\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9248\n",
      "Epoch 30: val_loss did not improve from 0.25868\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1909 - accuracy: 0.9248 - val_loss: 0.2702 - val_accuracy: 0.8997\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 9 - TP: 509, TN: 568, FP: 31, FN: 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.8791\n",
      "Epoch 1: val_loss improved from inf to 0.23745, saving model to model_radam_fold10.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 7s 13ms/step - loss: 0.3082 - accuracy: 0.8791 - val_loss: 0.2374 - val_accuracy: 0.9114\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9036\n",
      "Epoch 2: val_loss did not improve from 0.23745\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2522 - accuracy: 0.9036 - val_loss: 0.2377 - val_accuracy: 0.9006\n",
      "Epoch 3/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9058\n",
      "Epoch 3: val_loss improved from 0.23745 to 0.23701, saving model to model_radam_fold10.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2447 - accuracy: 0.9057 - val_loss: 0.2370 - val_accuracy: 0.9039\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9065\n",
      "Epoch 4: val_loss improved from 0.23701 to 0.23525, saving model to model_radam_fold10.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2410 - accuracy: 0.9065 - val_loss: 0.2353 - val_accuracy: 0.9056\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9080\n",
      "Epoch 5: val_loss did not improve from 0.23525\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2382 - accuracy: 0.9080 - val_loss: 0.2358 - val_accuracy: 0.9014\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.9099\n",
      "Epoch 6: val_loss improved from 0.23525 to 0.23218, saving model to model_radam_fold10.hdf5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2340 - accuracy: 0.9099 - val_loss: 0.2322 - val_accuracy: 0.9106\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9127\n",
      "Epoch 7: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2301 - accuracy: 0.9127 - val_loss: 0.2337 - val_accuracy: 0.9039\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9128\n",
      "Epoch 8: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2285 - accuracy: 0.9128 - val_loss: 0.2324 - val_accuracy: 0.9048\n",
      "Epoch 9/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.9121\n",
      "Epoch 9: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 9ms/step - loss: 0.2257 - accuracy: 0.9120 - val_loss: 0.2324 - val_accuracy: 0.9123\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9126\n",
      "Epoch 10: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2235 - accuracy: 0.9126 - val_loss: 0.2330 - val_accuracy: 0.9081\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9149\n",
      "Epoch 11: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2196 - accuracy: 0.9149 - val_loss: 0.2331 - val_accuracy: 0.9098\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9154\n",
      "Epoch 12: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2173 - accuracy: 0.9154 - val_loss: 0.2393 - val_accuracy: 0.9039\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9140\n",
      "Epoch 13: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2169 - accuracy: 0.9140 - val_loss: 0.2335 - val_accuracy: 0.9073\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9159\n",
      "Epoch 14: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2161 - accuracy: 0.9159 - val_loss: 0.2325 - val_accuracy: 0.9123\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9162\n",
      "Epoch 15: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2133 - accuracy: 0.9162 - val_loss: 0.2343 - val_accuracy: 0.9073\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9188\n",
      "Epoch 16: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2104 - accuracy: 0.9188 - val_loss: 0.2360 - val_accuracy: 0.9123\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9165\n",
      "Epoch 17: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2097 - accuracy: 0.9165 - val_loss: 0.2380 - val_accuracy: 0.9131\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2078 - accuracy: 0.9182\n",
      "Epoch 18: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2078 - accuracy: 0.9182 - val_loss: 0.2349 - val_accuracy: 0.9089\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9185\n",
      "Epoch 19: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2053 - accuracy: 0.9185 - val_loss: 0.2399 - val_accuracy: 0.9123\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9172\n",
      "Epoch 20: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2073 - accuracy: 0.9172 - val_loss: 0.2378 - val_accuracy: 0.9140\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9191\n",
      "Epoch 21: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2033 - accuracy: 0.9191 - val_loss: 0.2367 - val_accuracy: 0.9098\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9202\n",
      "Epoch 22: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2015 - accuracy: 0.9202 - val_loss: 0.2360 - val_accuracy: 0.9114\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9210\n",
      "Epoch 23: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2003 - accuracy: 0.9210 - val_loss: 0.2392 - val_accuracy: 0.9123\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9216\n",
      "Epoch 24: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1996 - accuracy: 0.9216 - val_loss: 0.2410 - val_accuracy: 0.9165\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9215\n",
      "Epoch 25: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1976 - accuracy: 0.9215 - val_loss: 0.2390 - val_accuracy: 0.9148\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9193\n",
      "Epoch 26: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1983 - accuracy: 0.9193 - val_loss: 0.2360 - val_accuracy: 0.9173\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9222\n",
      "Epoch 27: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1968 - accuracy: 0.9222 - val_loss: 0.2380 - val_accuracy: 0.9148\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9225\n",
      "Epoch 28: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1952 - accuracy: 0.9225 - val_loss: 0.2375 - val_accuracy: 0.9156\n",
      "Epoch 29/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.9221\n",
      "Epoch 29: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9222 - val_loss: 0.2404 - val_accuracy: 0.9198\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9219\n",
      "Epoch 30: val_loss did not improve from 0.23218\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.1956 - accuracy: 0.9219 - val_loss: 0.2412 - val_accuracy: 0.9173\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 10 - TP: 521, TN: 577, FP: 30, FN: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7849\n",
      "Epoch 1: val_loss improved from inf to 0.32291, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.5213 - accuracy: 0.7861 - val_loss: 0.3229 - val_accuracy: 0.8790\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8870\n",
      "Epoch 2: val_loss improved from 0.32291 to 0.27598, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2987 - accuracy: 0.8866 - val_loss: 0.2760 - val_accuracy: 0.8890\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.8949\n",
      "Epoch 3: val_loss improved from 0.27598 to 0.26658, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8951 - val_loss: 0.2666 - val_accuracy: 0.8907\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2691 - accuracy: 0.8965\n",
      "Epoch 4: val_loss improved from 0.26658 to 0.26188, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8967 - val_loss: 0.2619 - val_accuracy: 0.8965\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.9005\n",
      "Epoch 5: val_loss improved from 0.26188 to 0.25771, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.9006 - val_loss: 0.2577 - val_accuracy: 0.8957\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.9013\n",
      "Epoch 6: val_loss improved from 0.25771 to 0.25606, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.9013 - val_loss: 0.2561 - val_accuracy: 0.8957\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9028\n",
      "Epoch 7: val_loss improved from 0.25606 to 0.25526, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.9029 - val_loss: 0.2553 - val_accuracy: 0.9007\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.9021\n",
      "Epoch 8: val_loss improved from 0.25526 to 0.25351, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.9021 - val_loss: 0.2535 - val_accuracy: 0.9015\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.9030\n",
      "Epoch 9: val_loss improved from 0.25351 to 0.25256, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.9027 - val_loss: 0.2526 - val_accuracy: 0.8998\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2547 - accuracy: 0.9029\n",
      "Epoch 10: val_loss improved from 0.25256 to 0.25229, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.9030 - val_loss: 0.2523 - val_accuracy: 0.8990\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9026\n",
      "Epoch 11: val_loss did not improve from 0.25229\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2520 - accuracy: 0.9025 - val_loss: 0.2547 - val_accuracy: 0.9040\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9050\n",
      "Epoch 12: val_loss improved from 0.25229 to 0.25209, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2518 - accuracy: 0.9050 - val_loss: 0.2521 - val_accuracy: 0.8982\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9023\n",
      "Epoch 13: val_loss did not improve from 0.25209\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.9022 - val_loss: 0.2533 - val_accuracy: 0.8998\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9064\n",
      "Epoch 14: val_loss did not improve from 0.25209\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2491 - accuracy: 0.9062 - val_loss: 0.2524 - val_accuracy: 0.8973\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9042\n",
      "Epoch 15: val_loss improved from 0.25209 to 0.25140, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2492 - accuracy: 0.9041 - val_loss: 0.2514 - val_accuracy: 0.8973\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9058\n",
      "Epoch 16: val_loss did not improve from 0.25140\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2480 - accuracy: 0.9056 - val_loss: 0.2514 - val_accuracy: 0.8990\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.9050\n",
      "Epoch 17: val_loss did not improve from 0.25140\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.9050 - val_loss: 0.2518 - val_accuracy: 0.8982\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9050\n",
      "Epoch 18: val_loss did not improve from 0.25140\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2461 - accuracy: 0.9051 - val_loss: 0.2517 - val_accuracy: 0.8990\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2460 - accuracy: 0.9060\n",
      "Epoch 19: val_loss improved from 0.25140 to 0.25086, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2454 - accuracy: 0.9064 - val_loss: 0.2509 - val_accuracy: 0.8998\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9055\n",
      "Epoch 20: val_loss did not improve from 0.25086\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9055 - val_loss: 0.2509 - val_accuracy: 0.8990\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9072\n",
      "Epoch 21: val_loss did not improve from 0.25086\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.9071 - val_loss: 0.2516 - val_accuracy: 0.8982\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.9058\n",
      "Epoch 22: val_loss did not improve from 0.25086\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9055 - val_loss: 0.2532 - val_accuracy: 0.8998\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.9078\n",
      "Epoch 23: val_loss improved from 0.25086 to 0.25037, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9080 - val_loss: 0.2504 - val_accuracy: 0.8998\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9064\n",
      "Epoch 24: val_loss did not improve from 0.25037\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2421 - accuracy: 0.9068 - val_loss: 0.2518 - val_accuracy: 0.8998\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9060\n",
      "Epoch 25: val_loss did not improve from 0.25037\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2429 - accuracy: 0.9058 - val_loss: 0.2508 - val_accuracy: 0.9015\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9079\n",
      "Epoch 26: val_loss improved from 0.25037 to 0.25022, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2412 - accuracy: 0.9073 - val_loss: 0.2502 - val_accuracy: 0.8998\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9073\n",
      "Epoch 27: val_loss improved from 0.25022 to 0.24968, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2396 - accuracy: 0.9070 - val_loss: 0.2497 - val_accuracy: 0.8990\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.9064\n",
      "Epoch 28: val_loss did not improve from 0.24968\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2399 - accuracy: 0.9065 - val_loss: 0.2512 - val_accuracy: 0.8998\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9064\n",
      "Epoch 29: val_loss did not improve from 0.24968\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2391 - accuracy: 0.9067 - val_loss: 0.2498 - val_accuracy: 0.8998\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9072\n",
      "Epoch 30: val_loss improved from 0.24968 to 0.24929, saving model to model_adamax_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2400 - accuracy: 0.9067 - val_loss: 0.2493 - val_accuracy: 0.8990\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 1 - TP: 543, TN: 534, FP: 54, FN: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.6882\n",
      "Epoch 1: val_loss improved from inf to 0.33675, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.5575 - accuracy: 0.6911 - val_loss: 0.3368 - val_accuracy: 0.8781\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8868\n",
      "Epoch 2: val_loss improved from 0.33675 to 0.27506, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.3026 - accuracy: 0.8867 - val_loss: 0.2751 - val_accuracy: 0.8990\n",
      "Epoch 3/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.8936\n",
      "Epoch 3: val_loss improved from 0.27506 to 0.26559, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2774 - accuracy: 0.8936 - val_loss: 0.2656 - val_accuracy: 0.9032\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8959\n",
      "Epoch 4: val_loss improved from 0.26559 to 0.26360, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.8961 - val_loss: 0.2636 - val_accuracy: 0.9032\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.8964\n",
      "Epoch 5: val_loss improved from 0.26360 to 0.25956, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.8966 - val_loss: 0.2596 - val_accuracy: 0.9082\n",
      "Epoch 6/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.8982\n",
      "Epoch 6: val_loss improved from 0.25956 to 0.25717, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2614 - accuracy: 0.8986 - val_loss: 0.2572 - val_accuracy: 0.9073\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9013\n",
      "Epoch 7: val_loss improved from 0.25717 to 0.25598, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2591 - accuracy: 0.9009 - val_loss: 0.2560 - val_accuracy: 0.9057\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.8981\n",
      "Epoch 8: val_loss improved from 0.25598 to 0.25511, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2580 - accuracy: 0.8983 - val_loss: 0.2551 - val_accuracy: 0.9073\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.9014\n",
      "Epoch 9: val_loss improved from 0.25511 to 0.25366, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2558 - accuracy: 0.9015 - val_loss: 0.2537 - val_accuracy: 0.9090\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9006\n",
      "Epoch 10: val_loss improved from 0.25366 to 0.25286, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2538 - accuracy: 0.9007 - val_loss: 0.2529 - val_accuracy: 0.9098\n",
      "Epoch 11/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.9020\n",
      "Epoch 11: val_loss improved from 0.25286 to 0.25080, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2525 - accuracy: 0.9016 - val_loss: 0.2508 - val_accuracy: 0.9073\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9024\n",
      "Epoch 12: val_loss improved from 0.25080 to 0.25054, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2519 - accuracy: 0.9023 - val_loss: 0.2505 - val_accuracy: 0.9098\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.9029\n",
      "Epoch 13: val_loss improved from 0.25054 to 0.24939, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2506 - accuracy: 0.9028 - val_loss: 0.2494 - val_accuracy: 0.9090\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9032\n",
      "Epoch 14: val_loss improved from 0.24939 to 0.24939, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2498 - accuracy: 0.9031 - val_loss: 0.2494 - val_accuracy: 0.9098\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.9023\n",
      "Epoch 15: val_loss improved from 0.24939 to 0.24847, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2495 - accuracy: 0.9026 - val_loss: 0.2485 - val_accuracy: 0.9082\n",
      "Epoch 16/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.9035\n",
      "Epoch 16: val_loss improved from 0.24847 to 0.24820, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2468 - accuracy: 0.9039 - val_loss: 0.2482 - val_accuracy: 0.9090\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.9052\n",
      "Epoch 17: val_loss did not improve from 0.24820\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2473 - accuracy: 0.9040 - val_loss: 0.2486 - val_accuracy: 0.9090\n",
      "Epoch 18/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9030\n",
      "Epoch 18: val_loss did not improve from 0.24820\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2455 - accuracy: 0.9027 - val_loss: 0.2510 - val_accuracy: 0.9082\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9037\n",
      "Epoch 19: val_loss improved from 0.24820 to 0.24748, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2471 - accuracy: 0.9035 - val_loss: 0.2475 - val_accuracy: 0.9107\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9055\n",
      "Epoch 20: val_loss improved from 0.24748 to 0.24684, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2444 - accuracy: 0.9054 - val_loss: 0.2468 - val_accuracy: 0.9098\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9050\n",
      "Epoch 21: val_loss improved from 0.24684 to 0.24560, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2435 - accuracy: 0.9051 - val_loss: 0.2456 - val_accuracy: 0.9124\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9050\n",
      "Epoch 22: val_loss did not improve from 0.24560\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9049 - val_loss: 0.2481 - val_accuracy: 0.9098\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.9072\n",
      "Epoch 23: val_loss did not improve from 0.24560\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2429 - accuracy: 0.9068 - val_loss: 0.2467 - val_accuracy: 0.9115\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9057\n",
      "Epoch 24: val_loss did not improve from 0.24560\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2420 - accuracy: 0.9053 - val_loss: 0.2462 - val_accuracy: 0.9090\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9064\n",
      "Epoch 25: val_loss improved from 0.24560 to 0.24503, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2417 - accuracy: 0.9068 - val_loss: 0.2450 - val_accuracy: 0.9115\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9036\n",
      "Epoch 26: val_loss improved from 0.24503 to 0.24372, saving model to model_adamax_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2427 - accuracy: 0.9039 - val_loss: 0.2437 - val_accuracy: 0.9115\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9063\n",
      "Epoch 27: val_loss did not improve from 0.24372\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2411 - accuracy: 0.9062 - val_loss: 0.2446 - val_accuracy: 0.9124\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9067\n",
      "Epoch 28: val_loss did not improve from 0.24372\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9060 - val_loss: 0.2459 - val_accuracy: 0.9115\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9057\n",
      "Epoch 29: val_loss did not improve from 0.24372\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2401 - accuracy: 0.9059 - val_loss: 0.2444 - val_accuracy: 0.9124\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9068\n",
      "Epoch 30: val_loss did not improve from 0.24372\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2391 - accuracy: 0.9072 - val_loss: 0.2445 - val_accuracy: 0.9098\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 2 - TP: 508, TN: 582, FP: 44, FN: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7687\n",
      "Epoch 1: val_loss improved from inf to 0.32746, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.5472 - accuracy: 0.7697 - val_loss: 0.3275 - val_accuracy: 0.8932\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8844\n",
      "Epoch 2: val_loss improved from 0.32746 to 0.27635, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.3057 - accuracy: 0.8837 - val_loss: 0.2763 - val_accuracy: 0.8948\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.8914\n",
      "Epoch 3: val_loss improved from 0.27635 to 0.26694, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2825 - accuracy: 0.8914 - val_loss: 0.2669 - val_accuracy: 0.8982\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.8941\n",
      "Epoch 4: val_loss improved from 0.26694 to 0.26308, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2728 - accuracy: 0.8943 - val_loss: 0.2631 - val_accuracy: 0.8965\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8972\n",
      "Epoch 5: val_loss improved from 0.26308 to 0.26081, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2664 - accuracy: 0.8973 - val_loss: 0.2608 - val_accuracy: 0.8990\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.8987\n",
      "Epoch 6: val_loss improved from 0.26081 to 0.25850, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2640 - accuracy: 0.8987 - val_loss: 0.2585 - val_accuracy: 0.9015\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9012\n",
      "Epoch 7: val_loss improved from 0.25850 to 0.25842, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2600 - accuracy: 0.9010 - val_loss: 0.2584 - val_accuracy: 0.8990\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9002\n",
      "Epoch 8: val_loss improved from 0.25842 to 0.25687, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2596 - accuracy: 0.9007 - val_loss: 0.2569 - val_accuracy: 0.8990\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2563 - accuracy: 0.9030\n",
      "Epoch 9: val_loss improved from 0.25687 to 0.25634, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2562 - accuracy: 0.9029 - val_loss: 0.2563 - val_accuracy: 0.8990\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.9034\n",
      "Epoch 10: val_loss improved from 0.25634 to 0.25423, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2545 - accuracy: 0.9032 - val_loss: 0.2542 - val_accuracy: 0.9023\n",
      "Epoch 11/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9036\n",
      "Epoch 11: val_loss did not improve from 0.25423\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2530 - accuracy: 0.9036 - val_loss: 0.2549 - val_accuracy: 0.9007\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9040\n",
      "Epoch 12: val_loss improved from 0.25423 to 0.25283, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2513 - accuracy: 0.9036 - val_loss: 0.2528 - val_accuracy: 0.9040\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.9042\n",
      "Epoch 13: val_loss improved from 0.25283 to 0.25219, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2509 - accuracy: 0.9041 - val_loss: 0.2522 - val_accuracy: 0.9048\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9033\n",
      "Epoch 14: val_loss did not improve from 0.25219\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2502 - accuracy: 0.9035 - val_loss: 0.2526 - val_accuracy: 0.9023\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9030\n",
      "Epoch 15: val_loss improved from 0.25219 to 0.25131, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2500 - accuracy: 0.9031 - val_loss: 0.2513 - val_accuracy: 0.9057\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.9049\n",
      "Epoch 16: val_loss did not improve from 0.25131\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2484 - accuracy: 0.9047 - val_loss: 0.2524 - val_accuracy: 0.9040\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.9039\n",
      "Epoch 17: val_loss improved from 0.25131 to 0.25051, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2486 - accuracy: 0.9037 - val_loss: 0.2505 - val_accuracy: 0.9057\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9046\n",
      "Epoch 18: val_loss improved from 0.25051 to 0.25034, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2464 - accuracy: 0.9046 - val_loss: 0.2503 - val_accuracy: 0.9057\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9036\n",
      "Epoch 19: val_loss improved from 0.25034 to 0.25004, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2469 - accuracy: 0.9036 - val_loss: 0.2500 - val_accuracy: 0.9057\n",
      "Epoch 20/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9049\n",
      "Epoch 20: val_loss did not improve from 0.25004\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2466 - accuracy: 0.9049 - val_loss: 0.2503 - val_accuracy: 0.9032\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9067\n",
      "Epoch 21: val_loss did not improve from 0.25004\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2442 - accuracy: 0.9067 - val_loss: 0.2526 - val_accuracy: 0.9040\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9050\n",
      "Epoch 22: val_loss improved from 0.25004 to 0.24963, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2440 - accuracy: 0.9053 - val_loss: 0.2496 - val_accuracy: 0.9057\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9055\n",
      "Epoch 23: val_loss improved from 0.24963 to 0.24869, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2432 - accuracy: 0.9055 - val_loss: 0.2487 - val_accuracy: 0.9057\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9064\n",
      "Epoch 24: val_loss did not improve from 0.24869\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9071 - val_loss: 0.2487 - val_accuracy: 0.9057\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9063\n",
      "Epoch 25: val_loss improved from 0.24869 to 0.24858, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2430 - accuracy: 0.9065 - val_loss: 0.2486 - val_accuracy: 0.9048\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9062\n",
      "Epoch 26: val_loss did not improve from 0.24858\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9066 - val_loss: 0.2497 - val_accuracy: 0.9048\n",
      "Epoch 27/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.9065\n",
      "Epoch 27: val_loss did not improve from 0.24858\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2419 - accuracy: 0.9066 - val_loss: 0.2487 - val_accuracy: 0.9073\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9077\n",
      "Epoch 28: val_loss improved from 0.24858 to 0.24782, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2407 - accuracy: 0.9079 - val_loss: 0.2478 - val_accuracy: 0.9040\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9075\n",
      "Epoch 29: val_loss improved from 0.24782 to 0.24780, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2403 - accuracy: 0.9076 - val_loss: 0.2478 - val_accuracy: 0.9065\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9080\n",
      "Epoch 30: val_loss improved from 0.24780 to 0.24661, saving model to model_adamax_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2403 - accuracy: 0.9077 - val_loss: 0.2466 - val_accuracy: 0.9048\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 3 - TP: 510, TN: 574, FP: 47, FN: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5422 - accuracy: 0.7349\n",
      "Epoch 1: val_loss improved from inf to 0.34000, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.5394 - accuracy: 0.7369 - val_loss: 0.3400 - val_accuracy: 0.8689\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8855\n",
      "Epoch 2: val_loss improved from 0.34000 to 0.28473, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.3045 - accuracy: 0.8852 - val_loss: 0.2847 - val_accuracy: 0.8915\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8918\n",
      "Epoch 3: val_loss improved from 0.28473 to 0.27393, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2785 - accuracy: 0.8924 - val_loss: 0.2739 - val_accuracy: 0.9007\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2694 - accuracy: 0.8952\n",
      "Epoch 4: val_loss improved from 0.27393 to 0.26951, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2692 - accuracy: 0.8956 - val_loss: 0.2695 - val_accuracy: 0.9048\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.8993\n",
      "Epoch 5: val_loss improved from 0.26951 to 0.26821, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2639 - accuracy: 0.8991 - val_loss: 0.2682 - val_accuracy: 0.9007\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.8998\n",
      "Epoch 6: val_loss improved from 0.26821 to 0.26529, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2613 - accuracy: 0.8999 - val_loss: 0.2653 - val_accuracy: 0.8998\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.9020\n",
      "Epoch 7: val_loss improved from 0.26529 to 0.26507, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2586 - accuracy: 0.9021 - val_loss: 0.2651 - val_accuracy: 0.9015\n",
      "Epoch 8/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9006\n",
      "Epoch 8: val_loss improved from 0.26507 to 0.26393, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2553 - accuracy: 0.9012 - val_loss: 0.2639 - val_accuracy: 0.8990\n",
      "Epoch 9/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9020\n",
      "Epoch 9: val_loss improved from 0.26393 to 0.26284, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2544 - accuracy: 0.9025 - val_loss: 0.2628 - val_accuracy: 0.8998\n",
      "Epoch 10/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9028\n",
      "Epoch 10: val_loss improved from 0.26284 to 0.26194, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2534 - accuracy: 0.9022 - val_loss: 0.2619 - val_accuracy: 0.9007\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9040\n",
      "Epoch 11: val_loss improved from 0.26194 to 0.26096, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2517 - accuracy: 0.9035 - val_loss: 0.2610 - val_accuracy: 0.8998\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9040\n",
      "Epoch 12: val_loss improved from 0.26096 to 0.26014, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2506 - accuracy: 0.9041 - val_loss: 0.2601 - val_accuracy: 0.8990\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9037\n",
      "Epoch 13: val_loss improved from 0.26014 to 0.26000, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2504 - accuracy: 0.9039 - val_loss: 0.2600 - val_accuracy: 0.9015\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9032\n",
      "Epoch 14: val_loss improved from 0.26000 to 0.25942, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2503 - accuracy: 0.9036 - val_loss: 0.2594 - val_accuracy: 0.9007\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9049\n",
      "Epoch 15: val_loss improved from 0.25942 to 0.25934, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2479 - accuracy: 0.9051 - val_loss: 0.2593 - val_accuracy: 0.9032\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.9039\n",
      "Epoch 16: val_loss did not improve from 0.25934\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2464 - accuracy: 0.9042 - val_loss: 0.2603 - val_accuracy: 0.8990\n",
      "Epoch 17/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9048\n",
      "Epoch 17: val_loss improved from 0.25934 to 0.25812, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2472 - accuracy: 0.9046 - val_loss: 0.2581 - val_accuracy: 0.9048\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9057\n",
      "Epoch 18: val_loss improved from 0.25812 to 0.25757, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2452 - accuracy: 0.9055 - val_loss: 0.2576 - val_accuracy: 0.9040\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.9057\n",
      "Epoch 19: val_loss improved from 0.25757 to 0.25727, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2456 - accuracy: 0.9059 - val_loss: 0.2573 - val_accuracy: 0.9040\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2456 - accuracy: 0.9068\n",
      "Epoch 20: val_loss improved from 0.25727 to 0.25634, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2457 - accuracy: 0.9069 - val_loss: 0.2563 - val_accuracy: 0.9040\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9046\n",
      "Epoch 21: val_loss did not improve from 0.25634\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2438 - accuracy: 0.9047 - val_loss: 0.2566 - val_accuracy: 0.9048\n",
      "Epoch 22/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9060\n",
      "Epoch 22: val_loss improved from 0.25634 to 0.25570, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2439 - accuracy: 0.9057 - val_loss: 0.2557 - val_accuracy: 0.9048\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9066\n",
      "Epoch 23: val_loss did not improve from 0.25570\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.9062 - val_loss: 0.2564 - val_accuracy: 0.9032\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9058\n",
      "Epoch 24: val_loss improved from 0.25570 to 0.25437, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2430 - accuracy: 0.9061 - val_loss: 0.2544 - val_accuracy: 0.9065\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2409 - accuracy: 0.9062\n",
      "Epoch 25: val_loss did not improve from 0.25437\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2410 - accuracy: 0.9061 - val_loss: 0.2552 - val_accuracy: 0.9057\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9064\n",
      "Epoch 26: val_loss did not improve from 0.25437\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9064 - val_loss: 0.2544 - val_accuracy: 0.9007\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9069\n",
      "Epoch 27: val_loss improved from 0.25437 to 0.25343, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2411 - accuracy: 0.9072 - val_loss: 0.2534 - val_accuracy: 0.9032\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9078\n",
      "Epoch 28: val_loss improved from 0.25343 to 0.25283, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2397 - accuracy: 0.9075 - val_loss: 0.2528 - val_accuracy: 0.9023\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9062\n",
      "Epoch 29: val_loss did not improve from 0.25283\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2383 - accuracy: 0.9061 - val_loss: 0.2534 - val_accuracy: 0.9057\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.9068\n",
      "Epoch 30: val_loss improved from 0.25283 to 0.25202, saving model to model_adamax_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2407 - accuracy: 0.9068 - val_loss: 0.2520 - val_accuracy: 0.9065\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 4 - TP: 532, TN: 554, FP: 38, FN: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7720\n",
      "Epoch 1: val_loss improved from inf to 0.34916, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.5472 - accuracy: 0.7737 - val_loss: 0.3492 - val_accuracy: 0.8740\n",
      "Epoch 2/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8849\n",
      "Epoch 2: val_loss improved from 0.34916 to 0.29466, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.3061 - accuracy: 0.8848 - val_loss: 0.2947 - val_accuracy: 0.8881\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.8939\n",
      "Epoch 3: val_loss improved from 0.29466 to 0.28149, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2786 - accuracy: 0.8939 - val_loss: 0.2815 - val_accuracy: 0.8907\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2693 - accuracy: 0.8939\n",
      "Epoch 4: val_loss improved from 0.28149 to 0.27534, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2692 - accuracy: 0.8939 - val_loss: 0.2753 - val_accuracy: 0.8932\n",
      "Epoch 5/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.2643 - accuracy: 0.8999\n",
      "Epoch 5: val_loss improved from 0.27534 to 0.27046, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2644 - accuracy: 0.8992 - val_loss: 0.2705 - val_accuracy: 0.8998\n",
      "Epoch 6/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.9003\n",
      "Epoch 6: val_loss improved from 0.27046 to 0.26773, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2617 - accuracy: 0.9002 - val_loss: 0.2677 - val_accuracy: 0.9015\n",
      "Epoch 7/30\n",
      "162/169 [===========================>..] - ETA: 0s - loss: 0.2572 - accuracy: 0.9012\n",
      "Epoch 7: val_loss improved from 0.26773 to 0.26644, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2583 - accuracy: 0.9012 - val_loss: 0.2664 - val_accuracy: 0.8998\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2571 - accuracy: 0.9013\n",
      "Epoch 8: val_loss improved from 0.26644 to 0.26447, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2568 - accuracy: 0.9015 - val_loss: 0.2645 - val_accuracy: 0.9015\n",
      "Epoch 9/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.9017\n",
      "Epoch 9: val_loss improved from 0.26447 to 0.26384, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2551 - accuracy: 0.9017 - val_loss: 0.2638 - val_accuracy: 0.9023\n",
      "Epoch 10/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9033\n",
      "Epoch 10: val_loss improved from 0.26384 to 0.26269, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2537 - accuracy: 0.9034 - val_loss: 0.2627 - val_accuracy: 0.8998\n",
      "Epoch 11/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.9040\n",
      "Epoch 11: val_loss improved from 0.26269 to 0.26104, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2530 - accuracy: 0.9040 - val_loss: 0.2610 - val_accuracy: 0.9015\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9031\n",
      "Epoch 12: val_loss improved from 0.26104 to 0.25979, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2517 - accuracy: 0.9034 - val_loss: 0.2598 - val_accuracy: 0.9007\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9041\n",
      "Epoch 13: val_loss improved from 0.25979 to 0.25961, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2508 - accuracy: 0.9041 - val_loss: 0.2596 - val_accuracy: 0.8990\n",
      "Epoch 14/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.9039\n",
      "Epoch 14: val_loss improved from 0.25961 to 0.25862, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2486 - accuracy: 0.9037 - val_loss: 0.2586 - val_accuracy: 0.8998\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9048\n",
      "Epoch 15: val_loss improved from 0.25862 to 0.25786, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2479 - accuracy: 0.9051 - val_loss: 0.2579 - val_accuracy: 0.8990\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9050\n",
      "Epoch 16: val_loss improved from 0.25786 to 0.25734, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2480 - accuracy: 0.9056 - val_loss: 0.2573 - val_accuracy: 0.8990\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9067\n",
      "Epoch 17: val_loss did not improve from 0.25734\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2455 - accuracy: 0.9066 - val_loss: 0.2581 - val_accuracy: 0.9048\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9062\n",
      "Epoch 18: val_loss improved from 0.25734 to 0.25718, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2459 - accuracy: 0.9062 - val_loss: 0.2572 - val_accuracy: 0.8965\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.9062\n",
      "Epoch 19: val_loss did not improve from 0.25718\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2453 - accuracy: 0.9059 - val_loss: 0.2572 - val_accuracy: 0.8982\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9053\n",
      "Epoch 20: val_loss improved from 0.25718 to 0.25660, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2435 - accuracy: 0.9057 - val_loss: 0.2566 - val_accuracy: 0.8982\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9065\n",
      "Epoch 21: val_loss improved from 0.25660 to 0.25580, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2433 - accuracy: 0.9067 - val_loss: 0.2558 - val_accuracy: 0.8998\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9075\n",
      "Epoch 22: val_loss improved from 0.25580 to 0.25539, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2432 - accuracy: 0.9075 - val_loss: 0.2554 - val_accuracy: 0.9015\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9051\n",
      "Epoch 23: val_loss did not improve from 0.25539\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9052 - val_loss: 0.2554 - val_accuracy: 0.9040\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9065\n",
      "Epoch 24: val_loss improved from 0.25539 to 0.25497, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2424 - accuracy: 0.9067 - val_loss: 0.2550 - val_accuracy: 0.9040\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.9065\n",
      "Epoch 25: val_loss improved from 0.25497 to 0.25435, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2422 - accuracy: 0.9064 - val_loss: 0.2543 - val_accuracy: 0.9015\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9080\n",
      "Epoch 26: val_loss did not improve from 0.25435\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9078 - val_loss: 0.2544 - val_accuracy: 0.9007\n",
      "Epoch 27/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9066\n",
      "Epoch 27: val_loss improved from 0.25435 to 0.25415, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2404 - accuracy: 0.9068 - val_loss: 0.2542 - val_accuracy: 0.9032\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9080\n",
      "Epoch 28: val_loss improved from 0.25415 to 0.25374, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2396 - accuracy: 0.9081 - val_loss: 0.2537 - val_accuracy: 0.9057\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9092\n",
      "Epoch 29: val_loss did not improve from 0.25374\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2386 - accuracy: 0.9090 - val_loss: 0.2541 - val_accuracy: 0.8990\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9076\n",
      "Epoch 30: val_loss improved from 0.25374 to 0.25347, saving model to model_adamax_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2381 - accuracy: 0.9075 - val_loss: 0.2535 - val_accuracy: 0.9015\n",
      "38/38 [==============================] - 2s 2ms/step\n",
      "Fold 5 - TP: 534, TN: 546, FP: 45, FN: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.5478 - accuracy: 0.7786\n",
      "Epoch 1: val_loss improved from inf to 0.33913, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.5462 - accuracy: 0.7792 - val_loss: 0.3391 - val_accuracy: 0.8688\n",
      "Epoch 2/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8861\n",
      "Epoch 2: val_loss improved from 0.33913 to 0.28718, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.3006 - accuracy: 0.8857 - val_loss: 0.2872 - val_accuracy: 0.8872\n",
      "Epoch 3/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2778 - accuracy: 0.8946\n",
      "Epoch 3: val_loss improved from 0.28718 to 0.27466, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2784 - accuracy: 0.8945 - val_loss: 0.2747 - val_accuracy: 0.8864\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8970\n",
      "Epoch 4: val_loss improved from 0.27466 to 0.26882, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2700 - accuracy: 0.8971 - val_loss: 0.2688 - val_accuracy: 0.8914\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.8987\n",
      "Epoch 5: val_loss improved from 0.26882 to 0.26707, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2653 - accuracy: 0.8988 - val_loss: 0.2671 - val_accuracy: 0.8972\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9007\n",
      "Epoch 6: val_loss improved from 0.26707 to 0.26485, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2610 - accuracy: 0.9004 - val_loss: 0.2649 - val_accuracy: 0.8989\n",
      "Epoch 7/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.8992\n",
      "Epoch 7: val_loss improved from 0.26485 to 0.26163, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2600 - accuracy: 0.8994 - val_loss: 0.2616 - val_accuracy: 0.9048\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9030\n",
      "Epoch 8: val_loss improved from 0.26163 to 0.26014, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2575 - accuracy: 0.9030 - val_loss: 0.2601 - val_accuracy: 0.9031\n",
      "Epoch 9/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.2573 - accuracy: 0.9008\n",
      "Epoch 9: val_loss improved from 0.26014 to 0.26012, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2570 - accuracy: 0.9008 - val_loss: 0.2601 - val_accuracy: 0.9056\n",
      "Epoch 10/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9009\n",
      "Epoch 10: val_loss did not improve from 0.26012\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.9010 - val_loss: 0.2601 - val_accuracy: 0.9023\n",
      "Epoch 11/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9021\n",
      "Epoch 11: val_loss improved from 0.26012 to 0.25747, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2526 - accuracy: 0.9022 - val_loss: 0.2575 - val_accuracy: 0.9039\n",
      "Epoch 12/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.9037\n",
      "Epoch 12: val_loss did not improve from 0.25747\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2511 - accuracy: 0.9038 - val_loss: 0.2580 - val_accuracy: 0.9023\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9021\n",
      "Epoch 13: val_loss improved from 0.25747 to 0.25732, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2509 - accuracy: 0.9026 - val_loss: 0.2573 - val_accuracy: 0.9023\n",
      "Epoch 14/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2497 - accuracy: 0.9033\n",
      "Epoch 14: val_loss improved from 0.25732 to 0.25557, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2492 - accuracy: 0.9034 - val_loss: 0.2556 - val_accuracy: 0.9073\n",
      "Epoch 15/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9048\n",
      "Epoch 15: val_loss did not improve from 0.25557\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2493 - accuracy: 0.9046 - val_loss: 0.2571 - val_accuracy: 0.9023\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9046\n",
      "Epoch 16: val_loss did not improve from 0.25557\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2493 - accuracy: 0.9045 - val_loss: 0.2559 - val_accuracy: 0.9014\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2478 - accuracy: 0.9044\n",
      "Epoch 17: val_loss improved from 0.25557 to 0.25498, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2473 - accuracy: 0.9046 - val_loss: 0.2550 - val_accuracy: 0.9031\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.9051\n",
      "Epoch 18: val_loss did not improve from 0.25498\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2457 - accuracy: 0.9051 - val_loss: 0.2550 - val_accuracy: 0.9031\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.9070\n",
      "Epoch 19: val_loss improved from 0.25498 to 0.25484, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2454 - accuracy: 0.9069 - val_loss: 0.2548 - val_accuracy: 0.9023\n",
      "Epoch 20/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.9057\n",
      "Epoch 20: val_loss did not improve from 0.25484\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2456 - accuracy: 0.9057 - val_loss: 0.2550 - val_accuracy: 0.9014\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9062\n",
      "Epoch 21: val_loss did not improve from 0.25484\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2446 - accuracy: 0.9066 - val_loss: 0.2555 - val_accuracy: 0.9014\n",
      "Epoch 22/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.9064\n",
      "Epoch 22: val_loss improved from 0.25484 to 0.25429, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2438 - accuracy: 0.9063 - val_loss: 0.2543 - val_accuracy: 0.9006\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.9059\n",
      "Epoch 23: val_loss did not improve from 0.25429\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2438 - accuracy: 0.9059 - val_loss: 0.2577 - val_accuracy: 0.8981\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9069\n",
      "Epoch 24: val_loss did not improve from 0.25429\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2433 - accuracy: 0.9070 - val_loss: 0.2546 - val_accuracy: 0.9014\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.9057\n",
      "Epoch 25: val_loss did not improve from 0.25429\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2430 - accuracy: 0.9059 - val_loss: 0.2547 - val_accuracy: 0.9014\n",
      "Epoch 26/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9072\n",
      "Epoch 26: val_loss did not improve from 0.25429\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9072 - val_loss: 0.2552 - val_accuracy: 0.9014\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9069\n",
      "Epoch 27: val_loss did not improve from 0.25429\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2414 - accuracy: 0.9070 - val_loss: 0.2555 - val_accuracy: 0.8989\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9084\n",
      "Epoch 28: val_loss improved from 0.25429 to 0.25354, saving model to model_adamax_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2409 - accuracy: 0.9088 - val_loss: 0.2535 - val_accuracy: 0.9064\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.9073\n",
      "Epoch 29: val_loss did not improve from 0.25354\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2401 - accuracy: 0.9070 - val_loss: 0.2558 - val_accuracy: 0.8989\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9091\n",
      "Epoch 30: val_loss did not improve from 0.25354\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2385 - accuracy: 0.9092 - val_loss: 0.2552 - val_accuracy: 0.8997\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 6 - TP: 570, TN: 507, FP: 47, FN: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.5524 - accuracy: 0.7280\n",
      "Epoch 1: val_loss improved from inf to 0.34368, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.5480 - accuracy: 0.7312 - val_loss: 0.3437 - val_accuracy: 0.8655\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8854\n",
      "Epoch 2: val_loss improved from 0.34368 to 0.29460, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.3061 - accuracy: 0.8850 - val_loss: 0.2946 - val_accuracy: 0.8872\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8938\n",
      "Epoch 3: val_loss improved from 0.29460 to 0.28227, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2795 - accuracy: 0.8935 - val_loss: 0.2823 - val_accuracy: 0.8889\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.8946\n",
      "Epoch 4: val_loss improved from 0.28227 to 0.27770, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2696 - accuracy: 0.8947 - val_loss: 0.2777 - val_accuracy: 0.8872\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.8985\n",
      "Epoch 5: val_loss improved from 0.27770 to 0.27418, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2643 - accuracy: 0.8991 - val_loss: 0.2742 - val_accuracy: 0.8889\n",
      "Epoch 6/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9011\n",
      "Epoch 6: val_loss improved from 0.27418 to 0.27299, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2609 - accuracy: 0.9013 - val_loss: 0.2730 - val_accuracy: 0.8889\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9019\n",
      "Epoch 7: val_loss improved from 0.27299 to 0.27073, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2588 - accuracy: 0.9017 - val_loss: 0.2707 - val_accuracy: 0.8939\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9030\n",
      "Epoch 8: val_loss improved from 0.27073 to 0.27040, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2554 - accuracy: 0.9030 - val_loss: 0.2704 - val_accuracy: 0.8897\n",
      "Epoch 9/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9023\n",
      "Epoch 9: val_loss improved from 0.27040 to 0.27008, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2550 - accuracy: 0.9023 - val_loss: 0.2701 - val_accuracy: 0.8906\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9035\n",
      "Epoch 10: val_loss improved from 0.27008 to 0.26982, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2521 - accuracy: 0.9034 - val_loss: 0.2698 - val_accuracy: 0.8914\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.9049\n",
      "Epoch 11: val_loss improved from 0.26982 to 0.26856, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2506 - accuracy: 0.9043 - val_loss: 0.2686 - val_accuracy: 0.8947\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.9049\n",
      "Epoch 12: val_loss did not improve from 0.26856\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2504 - accuracy: 0.9051 - val_loss: 0.2694 - val_accuracy: 0.8914\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9044\n",
      "Epoch 13: val_loss improved from 0.26856 to 0.26845, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2496 - accuracy: 0.9044 - val_loss: 0.2684 - val_accuracy: 0.8964\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.9070\n",
      "Epoch 14: val_loss improved from 0.26845 to 0.26739, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2473 - accuracy: 0.9071 - val_loss: 0.2674 - val_accuracy: 0.8922\n",
      "Epoch 15/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9061\n",
      "Epoch 15: val_loss improved from 0.26739 to 0.26641, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2476 - accuracy: 0.9060 - val_loss: 0.2664 - val_accuracy: 0.8956\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9067\n",
      "Epoch 16: val_loss improved from 0.26641 to 0.26600, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2456 - accuracy: 0.9062 - val_loss: 0.2660 - val_accuracy: 0.8947\n",
      "Epoch 17/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9048\n",
      "Epoch 17: val_loss did not improve from 0.26600\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2462 - accuracy: 0.9048 - val_loss: 0.2661 - val_accuracy: 0.8964\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9063\n",
      "Epoch 18: val_loss improved from 0.26600 to 0.26511, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2438 - accuracy: 0.9065 - val_loss: 0.2651 - val_accuracy: 0.8939\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9062\n",
      "Epoch 19: val_loss did not improve from 0.26511\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.9064 - val_loss: 0.2654 - val_accuracy: 0.8931\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9062\n",
      "Epoch 20: val_loss did not improve from 0.26511\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2420 - accuracy: 0.9068 - val_loss: 0.2657 - val_accuracy: 0.8964\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9079\n",
      "Epoch 21: val_loss did not improve from 0.26511\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2414 - accuracy: 0.9080 - val_loss: 0.2658 - val_accuracy: 0.8947\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9067\n",
      "Epoch 22: val_loss did not improve from 0.26511\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2422 - accuracy: 0.9065 - val_loss: 0.2651 - val_accuracy: 0.8964\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9085\n",
      "Epoch 23: val_loss improved from 0.26511 to 0.26476, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2404 - accuracy: 0.9085 - val_loss: 0.2648 - val_accuracy: 0.8981\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9067\n",
      "Epoch 24: val_loss did not improve from 0.26476\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2410 - accuracy: 0.9069 - val_loss: 0.2649 - val_accuracy: 0.8931\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9067\n",
      "Epoch 25: val_loss improved from 0.26476 to 0.26442, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2412 - accuracy: 0.9071 - val_loss: 0.2644 - val_accuracy: 0.8939\n",
      "Epoch 26/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2392 - accuracy: 0.9086\n",
      "Epoch 26: val_loss did not improve from 0.26442\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2391 - accuracy: 0.9085 - val_loss: 0.2654 - val_accuracy: 0.8922\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2397 - accuracy: 0.9079\n",
      "Epoch 27: val_loss improved from 0.26442 to 0.26354, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2394 - accuracy: 0.9081 - val_loss: 0.2635 - val_accuracy: 0.8972\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9106\n",
      "Epoch 28: val_loss did not improve from 0.26354\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2381 - accuracy: 0.9105 - val_loss: 0.2643 - val_accuracy: 0.8914\n",
      "Epoch 29/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.9095\n",
      "Epoch 29: val_loss did not improve from 0.26354\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2382 - accuracy: 0.9093 - val_loss: 0.2648 - val_accuracy: 0.8931\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9098\n",
      "Epoch 30: val_loss improved from 0.26354 to 0.26334, saving model to model_adamax_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2371 - accuracy: 0.9096 - val_loss: 0.2633 - val_accuracy: 0.8931\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 7 - TP: 506, TN: 563, FP: 54, FN: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5584 - accuracy: 0.7356\n",
      "Epoch 1: val_loss improved from inf to 0.35051, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.5552 - accuracy: 0.7377 - val_loss: 0.3505 - val_accuracy: 0.8613\n",
      "Epoch 2/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8859\n",
      "Epoch 2: val_loss improved from 0.35051 to 0.28301, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.3059 - accuracy: 0.8856 - val_loss: 0.2830 - val_accuracy: 0.8897\n",
      "Epoch 3/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2804 - accuracy: 0.8923\n",
      "Epoch 3: val_loss improved from 0.28301 to 0.27160, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2796 - accuracy: 0.8920 - val_loss: 0.2716 - val_accuracy: 0.9006\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.8957\n",
      "Epoch 4: val_loss improved from 0.27160 to 0.25940, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2706 - accuracy: 0.8956 - val_loss: 0.2594 - val_accuracy: 0.9006\n",
      "Epoch 5/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8984\n",
      "Epoch 5: val_loss improved from 0.25940 to 0.25805, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2660 - accuracy: 0.8984 - val_loss: 0.2581 - val_accuracy: 0.9064\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.8989\n",
      "Epoch 6: val_loss improved from 0.25805 to 0.25397, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2626 - accuracy: 0.8989 - val_loss: 0.2540 - val_accuracy: 0.9073\n",
      "Epoch 7/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9006\n",
      "Epoch 7: val_loss improved from 0.25397 to 0.25277, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2599 - accuracy: 0.9007 - val_loss: 0.2528 - val_accuracy: 0.9081\n",
      "Epoch 8/30\n",
      "160/169 [===========================>..] - ETA: 0s - loss: 0.2564 - accuracy: 0.9021\n",
      "Epoch 8: val_loss improved from 0.25277 to 0.25111, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2580 - accuracy: 0.9010 - val_loss: 0.2511 - val_accuracy: 0.9081\n",
      "Epoch 9/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2550 - accuracy: 0.9015\n",
      "Epoch 9: val_loss did not improve from 0.25111\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.9014 - val_loss: 0.2532 - val_accuracy: 0.9098\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9020\n",
      "Epoch 10: val_loss improved from 0.25111 to 0.24978, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2551 - accuracy: 0.9020 - val_loss: 0.2498 - val_accuracy: 0.9098\n",
      "Epoch 11/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9020\n",
      "Epoch 11: val_loss improved from 0.24978 to 0.24796, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2536 - accuracy: 0.9021 - val_loss: 0.2480 - val_accuracy: 0.9081\n",
      "Epoch 12/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.9015\n",
      "Epoch 12: val_loss did not improve from 0.24796\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.9014 - val_loss: 0.2483 - val_accuracy: 0.9081\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9030\n",
      "Epoch 13: val_loss improved from 0.24796 to 0.24675, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2514 - accuracy: 0.9030 - val_loss: 0.2467 - val_accuracy: 0.9089\n",
      "Epoch 14/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9034\n",
      "Epoch 14: val_loss improved from 0.24675 to 0.24617, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2503 - accuracy: 0.9035 - val_loss: 0.2462 - val_accuracy: 0.9089\n",
      "Epoch 15/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2483 - accuracy: 0.9045\n",
      "Epoch 15: val_loss improved from 0.24617 to 0.24573, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2492 - accuracy: 0.9043 - val_loss: 0.2457 - val_accuracy: 0.9081\n",
      "Epoch 16/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2480 - accuracy: 0.9050\n",
      "Epoch 16: val_loss improved from 0.24573 to 0.24542, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2478 - accuracy: 0.9050 - val_loss: 0.2454 - val_accuracy: 0.9089\n",
      "Epoch 17/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.2474 - accuracy: 0.9029\n",
      "Epoch 17: val_loss did not improve from 0.24542\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2462 - accuracy: 0.9034 - val_loss: 0.2464 - val_accuracy: 0.9081\n",
      "Epoch 18/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.9049\n",
      "Epoch 18: val_loss did not improve from 0.24542\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.9049 - val_loss: 0.2463 - val_accuracy: 0.9081\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9053\n",
      "Epoch 19: val_loss improved from 0.24542 to 0.24432, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2459 - accuracy: 0.9053 - val_loss: 0.2443 - val_accuracy: 0.9106\n",
      "Epoch 20/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2474 - accuracy: 0.9054\n",
      "Epoch 20: val_loss did not improve from 0.24432\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2461 - accuracy: 0.9056 - val_loss: 0.2447 - val_accuracy: 0.9081\n",
      "Epoch 21/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2444 - accuracy: 0.9042\n",
      "Epoch 21: val_loss did not improve from 0.24432\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2448 - accuracy: 0.9040 - val_loss: 0.2476 - val_accuracy: 0.9098\n",
      "Epoch 22/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2447 - accuracy: 0.9067\n",
      "Epoch 22: val_loss improved from 0.24432 to 0.24409, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2440 - accuracy: 0.9060 - val_loss: 0.2441 - val_accuracy: 0.9098\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9062\n",
      "Epoch 23: val_loss did not improve from 0.24409\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2442 - accuracy: 0.9061 - val_loss: 0.2451 - val_accuracy: 0.9098\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.9051\n",
      "Epoch 24: val_loss did not improve from 0.24409\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2437 - accuracy: 0.9051 - val_loss: 0.2482 - val_accuracy: 0.9098\n",
      "Epoch 25/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9056\n",
      "Epoch 25: val_loss did not improve from 0.24409\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.9060 - val_loss: 0.2466 - val_accuracy: 0.9089\n",
      "Epoch 26/30\n",
      "164/169 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9058\n",
      "Epoch 26: val_loss did not improve from 0.24409\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2416 - accuracy: 0.9052 - val_loss: 0.2463 - val_accuracy: 0.9098\n",
      "Epoch 27/30\n",
      "164/169 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9064\n",
      "Epoch 27: val_loss did not improve from 0.24409\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2413 - accuracy: 0.9055 - val_loss: 0.2446 - val_accuracy: 0.9106\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9071\n",
      "Epoch 28: val_loss did not improve from 0.24409\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2413 - accuracy: 0.9071 - val_loss: 0.2445 - val_accuracy: 0.9106\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9074\n",
      "Epoch 29: val_loss improved from 0.24409 to 0.24401, saving model to model_adamax_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2390 - accuracy: 0.9070 - val_loss: 0.2440 - val_accuracy: 0.9098\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.9071\n",
      "Epoch 30: val_loss did not improve from 0.24401\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2398 - accuracy: 0.9071 - val_loss: 0.2450 - val_accuracy: 0.9106\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 8 - TP: 543, TN: 547, FP: 31, FN: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.5565 - accuracy: 0.7418\n",
      "Epoch 1: val_loss improved from inf to 0.33018, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.5545 - accuracy: 0.7432 - val_loss: 0.3302 - val_accuracy: 0.8822\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8834\n",
      "Epoch 2: val_loss improved from 0.33018 to 0.26722, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.3072 - accuracy: 0.8830 - val_loss: 0.2672 - val_accuracy: 0.8997\n",
      "Epoch 3/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.8943\n",
      "Epoch 3: val_loss improved from 0.26722 to 0.25629, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2790 - accuracy: 0.8945 - val_loss: 0.2563 - val_accuracy: 0.8964\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.8948\n",
      "Epoch 4: val_loss improved from 0.25629 to 0.25194, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2700 - accuracy: 0.8952 - val_loss: 0.2519 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "161/169 [===========================>..] - ETA: 0s - loss: 0.2672 - accuracy: 0.8996\n",
      "Epoch 5: val_loss improved from 0.25194 to 0.24948, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2657 - accuracy: 0.9006 - val_loss: 0.2495 - val_accuracy: 0.9031\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2623 - accuracy: 0.9002\n",
      "Epoch 6: val_loss improved from 0.24948 to 0.24867, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2618 - accuracy: 0.9004 - val_loss: 0.2487 - val_accuracy: 0.9031\n",
      "Epoch 7/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9011\n",
      "Epoch 7: val_loss improved from 0.24867 to 0.24778, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2601 - accuracy: 0.9012 - val_loss: 0.2478 - val_accuracy: 0.9023\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.9014\n",
      "Epoch 8: val_loss did not improve from 0.24778\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.9013 - val_loss: 0.2483 - val_accuracy: 0.8997\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.9017\n",
      "Epoch 9: val_loss improved from 0.24778 to 0.24698, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2564 - accuracy: 0.9023 - val_loss: 0.2470 - val_accuracy: 0.8989\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.9036\n",
      "Epoch 10: val_loss improved from 0.24698 to 0.24683, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2544 - accuracy: 0.9040 - val_loss: 0.2468 - val_accuracy: 0.8981\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9051\n",
      "Epoch 11: val_loss improved from 0.24683 to 0.24581, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2530 - accuracy: 0.9050 - val_loss: 0.2458 - val_accuracy: 0.8972\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9027\n",
      "Epoch 12: val_loss improved from 0.24581 to 0.24568, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2524 - accuracy: 0.9023 - val_loss: 0.2457 - val_accuracy: 0.8972\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9044\n",
      "Epoch 13: val_loss improved from 0.24568 to 0.24535, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2513 - accuracy: 0.9047 - val_loss: 0.2453 - val_accuracy: 0.8972\n",
      "Epoch 14/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9024\n",
      "Epoch 14: val_loss improved from 0.24535 to 0.24460, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2509 - accuracy: 0.9028 - val_loss: 0.2446 - val_accuracy: 0.9006\n",
      "Epoch 15/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9052\n",
      "Epoch 15: val_loss did not improve from 0.24460\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2491 - accuracy: 0.9055 - val_loss: 0.2451 - val_accuracy: 0.8972\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9042\n",
      "Epoch 16: val_loss did not improve from 0.24460\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2490 - accuracy: 0.9042 - val_loss: 0.2449 - val_accuracy: 0.8964\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.9035\n",
      "Epoch 17: val_loss improved from 0.24460 to 0.24389, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2477 - accuracy: 0.9035 - val_loss: 0.2439 - val_accuracy: 0.8989\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9054\n",
      "Epoch 18: val_loss improved from 0.24389 to 0.24377, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2463 - accuracy: 0.9055 - val_loss: 0.2438 - val_accuracy: 0.9006\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9050\n",
      "Epoch 19: val_loss improved from 0.24377 to 0.24311, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2456 - accuracy: 0.9054 - val_loss: 0.2431 - val_accuracy: 0.8981\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9042\n",
      "Epoch 20: val_loss did not improve from 0.24311\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2463 - accuracy: 0.9047 - val_loss: 0.2433 - val_accuracy: 0.9006\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9059\n",
      "Epoch 21: val_loss did not improve from 0.24311\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2459 - accuracy: 0.9056 - val_loss: 0.2444 - val_accuracy: 0.8997\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9081\n",
      "Epoch 22: val_loss improved from 0.24311 to 0.24275, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2430 - accuracy: 0.9073 - val_loss: 0.2428 - val_accuracy: 0.8997\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9084\n",
      "Epoch 23: val_loss did not improve from 0.24275\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2437 - accuracy: 0.9085 - val_loss: 0.2455 - val_accuracy: 0.8981\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9069\n",
      "Epoch 24: val_loss did not improve from 0.24275\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2430 - accuracy: 0.9067 - val_loss: 0.2430 - val_accuracy: 0.8981\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9070\n",
      "Epoch 25: val_loss did not improve from 0.24275\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2432 - accuracy: 0.9070 - val_loss: 0.2432 - val_accuracy: 0.9006\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.9049\n",
      "Epoch 26: val_loss improved from 0.24275 to 0.24237, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2434 - accuracy: 0.9048 - val_loss: 0.2424 - val_accuracy: 0.9006\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9064\n",
      "Epoch 27: val_loss improved from 0.24237 to 0.24175, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2411 - accuracy: 0.9068 - val_loss: 0.2418 - val_accuracy: 0.8997\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2425 - accuracy: 0.9059\n",
      "Epoch 28: val_loss did not improve from 0.24175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9061 - val_loss: 0.2419 - val_accuracy: 0.9014\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.9086\n",
      "Epoch 29: val_loss improved from 0.24175 to 0.24163, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2390 - accuracy: 0.9086 - val_loss: 0.2416 - val_accuracy: 0.9006\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2395 - accuracy: 0.9079\n",
      "Epoch 30: val_loss improved from 0.24163 to 0.24062, saving model to model_adamax_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2395 - accuracy: 0.9079 - val_loss: 0.2406 - val_accuracy: 0.9023\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 9 - TP: 500, TN: 580, FP: 45, FN: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.5470 - accuracy: 0.7562\n",
      "Epoch 1: val_loss improved from inf to 0.36164, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 7s 12ms/step - loss: 0.5434 - accuracy: 0.7585 - val_loss: 0.3616 - val_accuracy: 0.8613\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8875\n",
      "Epoch 2: val_loss improved from 0.36164 to 0.31953, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2980 - accuracy: 0.8875 - val_loss: 0.3195 - val_accuracy: 0.8797\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.8950\n",
      "Epoch 3: val_loss improved from 0.31953 to 0.30578, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2750 - accuracy: 0.8946 - val_loss: 0.3058 - val_accuracy: 0.8814\n",
      "Epoch 4/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.8964\n",
      "Epoch 4: val_loss improved from 0.30578 to 0.29899, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2667 - accuracy: 0.8963 - val_loss: 0.2990 - val_accuracy: 0.8855\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.9004\n",
      "Epoch 5: val_loss improved from 0.29899 to 0.29384, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2602 - accuracy: 0.9009 - val_loss: 0.2938 - val_accuracy: 0.8864\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9002\n",
      "Epoch 6: val_loss improved from 0.29384 to 0.29123, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2587 - accuracy: 0.9004 - val_loss: 0.2912 - val_accuracy: 0.8881\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.9031\n",
      "Epoch 7: val_loss improved from 0.29123 to 0.28877, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2558 - accuracy: 0.9029 - val_loss: 0.2888 - val_accuracy: 0.8906\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.9037\n",
      "Epoch 8: val_loss improved from 0.28877 to 0.28643, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2535 - accuracy: 0.9039 - val_loss: 0.2864 - val_accuracy: 0.8906\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2528 - accuracy: 0.9031\n",
      "Epoch 9: val_loss improved from 0.28643 to 0.28493, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2525 - accuracy: 0.9032 - val_loss: 0.2849 - val_accuracy: 0.8881\n",
      "Epoch 10/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9013\n",
      "Epoch 10: val_loss improved from 0.28493 to 0.28378, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2506 - accuracy: 0.9017 - val_loss: 0.2838 - val_accuracy: 0.8889\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.9056\n",
      "Epoch 11: val_loss improved from 0.28378 to 0.28260, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2491 - accuracy: 0.9060 - val_loss: 0.2826 - val_accuracy: 0.8889\n",
      "Epoch 12/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9051\n",
      "Epoch 12: val_loss improved from 0.28260 to 0.28094, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2489 - accuracy: 0.9051 - val_loss: 0.2809 - val_accuracy: 0.8897\n",
      "Epoch 13/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9051\n",
      "Epoch 13: val_loss improved from 0.28094 to 0.28089, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2474 - accuracy: 0.9051 - val_loss: 0.2809 - val_accuracy: 0.8922\n",
      "Epoch 14/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9061\n",
      "Epoch 14: val_loss improved from 0.28089 to 0.27923, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2465 - accuracy: 0.9061 - val_loss: 0.2792 - val_accuracy: 0.8906\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.9070\n",
      "Epoch 15: val_loss improved from 0.27923 to 0.27915, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2456 - accuracy: 0.9069 - val_loss: 0.2792 - val_accuracy: 0.8897\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.9053\n",
      "Epoch 16: val_loss improved from 0.27915 to 0.27737, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2457 - accuracy: 0.9049 - val_loss: 0.2774 - val_accuracy: 0.8889\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9065\n",
      "Epoch 17: val_loss improved from 0.27737 to 0.27699, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2440 - accuracy: 0.9064 - val_loss: 0.2770 - val_accuracy: 0.8914\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9060\n",
      "Epoch 18: val_loss did not improve from 0.27699\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.9060 - val_loss: 0.2776 - val_accuracy: 0.8922\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9074\n",
      "Epoch 19: val_loss improved from 0.27699 to 0.27637, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2422 - accuracy: 0.9070 - val_loss: 0.2764 - val_accuracy: 0.8906\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9080\n",
      "Epoch 20: val_loss improved from 0.27637 to 0.27554, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2408 - accuracy: 0.9079 - val_loss: 0.2755 - val_accuracy: 0.8906\n",
      "Epoch 21/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9077\n",
      "Epoch 21: val_loss improved from 0.27554 to 0.27476, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2415 - accuracy: 0.9073 - val_loss: 0.2748 - val_accuracy: 0.8897\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9080\n",
      "Epoch 22: val_loss did not improve from 0.27476\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9074 - val_loss: 0.2751 - val_accuracy: 0.8897\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9073\n",
      "Epoch 23: val_loss improved from 0.27476 to 0.27442, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2400 - accuracy: 0.9074 - val_loss: 0.2744 - val_accuracy: 0.8906\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9080\n",
      "Epoch 24: val_loss improved from 0.27442 to 0.27365, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2397 - accuracy: 0.9078 - val_loss: 0.2736 - val_accuracy: 0.8889\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.9092\n",
      "Epoch 25: val_loss did not improve from 0.27365\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2387 - accuracy: 0.9092 - val_loss: 0.2743 - val_accuracy: 0.8939\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9072\n",
      "Epoch 26: val_loss improved from 0.27365 to 0.27334, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2379 - accuracy: 0.9075 - val_loss: 0.2733 - val_accuracy: 0.8931\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9078\n",
      "Epoch 27: val_loss improved from 0.27334 to 0.27221, saving model to model_adamax_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2382 - accuracy: 0.9081 - val_loss: 0.2722 - val_accuracy: 0.8897\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2371 - accuracy: 0.9083\n",
      "Epoch 28: val_loss did not improve from 0.27221\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2374 - accuracy: 0.9082 - val_loss: 0.2730 - val_accuracy: 0.8889\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9086\n",
      "Epoch 29: val_loss did not improve from 0.27221\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2365 - accuracy: 0.9094 - val_loss: 0.2725 - val_accuracy: 0.8906\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9087\n",
      "Epoch 30: val_loss did not improve from 0.27221\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2359 - accuracy: 0.9092 - val_loss: 0.2723 - val_accuracy: 0.8881\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 10 - TP: 533, TN: 530, FP: 53, FN: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.3977 - accuracy: 0.8385\n",
      "Epoch 1: val_loss improved from inf to 0.30422, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.3973 - accuracy: 0.8387 - val_loss: 0.3042 - val_accuracy: 0.8798\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9018\n",
      "Epoch 2: val_loss improved from 0.30422 to 0.29169, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.9016 - val_loss: 0.2917 - val_accuracy: 0.8840\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.9045\n",
      "Epoch 3: val_loss improved from 0.29169 to 0.28524, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2502 - accuracy: 0.9046 - val_loss: 0.2852 - val_accuracy: 0.8773\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9070\n",
      "Epoch 4: val_loss improved from 0.28524 to 0.28136, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9070 - val_loss: 0.2814 - val_accuracy: 0.8806\n",
      "Epoch 5/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9091\n",
      "Epoch 5: val_loss improved from 0.28136 to 0.27919, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2408 - accuracy: 0.9092 - val_loss: 0.2792 - val_accuracy: 0.8865\n",
      "Epoch 6/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9101\n",
      "Epoch 6: val_loss did not improve from 0.27919\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2370 - accuracy: 0.9102 - val_loss: 0.2803 - val_accuracy: 0.8856\n",
      "Epoch 7/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2340 - accuracy: 0.9127\n",
      "Epoch 7: val_loss did not improve from 0.27919\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2345 - accuracy: 0.9121 - val_loss: 0.2808 - val_accuracy: 0.8907\n",
      "Epoch 8/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9109\n",
      "Epoch 8: val_loss improved from 0.27919 to 0.27854, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2332 - accuracy: 0.9110 - val_loss: 0.2785 - val_accuracy: 0.8873\n",
      "Epoch 9/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9129\n",
      "Epoch 9: val_loss did not improve from 0.27854\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2305 - accuracy: 0.9124 - val_loss: 0.2795 - val_accuracy: 0.8873\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9135\n",
      "Epoch 10: val_loss improved from 0.27854 to 0.27610, saving model to model_adam_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2271 - accuracy: 0.9131 - val_loss: 0.2761 - val_accuracy: 0.8856\n",
      "Epoch 11/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9148\n",
      "Epoch 11: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2253 - accuracy: 0.9145 - val_loss: 0.2852 - val_accuracy: 0.8898\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2234 - accuracy: 0.9145\n",
      "Epoch 12: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2234 - accuracy: 0.9144 - val_loss: 0.2802 - val_accuracy: 0.8823\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2212 - accuracy: 0.9153\n",
      "Epoch 13: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2208 - accuracy: 0.9152 - val_loss: 0.2910 - val_accuracy: 0.8765\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9167\n",
      "Epoch 14: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2201 - accuracy: 0.9164 - val_loss: 0.2790 - val_accuracy: 0.8898\n",
      "Epoch 15/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9154\n",
      "Epoch 15: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2196 - accuracy: 0.9158 - val_loss: 0.2807 - val_accuracy: 0.8923\n",
      "Epoch 16/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2157 - accuracy: 0.9178\n",
      "Epoch 16: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2159 - accuracy: 0.9178 - val_loss: 0.2825 - val_accuracy: 0.8848\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2144 - accuracy: 0.9183\n",
      "Epoch 17: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2143 - accuracy: 0.9184 - val_loss: 0.2814 - val_accuracy: 0.8873\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9177\n",
      "Epoch 18: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2111 - accuracy: 0.9177 - val_loss: 0.2857 - val_accuracy: 0.8840\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9186\n",
      "Epoch 19: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2102 - accuracy: 0.9188 - val_loss: 0.2830 - val_accuracy: 0.8898\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9207\n",
      "Epoch 20: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2082 - accuracy: 0.9206 - val_loss: 0.2883 - val_accuracy: 0.8957\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9209\n",
      "Epoch 21: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2056 - accuracy: 0.9208 - val_loss: 0.2833 - val_accuracy: 0.8957\n",
      "Epoch 22/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2056 - accuracy: 0.9207\n",
      "Epoch 22: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2055 - accuracy: 0.9207 - val_loss: 0.2921 - val_accuracy: 0.8965\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9205\n",
      "Epoch 23: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2061 - accuracy: 0.9205 - val_loss: 0.2896 - val_accuracy: 0.8865\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2056 - accuracy: 0.9205\n",
      "Epoch 24: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2048 - accuracy: 0.9208 - val_loss: 0.2853 - val_accuracy: 0.8923\n",
      "Epoch 25/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9205\n",
      "Epoch 25: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2028 - accuracy: 0.9207 - val_loss: 0.2860 - val_accuracy: 0.8940\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9214\n",
      "Epoch 26: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2026 - accuracy: 0.9218 - val_loss: 0.2842 - val_accuracy: 0.8923\n",
      "Epoch 27/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9209\n",
      "Epoch 27: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2013 - accuracy: 0.9210 - val_loss: 0.3010 - val_accuracy: 0.8848\n",
      "Epoch 28/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9233\n",
      "Epoch 28: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1987 - accuracy: 0.9231 - val_loss: 0.2975 - val_accuracy: 0.8965\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9233\n",
      "Epoch 29: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1976 - accuracy: 0.9230 - val_loss: 0.2944 - val_accuracy: 0.8932\n",
      "Epoch 30/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9251\n",
      "Epoch 30: val_loss did not improve from 0.27610\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1956 - accuracy: 0.9249 - val_loss: 0.2982 - val_accuracy: 0.8973\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 1 - TP: 525, TN: 550, FP: 40, FN: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8674\n",
      "Epoch 1: val_loss improved from inf to 0.24113, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.3113 - accuracy: 0.8675 - val_loss: 0.2411 - val_accuracy: 0.9015\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2558 - accuracy: 0.9030\n",
      "Epoch 2: val_loss improved from 0.24113 to 0.23062, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.9032 - val_loss: 0.2306 - val_accuracy: 0.9174\n",
      "Epoch 3/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2476 - accuracy: 0.9036\n",
      "Epoch 3: val_loss improved from 0.23062 to 0.22719, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.9033 - val_loss: 0.2272 - val_accuracy: 0.9182\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9076\n",
      "Epoch 4: val_loss did not improve from 0.22719\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2417 - accuracy: 0.9079 - val_loss: 0.2284 - val_accuracy: 0.9190\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.9079\n",
      "Epoch 5: val_loss improved from 0.22719 to 0.22294, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9079 - val_loss: 0.2229 - val_accuracy: 0.9207\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.9087\n",
      "Epoch 6: val_loss did not improve from 0.22294\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2342 - accuracy: 0.9088 - val_loss: 0.2229 - val_accuracy: 0.9207\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9093\n",
      "Epoch 7: val_loss improved from 0.22294 to 0.22008, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2322 - accuracy: 0.9091 - val_loss: 0.2201 - val_accuracy: 0.9207\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9109\n",
      "Epoch 8: val_loss did not improve from 0.22008\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2287 - accuracy: 0.9109 - val_loss: 0.2207 - val_accuracy: 0.9174\n",
      "Epoch 9/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9116\n",
      "Epoch 9: val_loss improved from 0.22008 to 0.21887, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2273 - accuracy: 0.9115 - val_loss: 0.2189 - val_accuracy: 0.9182\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2232 - accuracy: 0.9130\n",
      "Epoch 10: val_loss improved from 0.21887 to 0.21838, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2227 - accuracy: 0.9132 - val_loss: 0.2184 - val_accuracy: 0.9240\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9127\n",
      "Epoch 11: val_loss did not improve from 0.21838\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2216 - accuracy: 0.9127 - val_loss: 0.2190 - val_accuracy: 0.9207\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9142\n",
      "Epoch 12: val_loss improved from 0.21838 to 0.21778, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2181 - accuracy: 0.9144 - val_loss: 0.2178 - val_accuracy: 0.9240\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9163\n",
      "Epoch 13: val_loss did not improve from 0.21778\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2174 - accuracy: 0.9166 - val_loss: 0.2199 - val_accuracy: 0.9190\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9146\n",
      "Epoch 14: val_loss improved from 0.21778 to 0.21401, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2150 - accuracy: 0.9146 - val_loss: 0.2140 - val_accuracy: 0.9249\n",
      "Epoch 15/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2130 - accuracy: 0.9178\n",
      "Epoch 15: val_loss did not improve from 0.21401\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2131 - accuracy: 0.9176 - val_loss: 0.2169 - val_accuracy: 0.9207\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9168\n",
      "Epoch 16: val_loss improved from 0.21401 to 0.21349, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2127 - accuracy: 0.9166 - val_loss: 0.2135 - val_accuracy: 0.9232\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9173\n",
      "Epoch 17: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2091 - accuracy: 0.9174 - val_loss: 0.2159 - val_accuracy: 0.9224\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9182\n",
      "Epoch 18: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2081 - accuracy: 0.9181 - val_loss: 0.2156 - val_accuracy: 0.9215\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9177\n",
      "Epoch 19: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2075 - accuracy: 0.9182 - val_loss: 0.2152 - val_accuracy: 0.9249\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9206\n",
      "Epoch 20: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2063 - accuracy: 0.9204 - val_loss: 0.2136 - val_accuracy: 0.9265\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2064 - accuracy: 0.9177\n",
      "Epoch 21: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2058 - accuracy: 0.9181 - val_loss: 0.2137 - val_accuracy: 0.9215\n",
      "Epoch 22/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9208\n",
      "Epoch 22: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2031 - accuracy: 0.9212 - val_loss: 0.2195 - val_accuracy: 0.9149\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9203\n",
      "Epoch 23: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2030 - accuracy: 0.9202 - val_loss: 0.2149 - val_accuracy: 0.9240\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9206\n",
      "Epoch 24: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2015 - accuracy: 0.9204 - val_loss: 0.2151 - val_accuracy: 0.9249\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9209\n",
      "Epoch 25: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1986 - accuracy: 0.9214 - val_loss: 0.2146 - val_accuracy: 0.9174\n",
      "Epoch 26/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9225\n",
      "Epoch 26: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1982 - accuracy: 0.9225 - val_loss: 0.2167 - val_accuracy: 0.9215\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9208\n",
      "Epoch 27: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1969 - accuracy: 0.9209 - val_loss: 0.2140 - val_accuracy: 0.9249\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9220\n",
      "Epoch 28: val_loss did not improve from 0.21349\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1972 - accuracy: 0.9221 - val_loss: 0.2146 - val_accuracy: 0.9174\n",
      "Epoch 29/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9229\n",
      "Epoch 29: val_loss improved from 0.21349 to 0.21312, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.1970 - accuracy: 0.9230 - val_loss: 0.2131 - val_accuracy: 0.9199\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1953 - accuracy: 0.9239\n",
      "Epoch 30: val_loss improved from 0.21312 to 0.21249, saving model to model_adam_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.1950 - accuracy: 0.9241 - val_loss: 0.2125 - val_accuracy: 0.9232\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 2 - TP: 536, TN: 570, FP: 29, FN: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8859\n",
      "Epoch 1: val_loss improved from inf to 0.24867, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.3073 - accuracy: 0.8857 - val_loss: 0.2487 - val_accuracy: 0.9107\n",
      "Epoch 2/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9025\n",
      "Epoch 2: val_loss improved from 0.24867 to 0.24232, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2523 - accuracy: 0.9024 - val_loss: 0.2423 - val_accuracy: 0.9140\n",
      "Epoch 3/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9059\n",
      "Epoch 3: val_loss improved from 0.24232 to 0.23958, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2449 - accuracy: 0.9061 - val_loss: 0.2396 - val_accuracy: 0.9140\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9063\n",
      "Epoch 4: val_loss improved from 0.23958 to 0.23897, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2412 - accuracy: 0.9066 - val_loss: 0.2390 - val_accuracy: 0.9132\n",
      "Epoch 5/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2364 - accuracy: 0.9085\n",
      "Epoch 5: val_loss improved from 0.23897 to 0.23677, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2361 - accuracy: 0.9076 - val_loss: 0.2368 - val_accuracy: 0.9132\n",
      "Epoch 6/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2355 - accuracy: 0.9082\n",
      "Epoch 6: val_loss improved from 0.23677 to 0.23650, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2352 - accuracy: 0.9084 - val_loss: 0.2365 - val_accuracy: 0.9115\n",
      "Epoch 7/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.9086\n",
      "Epoch 7: val_loss improved from 0.23650 to 0.23303, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2316 - accuracy: 0.9090 - val_loss: 0.2330 - val_accuracy: 0.9149\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9106\n",
      "Epoch 8: val_loss did not improve from 0.23303\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2283 - accuracy: 0.9106 - val_loss: 0.2350 - val_accuracy: 0.9115\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9112\n",
      "Epoch 9: val_loss improved from 0.23303 to 0.23120, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2248 - accuracy: 0.9112 - val_loss: 0.2312 - val_accuracy: 0.9157\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9120\n",
      "Epoch 10: val_loss improved from 0.23120 to 0.23057, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.9120 - val_loss: 0.2306 - val_accuracy: 0.9132\n",
      "Epoch 11/30\n",
      "162/169 [===========================>..] - ETA: 0s - loss: 0.2206 - accuracy: 0.9132\n",
      "Epoch 11: val_loss improved from 0.23057 to 0.22995, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2204 - accuracy: 0.9132 - val_loss: 0.2300 - val_accuracy: 0.9165\n",
      "Epoch 12/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2158 - accuracy: 0.9154\n",
      "Epoch 12: val_loss improved from 0.22995 to 0.22478, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2180 - accuracy: 0.9144 - val_loss: 0.2248 - val_accuracy: 0.9132\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.9176\n",
      "Epoch 13: val_loss did not improve from 0.22478\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2155 - accuracy: 0.9174 - val_loss: 0.2273 - val_accuracy: 0.9115\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9166\n",
      "Epoch 14: val_loss improved from 0.22478 to 0.22466, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2143 - accuracy: 0.9165 - val_loss: 0.2247 - val_accuracy: 0.9149\n",
      "Epoch 15/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9161\n",
      "Epoch 15: val_loss did not improve from 0.22466\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2123 - accuracy: 0.9160 - val_loss: 0.2264 - val_accuracy: 0.9157\n",
      "Epoch 16/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9179\n",
      "Epoch 16: val_loss did not improve from 0.22466\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2109 - accuracy: 0.9179 - val_loss: 0.2308 - val_accuracy: 0.9140\n",
      "Epoch 17/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9164\n",
      "Epoch 17: val_loss improved from 0.22466 to 0.22169, saving model to model_adam_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2103 - accuracy: 0.9163 - val_loss: 0.2217 - val_accuracy: 0.9157\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9194\n",
      "Epoch 18: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2088 - accuracy: 0.9190 - val_loss: 0.2238 - val_accuracy: 0.9140\n",
      "Epoch 19/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9187\n",
      "Epoch 19: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2069 - accuracy: 0.9187 - val_loss: 0.2286 - val_accuracy: 0.9107\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9193\n",
      "Epoch 20: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2058 - accuracy: 0.9196 - val_loss: 0.2258 - val_accuracy: 0.9140\n",
      "Epoch 21/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2062 - accuracy: 0.9178\n",
      "Epoch 21: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2064 - accuracy: 0.9178 - val_loss: 0.2246 - val_accuracy: 0.9165\n",
      "Epoch 22/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9207\n",
      "Epoch 22: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2020 - accuracy: 0.9204 - val_loss: 0.2281 - val_accuracy: 0.9140\n",
      "Epoch 23/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.9208\n",
      "Epoch 23: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2009 - accuracy: 0.9208 - val_loss: 0.2285 - val_accuracy: 0.9107\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9221\n",
      "Epoch 24: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1998 - accuracy: 0.9217 - val_loss: 0.2290 - val_accuracy: 0.9140\n",
      "Epoch 25/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9198\n",
      "Epoch 25: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1996 - accuracy: 0.9199 - val_loss: 0.2256 - val_accuracy: 0.9157\n",
      "Epoch 26/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9210\n",
      "Epoch 26: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1983 - accuracy: 0.9209 - val_loss: 0.2361 - val_accuracy: 0.9107\n",
      "Epoch 27/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1967 - accuracy: 0.9217\n",
      "Epoch 27: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1965 - accuracy: 0.9218 - val_loss: 0.2276 - val_accuracy: 0.9149\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.9228\n",
      "Epoch 28: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1979 - accuracy: 0.9228 - val_loss: 0.2417 - val_accuracy: 0.9098\n",
      "Epoch 29/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.1942 - accuracy: 0.9225\n",
      "Epoch 29: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.1948 - accuracy: 0.9221 - val_loss: 0.2287 - val_accuracy: 0.9165\n",
      "Epoch 30/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1945 - accuracy: 0.9227\n",
      "Epoch 30: val_loss did not improve from 0.22169\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1946 - accuracy: 0.9228 - val_loss: 0.2277 - val_accuracy: 0.9132\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 3 - TP: 533, TN: 561, FP: 41, FN: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8848\n",
      "Epoch 1: val_loss improved from inf to 0.25370, saving model to model_adam_fold4.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.3083 - accuracy: 0.8848 - val_loss: 0.2537 - val_accuracy: 0.9015\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9030\n",
      "Epoch 2: val_loss improved from 0.25370 to 0.23968, saving model to model_adam_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2530 - accuracy: 0.9036 - val_loss: 0.2397 - val_accuracy: 0.9065\n",
      "Epoch 3/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9077\n",
      "Epoch 3: val_loss did not improve from 0.23968\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2445 - accuracy: 0.9077 - val_loss: 0.2417 - val_accuracy: 0.9032\n",
      "Epoch 4/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9077\n",
      "Epoch 4: val_loss improved from 0.23968 to 0.23753, saving model to model_adam_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2410 - accuracy: 0.9071 - val_loss: 0.2375 - val_accuracy: 0.9065\n",
      "Epoch 5/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9093\n",
      "Epoch 5: val_loss did not improve from 0.23753\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2378 - accuracy: 0.9093 - val_loss: 0.2383 - val_accuracy: 0.9057\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9109\n",
      "Epoch 6: val_loss improved from 0.23753 to 0.23342, saving model to model_adam_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2325 - accuracy: 0.9108 - val_loss: 0.2334 - val_accuracy: 0.9090\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2304 - accuracy: 0.9111\n",
      "Epoch 7: val_loss did not improve from 0.23342\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2295 - accuracy: 0.9118 - val_loss: 0.2416 - val_accuracy: 0.9032\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9128\n",
      "Epoch 8: val_loss improved from 0.23342 to 0.23206, saving model to model_adam_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2275 - accuracy: 0.9130 - val_loss: 0.2321 - val_accuracy: 0.9082\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.9145\n",
      "Epoch 9: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2239 - accuracy: 0.9146 - val_loss: 0.2342 - val_accuracy: 0.9090\n",
      "Epoch 10/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9144\n",
      "Epoch 10: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2216 - accuracy: 0.9145 - val_loss: 0.2359 - val_accuracy: 0.9065\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9150\n",
      "Epoch 11: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2192 - accuracy: 0.9148 - val_loss: 0.2359 - val_accuracy: 0.9073\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9150\n",
      "Epoch 12: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2182 - accuracy: 0.9153 - val_loss: 0.2405 - val_accuracy: 0.9073\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9160\n",
      "Epoch 13: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2156 - accuracy: 0.9162 - val_loss: 0.2442 - val_accuracy: 0.9098\n",
      "Epoch 14/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9155\n",
      "Epoch 14: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2156 - accuracy: 0.9158 - val_loss: 0.2384 - val_accuracy: 0.9115\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9189\n",
      "Epoch 15: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2118 - accuracy: 0.9185 - val_loss: 0.2368 - val_accuracy: 0.9132\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9192\n",
      "Epoch 16: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2115 - accuracy: 0.9190 - val_loss: 0.2384 - val_accuracy: 0.9107\n",
      "Epoch 17/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.2090 - accuracy: 0.9178\n",
      "Epoch 17: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2096 - accuracy: 0.9173 - val_loss: 0.2378 - val_accuracy: 0.9057\n",
      "Epoch 18/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9183\n",
      "Epoch 18: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2085 - accuracy: 0.9183 - val_loss: 0.2373 - val_accuracy: 0.9132\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2075 - accuracy: 0.9182\n",
      "Epoch 19: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2076 - accuracy: 0.9183 - val_loss: 0.2395 - val_accuracy: 0.9132\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9188\n",
      "Epoch 20: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2058 - accuracy: 0.9187 - val_loss: 0.2350 - val_accuracy: 0.9107\n",
      "Epoch 21/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9193\n",
      "Epoch 21: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2040 - accuracy: 0.9194 - val_loss: 0.2408 - val_accuracy: 0.9115\n",
      "Epoch 22/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9218\n",
      "Epoch 22: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2021 - accuracy: 0.9219 - val_loss: 0.2381 - val_accuracy: 0.9132\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9219\n",
      "Epoch 23: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2006 - accuracy: 0.9223 - val_loss: 0.2411 - val_accuracy: 0.9149\n",
      "Epoch 24/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.9218\n",
      "Epoch 24: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1992 - accuracy: 0.9221 - val_loss: 0.2420 - val_accuracy: 0.9107\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9201\n",
      "Epoch 25: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2008 - accuracy: 0.9203 - val_loss: 0.2442 - val_accuracy: 0.9115\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9220\n",
      "Epoch 26: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1978 - accuracy: 0.9220 - val_loss: 0.2431 - val_accuracy: 0.9149\n",
      "Epoch 27/30\n",
      "162/169 [===========================>..] - ETA: 0s - loss: 0.1971 - accuracy: 0.9225\n",
      "Epoch 27: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.1960 - accuracy: 0.9230 - val_loss: 0.2399 - val_accuracy: 0.9132\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9209\n",
      "Epoch 28: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1963 - accuracy: 0.9210 - val_loss: 0.2397 - val_accuracy: 0.9124\n",
      "Epoch 29/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9227\n",
      "Epoch 29: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1962 - accuracy: 0.9227 - val_loss: 0.2393 - val_accuracy: 0.9140\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9232\n",
      "Epoch 30: val_loss did not improve from 0.23206\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1925 - accuracy: 0.9232 - val_loss: 0.2449 - val_accuracy: 0.9182\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 4 - TP: 531, TN: 569, FP: 29, FN: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8886\n",
      "Epoch 1: val_loss improved from inf to 0.25154, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 7s 11ms/step - loss: 0.3034 - accuracy: 0.8887 - val_loss: 0.2515 - val_accuracy: 0.8948\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.9041\n",
      "Epoch 2: val_loss improved from 0.25154 to 0.24398, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2517 - accuracy: 0.9040 - val_loss: 0.2440 - val_accuracy: 0.8998\n",
      "Epoch 3/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9082\n",
      "Epoch 3: val_loss improved from 0.24398 to 0.24175, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2453 - accuracy: 0.9082 - val_loss: 0.2417 - val_accuracy: 0.9007\n",
      "Epoch 4/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9109\n",
      "Epoch 4: val_loss improved from 0.24175 to 0.23777, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2396 - accuracy: 0.9105 - val_loss: 0.2378 - val_accuracy: 0.9032\n",
      "Epoch 5/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9100\n",
      "Epoch 5: val_loss did not improve from 0.23777\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2375 - accuracy: 0.9095 - val_loss: 0.2378 - val_accuracy: 0.9048\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9111\n",
      "Epoch 6: val_loss improved from 0.23777 to 0.23675, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2339 - accuracy: 0.9108 - val_loss: 0.2368 - val_accuracy: 0.9082\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9136\n",
      "Epoch 7: val_loss improved from 0.23675 to 0.23403, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2305 - accuracy: 0.9138 - val_loss: 0.2340 - val_accuracy: 0.9057\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9135\n",
      "Epoch 8: val_loss did not improve from 0.23403\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2271 - accuracy: 0.9135 - val_loss: 0.2361 - val_accuracy: 0.9015\n",
      "Epoch 9/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2255 - accuracy: 0.9145\n",
      "Epoch 9: val_loss did not improve from 0.23403\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2255 - accuracy: 0.9145 - val_loss: 0.2353 - val_accuracy: 0.9057\n",
      "Epoch 10/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9148\n",
      "Epoch 10: val_loss improved from 0.23403 to 0.23267, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2251 - accuracy: 0.9147 - val_loss: 0.2327 - val_accuracy: 0.9057\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2217 - accuracy: 0.9159\n",
      "Epoch 11: val_loss did not improve from 0.23267\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2204 - accuracy: 0.9162 - val_loss: 0.2338 - val_accuracy: 0.9048\n",
      "Epoch 12/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9173\n",
      "Epoch 12: val_loss improved from 0.23267 to 0.23147, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2176 - accuracy: 0.9169 - val_loss: 0.2315 - val_accuracy: 0.9098\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2143 - accuracy: 0.9182\n",
      "Epoch 13: val_loss did not improve from 0.23147\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2152 - accuracy: 0.9179 - val_loss: 0.2324 - val_accuracy: 0.9107\n",
      "Epoch 14/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9175\n",
      "Epoch 14: val_loss did not improve from 0.23147\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2142 - accuracy: 0.9178 - val_loss: 0.2324 - val_accuracy: 0.9065\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9177\n",
      "Epoch 15: val_loss did not improve from 0.23147\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2124 - accuracy: 0.9178 - val_loss: 0.2349 - val_accuracy: 0.9115\n",
      "Epoch 16/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2122 - accuracy: 0.9182\n",
      "Epoch 16: val_loss improved from 0.23147 to 0.23133, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2117 - accuracy: 0.9183 - val_loss: 0.2313 - val_accuracy: 0.9132\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9214\n",
      "Epoch 17: val_loss did not improve from 0.23133\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2092 - accuracy: 0.9213 - val_loss: 0.2324 - val_accuracy: 0.9098\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2067 - accuracy: 0.9208\n",
      "Epoch 18: val_loss improved from 0.23133 to 0.23050, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2066 - accuracy: 0.9208 - val_loss: 0.2305 - val_accuracy: 0.9107\n",
      "Epoch 19/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2065 - accuracy: 0.9204\n",
      "Epoch 19: val_loss did not improve from 0.23050\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2065 - accuracy: 0.9205 - val_loss: 0.2309 - val_accuracy: 0.9149\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9203\n",
      "Epoch 20: val_loss did not improve from 0.23050\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2050 - accuracy: 0.9201 - val_loss: 0.2325 - val_accuracy: 0.9073\n",
      "Epoch 21/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2041 - accuracy: 0.9205\n",
      "Epoch 21: val_loss did not improve from 0.23050\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2038 - accuracy: 0.9207 - val_loss: 0.2369 - val_accuracy: 0.9132\n",
      "Epoch 22/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9224\n",
      "Epoch 22: val_loss improved from 0.23050 to 0.22935, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2033 - accuracy: 0.9226 - val_loss: 0.2293 - val_accuracy: 0.9140\n",
      "Epoch 23/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2012 - accuracy: 0.9220\n",
      "Epoch 23: val_loss improved from 0.22935 to 0.22788, saving model to model_adam_fold5.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2013 - accuracy: 0.9220 - val_loss: 0.2279 - val_accuracy: 0.9174\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9218\n",
      "Epoch 24: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1999 - accuracy: 0.9220 - val_loss: 0.2325 - val_accuracy: 0.9098\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9223\n",
      "Epoch 25: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1986 - accuracy: 0.9220 - val_loss: 0.2333 - val_accuracy: 0.9157\n",
      "Epoch 26/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9233\n",
      "Epoch 26: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1977 - accuracy: 0.9236 - val_loss: 0.2372 - val_accuracy: 0.9132\n",
      "Epoch 27/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9231\n",
      "Epoch 27: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1972 - accuracy: 0.9231 - val_loss: 0.2339 - val_accuracy: 0.9165\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1963 - accuracy: 0.9239\n",
      "Epoch 28: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1967 - accuracy: 0.9238 - val_loss: 0.2387 - val_accuracy: 0.9190\n",
      "Epoch 29/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9249\n",
      "Epoch 29: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1944 - accuracy: 0.9247 - val_loss: 0.2379 - val_accuracy: 0.9124\n",
      "Epoch 30/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9247\n",
      "Epoch 30: val_loss did not improve from 0.22788\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1942 - accuracy: 0.9244 - val_loss: 0.2378 - val_accuracy: 0.9182\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 5 - TP: 568, TN: 532, FP: 31, FN: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8675\n",
      "Epoch 1: val_loss improved from inf to 0.26041, saving model to model_adam_fold6.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.3045 - accuracy: 0.8675 - val_loss: 0.2604 - val_accuracy: 0.9014\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9069\n",
      "Epoch 2: val_loss improved from 0.26041 to 0.25353, saving model to model_adam_fold6.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2502 - accuracy: 0.9060 - val_loss: 0.2535 - val_accuracy: 0.9031\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9088\n",
      "Epoch 3: val_loss improved from 0.25353 to 0.25301, saving model to model_adam_fold6.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2443 - accuracy: 0.9088 - val_loss: 0.2530 - val_accuracy: 0.8964\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9074\n",
      "Epoch 4: val_loss improved from 0.25301 to 0.25216, saving model to model_adam_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2404 - accuracy: 0.9073 - val_loss: 0.2522 - val_accuracy: 0.8964\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9101\n",
      "Epoch 5: val_loss did not improve from 0.25216\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2358 - accuracy: 0.9099 - val_loss: 0.2539 - val_accuracy: 0.8989\n",
      "Epoch 6/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9100\n",
      "Epoch 6: val_loss did not improve from 0.25216\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2330 - accuracy: 0.9099 - val_loss: 0.2523 - val_accuracy: 0.9048\n",
      "Epoch 7/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9110\n",
      "Epoch 7: val_loss improved from 0.25216 to 0.25126, saving model to model_adam_fold6.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2296 - accuracy: 0.9112 - val_loss: 0.2513 - val_accuracy: 0.9089\n",
      "Epoch 8/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9136\n",
      "Epoch 8: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2271 - accuracy: 0.9138 - val_loss: 0.2544 - val_accuracy: 0.9023\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9111\n",
      "Epoch 9: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2244 - accuracy: 0.9112 - val_loss: 0.2549 - val_accuracy: 0.9064\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9149\n",
      "Epoch 10: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2220 - accuracy: 0.9148 - val_loss: 0.2540 - val_accuracy: 0.9031\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2180 - accuracy: 0.9155\n",
      "Epoch 11: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2182 - accuracy: 0.9155 - val_loss: 0.2688 - val_accuracy: 0.9006\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.9153\n",
      "Epoch 12: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2177 - accuracy: 0.9154 - val_loss: 0.2551 - val_accuracy: 0.9064\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9160\n",
      "Epoch 13: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2163 - accuracy: 0.9162 - val_loss: 0.2519 - val_accuracy: 0.9064\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy: 0.9165\n",
      "Epoch 14: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2137 - accuracy: 0.9165 - val_loss: 0.2626 - val_accuracy: 0.9031\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9180\n",
      "Epoch 15: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2117 - accuracy: 0.9173 - val_loss: 0.2584 - val_accuracy: 0.9031\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9178\n",
      "Epoch 16: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2097 - accuracy: 0.9180 - val_loss: 0.2562 - val_accuracy: 0.9064\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2086 - accuracy: 0.9185\n",
      "Epoch 17: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2085 - accuracy: 0.9185 - val_loss: 0.2561 - val_accuracy: 0.9089\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2064 - accuracy: 0.9185\n",
      "Epoch 18: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2064 - accuracy: 0.9185 - val_loss: 0.2573 - val_accuracy: 0.9064\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9183\n",
      "Epoch 19: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2062 - accuracy: 0.9188 - val_loss: 0.2620 - val_accuracy: 0.9056\n",
      "Epoch 20/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2045 - accuracy: 0.9203\n",
      "Epoch 20: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2037 - accuracy: 0.9207 - val_loss: 0.2610 - val_accuracy: 0.9048\n",
      "Epoch 21/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2023 - accuracy: 0.9209\n",
      "Epoch 21: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2024 - accuracy: 0.9208 - val_loss: 0.2582 - val_accuracy: 0.9081\n",
      "Epoch 22/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2018 - accuracy: 0.9211\n",
      "Epoch 22: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2014 - accuracy: 0.9210 - val_loss: 0.2611 - val_accuracy: 0.9031\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9222\n",
      "Epoch 23: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2011 - accuracy: 0.9225 - val_loss: 0.2617 - val_accuracy: 0.9014\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9220\n",
      "Epoch 24: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1993 - accuracy: 0.9215 - val_loss: 0.2593 - val_accuracy: 0.9064\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9226\n",
      "Epoch 25: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1975 - accuracy: 0.9222 - val_loss: 0.2674 - val_accuracy: 0.9048\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9233\n",
      "Epoch 26: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1967 - accuracy: 0.9237 - val_loss: 0.2772 - val_accuracy: 0.9048\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9221\n",
      "Epoch 27: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1967 - accuracy: 0.9224 - val_loss: 0.2731 - val_accuracy: 0.9089\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1928 - accuracy: 0.9243\n",
      "Epoch 28: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1934 - accuracy: 0.9239 - val_loss: 0.2689 - val_accuracy: 0.9073\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9246\n",
      "Epoch 29: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1934 - accuracy: 0.9245 - val_loss: 0.2669 - val_accuracy: 0.9073\n",
      "Epoch 30/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9258\n",
      "Epoch 30: val_loss did not improve from 0.25126\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1916 - accuracy: 0.9258 - val_loss: 0.2707 - val_accuracy: 0.9048\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 6 - TP: 525, TN: 558, FP: 50, FN: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8724\n",
      "Epoch 1: val_loss improved from inf to 0.28017, saving model to model_adam_fold7.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.3026 - accuracy: 0.8725 - val_loss: 0.2802 - val_accuracy: 0.8872\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9035\n",
      "Epoch 2: val_loss improved from 0.28017 to 0.26758, saving model to model_adam_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2500 - accuracy: 0.9031 - val_loss: 0.2676 - val_accuracy: 0.9014\n",
      "Epoch 3/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9076\n",
      "Epoch 3: val_loss did not improve from 0.26758\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2433 - accuracy: 0.9081 - val_loss: 0.2732 - val_accuracy: 0.8972\n",
      "Epoch 4/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9093\n",
      "Epoch 4: val_loss improved from 0.26758 to 0.26560, saving model to model_adam_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2381 - accuracy: 0.9097 - val_loss: 0.2656 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9089\n",
      "Epoch 5: val_loss improved from 0.26560 to 0.26458, saving model to model_adam_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2349 - accuracy: 0.9088 - val_loss: 0.2646 - val_accuracy: 0.9014\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9107\n",
      "Epoch 6: val_loss improved from 0.26458 to 0.26412, saving model to model_adam_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2316 - accuracy: 0.9107 - val_loss: 0.2641 - val_accuracy: 0.9014\n",
      "Epoch 7/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9107\n",
      "Epoch 7: val_loss did not improve from 0.26412\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2291 - accuracy: 0.9113 - val_loss: 0.2663 - val_accuracy: 0.9006\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9143\n",
      "Epoch 8: val_loss improved from 0.26412 to 0.26175, saving model to model_adam_fold7.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2252 - accuracy: 0.9142 - val_loss: 0.2617 - val_accuracy: 0.9056\n",
      "Epoch 9/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9139\n",
      "Epoch 9: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2224 - accuracy: 0.9139 - val_loss: 0.2637 - val_accuracy: 0.9048\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9155\n",
      "Epoch 10: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2194 - accuracy: 0.9153 - val_loss: 0.2638 - val_accuracy: 0.9056\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9156\n",
      "Epoch 11: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2196 - accuracy: 0.9153 - val_loss: 0.2642 - val_accuracy: 0.9031\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9162\n",
      "Epoch 12: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2165 - accuracy: 0.9162 - val_loss: 0.2659 - val_accuracy: 0.9031\n",
      "Epoch 13/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9175\n",
      "Epoch 13: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2137 - accuracy: 0.9169 - val_loss: 0.2635 - val_accuracy: 0.9073\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2120 - accuracy: 0.9159\n",
      "Epoch 14: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2118 - accuracy: 0.9164 - val_loss: 0.2653 - val_accuracy: 0.9048\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9193\n",
      "Epoch 15: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2101 - accuracy: 0.9192 - val_loss: 0.2686 - val_accuracy: 0.9056\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9168\n",
      "Epoch 16: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2083 - accuracy: 0.9169 - val_loss: 0.2718 - val_accuracy: 0.9064\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9171\n",
      "Epoch 17: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2079 - accuracy: 0.9174 - val_loss: 0.2689 - val_accuracy: 0.9073\n",
      "Epoch 18/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9200\n",
      "Epoch 18: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2063 - accuracy: 0.9197 - val_loss: 0.2754 - val_accuracy: 0.9064\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9187\n",
      "Epoch 19: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2046 - accuracy: 0.9191 - val_loss: 0.2734 - val_accuracy: 0.9064\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.9211\n",
      "Epoch 20: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2033 - accuracy: 0.9210 - val_loss: 0.2743 - val_accuracy: 0.9081\n",
      "Epoch 21/30\n",
      "161/169 [===========================>..] - ETA: 0s - loss: 0.2028 - accuracy: 0.9210\n",
      "Epoch 21: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2017 - accuracy: 0.9210 - val_loss: 0.2743 - val_accuracy: 0.9081\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9222\n",
      "Epoch 22: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2006 - accuracy: 0.9218 - val_loss: 0.2772 - val_accuracy: 0.9089\n",
      "Epoch 23/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1978 - accuracy: 0.9220\n",
      "Epoch 23: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1984 - accuracy: 0.9214 - val_loss: 0.2729 - val_accuracy: 0.9106\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1959 - accuracy: 0.9210\n",
      "Epoch 24: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1960 - accuracy: 0.9211 - val_loss: 0.2805 - val_accuracy: 0.9081\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9203\n",
      "Epoch 25: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1968 - accuracy: 0.9206 - val_loss: 0.2755 - val_accuracy: 0.9081\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1953 - accuracy: 0.9232\n",
      "Epoch 26: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1960 - accuracy: 0.9229 - val_loss: 0.2833 - val_accuracy: 0.9089\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9222\n",
      "Epoch 27: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1958 - accuracy: 0.9228 - val_loss: 0.2754 - val_accuracy: 0.9114\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9239\n",
      "Epoch 28: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1921 - accuracy: 0.9239 - val_loss: 0.2764 - val_accuracy: 0.9106\n",
      "Epoch 29/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9234\n",
      "Epoch 29: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1913 - accuracy: 0.9234 - val_loss: 0.2809 - val_accuracy: 0.9123\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9242\n",
      "Epoch 30: val_loss did not improve from 0.26175\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1912 - accuracy: 0.9241 - val_loss: 0.2833 - val_accuracy: 0.9098\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 7 - TP: 506, TN: 583, FP: 41, FN: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8826\n",
      "Epoch 1: val_loss improved from inf to 0.27625, saving model to model_adam_fold8.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.3010 - accuracy: 0.8826 - val_loss: 0.2763 - val_accuracy: 0.9039\n",
      "Epoch 2/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2485 - accuracy: 0.9040\n",
      "Epoch 2: val_loss did not improve from 0.27625\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.9039 - val_loss: 0.2765 - val_accuracy: 0.9006\n",
      "Epoch 3/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2432 - accuracy: 0.9048\n",
      "Epoch 3: val_loss improved from 0.27625 to 0.27225, saving model to model_adam_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2427 - accuracy: 0.9054 - val_loss: 0.2722 - val_accuracy: 0.9023\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9088\n",
      "Epoch 4: val_loss did not improve from 0.27225\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2383 - accuracy: 0.9088 - val_loss: 0.2723 - val_accuracy: 0.9023\n",
      "Epoch 5/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9075\n",
      "Epoch 5: val_loss did not improve from 0.27225\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2346 - accuracy: 0.9076 - val_loss: 0.2738 - val_accuracy: 0.8956\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9089\n",
      "Epoch 6: val_loss improved from 0.27225 to 0.27186, saving model to model_adam_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2315 - accuracy: 0.9090 - val_loss: 0.2719 - val_accuracy: 0.9056\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9111\n",
      "Epoch 7: val_loss did not improve from 0.27186\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2278 - accuracy: 0.9111 - val_loss: 0.2753 - val_accuracy: 0.9064\n",
      "Epoch 8/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9110\n",
      "Epoch 8: val_loss did not improve from 0.27186\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2252 - accuracy: 0.9110 - val_loss: 0.2731 - val_accuracy: 0.9039\n",
      "Epoch 9/30\n",
      "162/169 [===========================>..] - ETA: 0s - loss: 0.2226 - accuracy: 0.9135\n",
      "Epoch 9: val_loss did not improve from 0.27186\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2231 - accuracy: 0.9135 - val_loss: 0.2749 - val_accuracy: 0.8997\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2208 - accuracy: 0.9135\n",
      "Epoch 10: val_loss improved from 0.27186 to 0.26992, saving model to model_adam_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2205 - accuracy: 0.9137 - val_loss: 0.2699 - val_accuracy: 0.9073\n",
      "Epoch 11/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2162 - accuracy: 0.9157\n",
      "Epoch 11: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2171 - accuracy: 0.9152 - val_loss: 0.2770 - val_accuracy: 0.8931\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9167\n",
      "Epoch 12: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2154 - accuracy: 0.9166 - val_loss: 0.2745 - val_accuracy: 0.9056\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9149\n",
      "Epoch 13: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2132 - accuracy: 0.9149 - val_loss: 0.2742 - val_accuracy: 0.8997\n",
      "Epoch 14/30\n",
      "160/169 [===========================>..] - ETA: 0s - loss: 0.2099 - accuracy: 0.9177\n",
      "Epoch 14: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2116 - accuracy: 0.9173 - val_loss: 0.2720 - val_accuracy: 0.9023\n",
      "Epoch 15/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2085 - accuracy: 0.9153\n",
      "Epoch 15: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2086 - accuracy: 0.9160 - val_loss: 0.2756 - val_accuracy: 0.9023\n",
      "Epoch 16/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9169\n",
      "Epoch 16: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2084 - accuracy: 0.9169 - val_loss: 0.2738 - val_accuracy: 0.9039\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9204\n",
      "Epoch 17: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2046 - accuracy: 0.9204 - val_loss: 0.2892 - val_accuracy: 0.8981\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9192\n",
      "Epoch 18: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2048 - accuracy: 0.9192 - val_loss: 0.2831 - val_accuracy: 0.9006\n",
      "Epoch 19/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9195\n",
      "Epoch 19: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2025 - accuracy: 0.9194 - val_loss: 0.2815 - val_accuracy: 0.8997\n",
      "Epoch 20/30\n",
      "161/169 [===========================>..] - ETA: 0s - loss: 0.2025 - accuracy: 0.9199\n",
      "Epoch 20: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2017 - accuracy: 0.9210 - val_loss: 0.2807 - val_accuracy: 0.9039\n",
      "Epoch 21/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9204\n",
      "Epoch 21: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2003 - accuracy: 0.9205 - val_loss: 0.2800 - val_accuracy: 0.9006\n",
      "Epoch 22/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9208\n",
      "Epoch 22: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1993 - accuracy: 0.9209 - val_loss: 0.2761 - val_accuracy: 0.9014\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9222\n",
      "Epoch 23: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1982 - accuracy: 0.9220 - val_loss: 0.2793 - val_accuracy: 0.9039\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.9214\n",
      "Epoch 24: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1969 - accuracy: 0.9214 - val_loss: 0.2783 - val_accuracy: 0.9014\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9217\n",
      "Epoch 25: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1963 - accuracy: 0.9217 - val_loss: 0.2860 - val_accuracy: 0.9023\n",
      "Epoch 26/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9219\n",
      "Epoch 26: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1951 - accuracy: 0.9218 - val_loss: 0.2827 - val_accuracy: 0.9056\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9223\n",
      "Epoch 27: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1925 - accuracy: 0.9223 - val_loss: 0.2848 - val_accuracy: 0.9023\n",
      "Epoch 28/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9220\n",
      "Epoch 28: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1937 - accuracy: 0.9222 - val_loss: 0.2919 - val_accuracy: 0.8981\n",
      "Epoch 29/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9249\n",
      "Epoch 29: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1903 - accuracy: 0.9249 - val_loss: 0.2894 - val_accuracy: 0.9048\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9224\n",
      "Epoch 30: val_loss did not improve from 0.26992\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1908 - accuracy: 0.9224 - val_loss: 0.2861 - val_accuracy: 0.9056\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 8 - TP: 533, TN: 551, FP: 41, FN: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8721\n",
      "Epoch 1: val_loss improved from inf to 0.27400, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.3055 - accuracy: 0.8721 - val_loss: 0.2740 - val_accuracy: 0.8889\n",
      "Epoch 2/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2475 - accuracy: 0.9058\n",
      "Epoch 2: val_loss did not improve from 0.27400\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.9057 - val_loss: 0.2771 - val_accuracy: 0.8881\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.9067\n",
      "Epoch 3: val_loss did not improve from 0.27400\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2409 - accuracy: 0.9070 - val_loss: 0.2741 - val_accuracy: 0.8897\n",
      "Epoch 4/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2361 - accuracy: 0.9090\n",
      "Epoch 4: val_loss improved from 0.27400 to 0.26942, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2370 - accuracy: 0.9086 - val_loss: 0.2694 - val_accuracy: 0.8914\n",
      "Epoch 5/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.2342 - accuracy: 0.9112\n",
      "Epoch 5: val_loss did not improve from 0.26942\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2336 - accuracy: 0.9113 - val_loss: 0.2708 - val_accuracy: 0.8889\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9126\n",
      "Epoch 6: val_loss improved from 0.26942 to 0.26891, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2302 - accuracy: 0.9126 - val_loss: 0.2689 - val_accuracy: 0.8981\n",
      "Epoch 7/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.2301 - accuracy: 0.9127\n",
      "Epoch 7: val_loss did not improve from 0.26891\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2285 - accuracy: 0.9130 - val_loss: 0.2698 - val_accuracy: 0.8906\n",
      "Epoch 8/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2250 - accuracy: 0.9136\n",
      "Epoch 8: val_loss improved from 0.26891 to 0.26784, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2250 - accuracy: 0.9136 - val_loss: 0.2678 - val_accuracy: 0.8956\n",
      "Epoch 9/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2227 - accuracy: 0.9153\n",
      "Epoch 9: val_loss improved from 0.26784 to 0.26489, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2224 - accuracy: 0.9157 - val_loss: 0.2649 - val_accuracy: 0.8914\n",
      "Epoch 10/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2200 - accuracy: 0.9160\n",
      "Epoch 10: val_loss improved from 0.26489 to 0.26277, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2203 - accuracy: 0.9159 - val_loss: 0.2628 - val_accuracy: 0.8972\n",
      "Epoch 11/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2191 - accuracy: 0.9140\n",
      "Epoch 11: val_loss improved from 0.26277 to 0.26269, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2192 - accuracy: 0.9141 - val_loss: 0.2627 - val_accuracy: 0.8964\n",
      "Epoch 12/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9169\n",
      "Epoch 12: val_loss did not improve from 0.26269\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2172 - accuracy: 0.9169 - val_loss: 0.2651 - val_accuracy: 0.8964\n",
      "Epoch 13/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.2135 - accuracy: 0.9170\n",
      "Epoch 13: val_loss did not improve from 0.26269\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2152 - accuracy: 0.9162 - val_loss: 0.2629 - val_accuracy: 0.8981\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9170\n",
      "Epoch 14: val_loss improved from 0.26269 to 0.25784, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2120 - accuracy: 0.9170 - val_loss: 0.2578 - val_accuracy: 0.8981\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9174\n",
      "Epoch 15: val_loss did not improve from 0.25784\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2108 - accuracy: 0.9176 - val_loss: 0.2608 - val_accuracy: 0.8989\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9184\n",
      "Epoch 16: val_loss did not improve from 0.25784\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2092 - accuracy: 0.9184 - val_loss: 0.2619 - val_accuracy: 0.8956\n",
      "Epoch 17/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.2093 - accuracy: 0.9175\n",
      "Epoch 17: val_loss did not improve from 0.25784\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2078 - accuracy: 0.9180 - val_loss: 0.2605 - val_accuracy: 0.8981\n",
      "Epoch 18/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.9193\n",
      "Epoch 18: val_loss did not improve from 0.25784\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2063 - accuracy: 0.9193 - val_loss: 0.2652 - val_accuracy: 0.9031\n",
      "Epoch 19/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9186\n",
      "Epoch 19: val_loss did not improve from 0.25784\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2056 - accuracy: 0.9187 - val_loss: 0.2704 - val_accuracy: 0.8956\n",
      "Epoch 20/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9198\n",
      "Epoch 20: val_loss improved from 0.25784 to 0.25678, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2041 - accuracy: 0.9198 - val_loss: 0.2568 - val_accuracy: 0.9014\n",
      "Epoch 21/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9211\n",
      "Epoch 21: val_loss improved from 0.25678 to 0.25573, saving model to model_adam_fold9.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2014 - accuracy: 0.9210 - val_loss: 0.2557 - val_accuracy: 0.8989\n",
      "Epoch 22/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.9212\n",
      "Epoch 22: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2008 - accuracy: 0.9211 - val_loss: 0.2661 - val_accuracy: 0.8989\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9223\n",
      "Epoch 23: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1983 - accuracy: 0.9222 - val_loss: 0.2620 - val_accuracy: 0.9006\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1967 - accuracy: 0.9229\n",
      "Epoch 24: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1971 - accuracy: 0.9226 - val_loss: 0.2660 - val_accuracy: 0.9023\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9228\n",
      "Epoch 25: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1968 - accuracy: 0.9226 - val_loss: 0.2603 - val_accuracy: 0.9039\n",
      "Epoch 26/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1951 - accuracy: 0.9232\n",
      "Epoch 26: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1952 - accuracy: 0.9232 - val_loss: 0.2630 - val_accuracy: 0.8981\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9236\n",
      "Epoch 27: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1942 - accuracy: 0.9235 - val_loss: 0.2663 - val_accuracy: 0.9014\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1935 - accuracy: 0.9236\n",
      "Epoch 28: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1937 - accuracy: 0.9236 - val_loss: 0.2576 - val_accuracy: 0.9006\n",
      "Epoch 29/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9243\n",
      "Epoch 29: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1924 - accuracy: 0.9245 - val_loss: 0.2618 - val_accuracy: 0.9014\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9244\n",
      "Epoch 30: val_loss did not improve from 0.25573\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1904 - accuracy: 0.9245 - val_loss: 0.2634 - val_accuracy: 0.9023\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 9 - TP: 515, TN: 565, FP: 31, FN: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8791\n",
      "Epoch 1: val_loss improved from inf to 0.24491, saving model to model_adam_fold10.hdf5\n",
      "169/169 [==============================] - 6s 12ms/step - loss: 0.3012 - accuracy: 0.8801 - val_loss: 0.2449 - val_accuracy: 0.9123\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9037\n",
      "Epoch 2: val_loss did not improve from 0.24491\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2525 - accuracy: 0.9035 - val_loss: 0.2458 - val_accuracy: 0.9140\n",
      "Epoch 3/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9072\n",
      "Epoch 3: val_loss improved from 0.24491 to 0.24129, saving model to model_adam_fold10.hdf5\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2445 - accuracy: 0.9072 - val_loss: 0.2413 - val_accuracy: 0.9156\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9075\n",
      "Epoch 4: val_loss did not improve from 0.24129\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2408 - accuracy: 0.9073 - val_loss: 0.2450 - val_accuracy: 0.9081\n",
      "Epoch 5/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9066\n",
      "Epoch 5: val_loss improved from 0.24129 to 0.24022, saving model to model_adam_fold10.hdf5\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2374 - accuracy: 0.9068 - val_loss: 0.2402 - val_accuracy: 0.9123\n",
      "Epoch 6/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9100\n",
      "Epoch 6: val_loss did not improve from 0.24022\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2348 - accuracy: 0.9100 - val_loss: 0.2446 - val_accuracy: 0.9006\n",
      "Epoch 7/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9110\n",
      "Epoch 7: val_loss improved from 0.24022 to 0.23799, saving model to model_adam_fold10.hdf5\n",
      "169/169 [==============================] - 1s 8ms/step - loss: 0.2325 - accuracy: 0.9105 - val_loss: 0.2380 - val_accuracy: 0.9131\n",
      "Epoch 8/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9105\n",
      "Epoch 8: val_loss did not improve from 0.23799\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2282 - accuracy: 0.9108 - val_loss: 0.2409 - val_accuracy: 0.9106\n",
      "Epoch 9/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.2233 - accuracy: 0.9125\n",
      "Epoch 9: val_loss improved from 0.23799 to 0.23794, saving model to model_adam_fold10.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.2241 - accuracy: 0.9122 - val_loss: 0.2379 - val_accuracy: 0.9106\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9147\n",
      "Epoch 10: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2220 - accuracy: 0.9148 - val_loss: 0.2399 - val_accuracy: 0.9106\n",
      "Epoch 11/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9149\n",
      "Epoch 11: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2204 - accuracy: 0.9147 - val_loss: 0.2421 - val_accuracy: 0.9064\n",
      "Epoch 12/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2171 - accuracy: 0.9153\n",
      "Epoch 12: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2170 - accuracy: 0.9153 - val_loss: 0.2386 - val_accuracy: 0.9114\n",
      "Epoch 13/30\n",
      "161/169 [===========================>..] - ETA: 0s - loss: 0.2140 - accuracy: 0.9152\n",
      "Epoch 13: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.2160 - accuracy: 0.9142 - val_loss: 0.2496 - val_accuracy: 0.9048\n",
      "Epoch 14/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9160\n",
      "Epoch 14: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2147 - accuracy: 0.9164 - val_loss: 0.2408 - val_accuracy: 0.9089\n",
      "Epoch 15/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.9160\n",
      "Epoch 15: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2122 - accuracy: 0.9162 - val_loss: 0.2397 - val_accuracy: 0.9131\n",
      "Epoch 16/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2106 - accuracy: 0.9172\n",
      "Epoch 16: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2107 - accuracy: 0.9168 - val_loss: 0.2411 - val_accuracy: 0.9165\n",
      "Epoch 17/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9169\n",
      "Epoch 17: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2098 - accuracy: 0.9170 - val_loss: 0.2481 - val_accuracy: 0.9089\n",
      "Epoch 18/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9190\n",
      "Epoch 18: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2078 - accuracy: 0.9188 - val_loss: 0.2405 - val_accuracy: 0.9098\n",
      "Epoch 19/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9189\n",
      "Epoch 19: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2061 - accuracy: 0.9196 - val_loss: 0.2417 - val_accuracy: 0.9106\n",
      "Epoch 20/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.9198\n",
      "Epoch 20: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2056 - accuracy: 0.9202 - val_loss: 0.2408 - val_accuracy: 0.9140\n",
      "Epoch 21/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9193\n",
      "Epoch 21: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2050 - accuracy: 0.9193 - val_loss: 0.2417 - val_accuracy: 0.9123\n",
      "Epoch 22/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9209\n",
      "Epoch 22: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2025 - accuracy: 0.9204 - val_loss: 0.2420 - val_accuracy: 0.9106\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.2011 - accuracy: 0.9201\n",
      "Epoch 23: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.2006 - accuracy: 0.9204 - val_loss: 0.2440 - val_accuracy: 0.9148\n",
      "Epoch 24/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1998 - accuracy: 0.9211\n",
      "Epoch 24: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1993 - accuracy: 0.9211 - val_loss: 0.2471 - val_accuracy: 0.9123\n",
      "Epoch 25/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9229\n",
      "Epoch 25: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1986 - accuracy: 0.9214 - val_loss: 0.2497 - val_accuracy: 0.9098\n",
      "Epoch 26/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9225\n",
      "Epoch 26: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1983 - accuracy: 0.9224 - val_loss: 0.2480 - val_accuracy: 0.9106\n",
      "Epoch 27/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9223\n",
      "Epoch 27: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1956 - accuracy: 0.9226 - val_loss: 0.2496 - val_accuracy: 0.9123\n",
      "Epoch 28/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9231\n",
      "Epoch 28: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1947 - accuracy: 0.9232 - val_loss: 0.2580 - val_accuracy: 0.9064\n",
      "Epoch 29/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9203\n",
      "Epoch 29: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1970 - accuracy: 0.9202 - val_loss: 0.2482 - val_accuracy: 0.9156\n",
      "Epoch 30/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9229\n",
      "Epoch 30: val_loss did not improve from 0.23794\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.1938 - accuracy: 0.9231 - val_loss: 0.2417 - val_accuracy: 0.9165\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 10 - TP: 531, TN: 566, FP: 37, FN: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5160\n",
      "Epoch 1: val_loss improved from inf to 0.69260, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 7s 10ms/step - loss: 0.6928 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5601\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.5463\n",
      "Epoch 2: val_loss improved from 0.69260 to 0.69236, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6925 - accuracy: 0.5463 - val_loss: 0.6924 - val_accuracy: 0.6002\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.5835\n",
      "Epoch 3: val_loss improved from 0.69236 to 0.69212, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6923 - accuracy: 0.5835 - val_loss: 0.6921 - val_accuracy: 0.6611\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.6083\n",
      "Epoch 4: val_loss improved from 0.69212 to 0.69188, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6921 - accuracy: 0.6083 - val_loss: 0.6919 - val_accuracy: 0.6945\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.6417\n",
      "Epoch 5: val_loss improved from 0.69188 to 0.69165, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6917 - accuracy: 0.6417 - val_loss: 0.6916 - val_accuracy: 0.7145\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.6578\n",
      "Epoch 6: val_loss improved from 0.69165 to 0.69140, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6915 - accuracy: 0.6578 - val_loss: 0.6914 - val_accuracy: 0.7270\n",
      "Epoch 7/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.6826\n",
      "Epoch 7: val_loss improved from 0.69140 to 0.69115, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.6825 - val_loss: 0.6912 - val_accuracy: 0.7412\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.7035\n",
      "Epoch 8: val_loss improved from 0.69115 to 0.69092, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6910 - accuracy: 0.7035 - val_loss: 0.6909 - val_accuracy: 0.7462\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.7213\n",
      "Epoch 9: val_loss improved from 0.69092 to 0.69066, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6908 - accuracy: 0.7213 - val_loss: 0.6907 - val_accuracy: 0.7621\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.6905 - accuracy: 0.7351\n",
      "Epoch 10: val_loss improved from 0.69066 to 0.69042, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.7353 - val_loss: 0.6904 - val_accuracy: 0.7763\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.7466\n",
      "Epoch 11: val_loss improved from 0.69042 to 0.69016, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6902 - accuracy: 0.7466 - val_loss: 0.6902 - val_accuracy: 0.7763\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.7566\n",
      "Epoch 12: val_loss improved from 0.69016 to 0.68991, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6900 - accuracy: 0.7566 - val_loss: 0.6899 - val_accuracy: 0.7805\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.7651\n",
      "Epoch 13: val_loss improved from 0.68991 to 0.68965, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6897 - accuracy: 0.7651 - val_loss: 0.6896 - val_accuracy: 0.7863\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.7691\n",
      "Epoch 14: val_loss improved from 0.68965 to 0.68939, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6895 - accuracy: 0.7691 - val_loss: 0.6894 - val_accuracy: 0.7938\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.7743\n",
      "Epoch 15: val_loss improved from 0.68939 to 0.68912, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6892 - accuracy: 0.7743 - val_loss: 0.6891 - val_accuracy: 0.7938\n",
      "Epoch 16/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.7735\n",
      "Epoch 16: val_loss improved from 0.68912 to 0.68885, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.7738 - val_loss: 0.6889 - val_accuracy: 0.7938\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.7863\n",
      "Epoch 17: val_loss improved from 0.68885 to 0.68858, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6886 - accuracy: 0.7861 - val_loss: 0.6886 - val_accuracy: 0.7980\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.7912\n",
      "Epoch 18: val_loss improved from 0.68858 to 0.68830, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6884 - accuracy: 0.7912 - val_loss: 0.6883 - val_accuracy: 0.7997\n",
      "Epoch 19/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.7929\n",
      "Epoch 19: val_loss improved from 0.68830 to 0.68802, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6881 - accuracy: 0.7930 - val_loss: 0.6880 - val_accuracy: 0.7997\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.7933\n",
      "Epoch 20: val_loss improved from 0.68802 to 0.68774, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6878 - accuracy: 0.7933 - val_loss: 0.6877 - val_accuracy: 0.8005\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.8005\n",
      "Epoch 21: val_loss improved from 0.68774 to 0.68745, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6875 - accuracy: 0.8005 - val_loss: 0.6874 - val_accuracy: 0.8038\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.7988\n",
      "Epoch 22: val_loss improved from 0.68745 to 0.68716, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6873 - accuracy: 0.7988 - val_loss: 0.6872 - val_accuracy: 0.8047\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.8002\n",
      "Epoch 23: val_loss improved from 0.68716 to 0.68687, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6869 - accuracy: 0.8002 - val_loss: 0.6869 - val_accuracy: 0.8072\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.8063\n",
      "Epoch 24: val_loss improved from 0.68687 to 0.68657, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6866 - accuracy: 0.8063 - val_loss: 0.6866 - val_accuracy: 0.8055\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.8041\n",
      "Epoch 25: val_loss improved from 0.68657 to 0.68626, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6863 - accuracy: 0.8041 - val_loss: 0.6863 - val_accuracy: 0.8063\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.8064\n",
      "Epoch 26: val_loss improved from 0.68626 to 0.68596, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6860 - accuracy: 0.8064 - val_loss: 0.6860 - val_accuracy: 0.8063\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.8128\n",
      "Epoch 27: val_loss improved from 0.68596 to 0.68565, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6857 - accuracy: 0.8128 - val_loss: 0.6856 - val_accuracy: 0.8072\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.8100\n",
      "Epoch 28: val_loss improved from 0.68565 to 0.68532, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6854 - accuracy: 0.8100 - val_loss: 0.6853 - val_accuracy: 0.8088\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.8134\n",
      "Epoch 29: val_loss improved from 0.68532 to 0.68499, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6850 - accuracy: 0.8134 - val_loss: 0.6850 - val_accuracy: 0.8088\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6847 - accuracy: 0.8116\n",
      "Epoch 30: val_loss improved from 0.68499 to 0.68465, saving model to model_sgd_fold1.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6847 - accuracy: 0.8116 - val_loss: 0.6847 - val_accuracy: 0.8088\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 1 - TP: 469, TN: 500, FP: 60, FN: 169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.6062\n",
      "Epoch 1: val_loss improved from inf to 0.69211, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.6921 - accuracy: 0.6063 - val_loss: 0.6921 - val_accuracy: 0.6269\n",
      "Epoch 2/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.6211\n",
      "Epoch 2: val_loss improved from 0.69211 to 0.69186, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6919 - accuracy: 0.6215 - val_loss: 0.6919 - val_accuracy: 0.6327\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.6116\n",
      "Epoch 3: val_loss improved from 0.69186 to 0.69160, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6916 - accuracy: 0.6116 - val_loss: 0.6916 - val_accuracy: 0.6227\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.6395\n",
      "Epoch 4: val_loss improved from 0.69160 to 0.69135, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.6395 - val_loss: 0.6913 - val_accuracy: 0.6202\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.6501\n",
      "Epoch 5: val_loss improved from 0.69135 to 0.69109, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6910 - accuracy: 0.6501 - val_loss: 0.6911 - val_accuracy: 0.6235\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.6566\n",
      "Epoch 6: val_loss improved from 0.69109 to 0.69083, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6908 - accuracy: 0.6566 - val_loss: 0.6908 - val_accuracy: 0.6202\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.6463\n",
      "Epoch 7: val_loss improved from 0.69083 to 0.69057, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6906 - accuracy: 0.6463 - val_loss: 0.6906 - val_accuracy: 0.6260\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.6456\n",
      "Epoch 8: val_loss improved from 0.69057 to 0.69031, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6902 - accuracy: 0.6456 - val_loss: 0.6903 - val_accuracy: 0.6411\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.6568\n",
      "Epoch 9: val_loss improved from 0.69031 to 0.69004, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6900 - accuracy: 0.6568 - val_loss: 0.6900 - val_accuracy: 0.6486\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.6806\n",
      "Epoch 10: val_loss improved from 0.69004 to 0.68978, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6897 - accuracy: 0.6806 - val_loss: 0.6898 - val_accuracy: 0.6469\n",
      "Epoch 11/30\n",
      "159/169 [===========================>..] - ETA: 0s - loss: 0.6895 - accuracy: 0.6608\n",
      "Epoch 11: val_loss improved from 0.68978 to 0.68951, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6895 - accuracy: 0.6615 - val_loss: 0.6895 - val_accuracy: 0.6503\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.6913\n",
      "Epoch 12: val_loss improved from 0.68951 to 0.68924, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6892 - accuracy: 0.6913 - val_loss: 0.6892 - val_accuracy: 0.6561\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.6918\n",
      "Epoch 13: val_loss improved from 0.68924 to 0.68896, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.6918 - val_loss: 0.6890 - val_accuracy: 0.6703\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.6940\n",
      "Epoch 14: val_loss improved from 0.68896 to 0.68868, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6886 - accuracy: 0.6940 - val_loss: 0.6887 - val_accuracy: 0.6720\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.6873\n",
      "Epoch 15: val_loss improved from 0.68868 to 0.68840, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.6873 - val_loss: 0.6884 - val_accuracy: 0.6903\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.7061\n",
      "Epoch 16: val_loss improved from 0.68840 to 0.68811, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6881 - accuracy: 0.7061 - val_loss: 0.6881 - val_accuracy: 0.6912\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.7077\n",
      "Epoch 17: val_loss improved from 0.68811 to 0.68782, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6877 - accuracy: 0.7077 - val_loss: 0.6878 - val_accuracy: 0.7012\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.7201\n",
      "Epoch 18: val_loss improved from 0.68782 to 0.68753, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.7201 - val_loss: 0.6875 - val_accuracy: 0.7028\n",
      "Epoch 19/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.7376\n",
      "Epoch 19: val_loss improved from 0.68753 to 0.68723, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6871 - accuracy: 0.7377 - val_loss: 0.6872 - val_accuracy: 0.7070\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.7291\n",
      "Epoch 20: val_loss improved from 0.68723 to 0.68692, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.7291 - val_loss: 0.6869 - val_accuracy: 0.7129\n",
      "Epoch 21/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.7416\n",
      "Epoch 21: val_loss improved from 0.68692 to 0.68661, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6865 - accuracy: 0.7415 - val_loss: 0.6866 - val_accuracy: 0.7220\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7460\n",
      "Epoch 22: val_loss improved from 0.68661 to 0.68630, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6862 - accuracy: 0.7460 - val_loss: 0.6863 - val_accuracy: 0.7279\n",
      "Epoch 23/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.7636\n",
      "Epoch 23: val_loss improved from 0.68630 to 0.68598, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.7636 - val_loss: 0.6860 - val_accuracy: 0.7412\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.7638\n",
      "Epoch 24: val_loss improved from 0.68598 to 0.68565, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.7638 - val_loss: 0.6857 - val_accuracy: 0.7454\n",
      "Epoch 25/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.7669\n",
      "Epoch 25: val_loss improved from 0.68565 to 0.68532, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6852 - accuracy: 0.7664 - val_loss: 0.6853 - val_accuracy: 0.7513\n",
      "Epoch 26/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.7713\n",
      "Epoch 26: val_loss improved from 0.68532 to 0.68498, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6848 - accuracy: 0.7716 - val_loss: 0.6850 - val_accuracy: 0.7554\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.7665\n",
      "Epoch 27: val_loss improved from 0.68498 to 0.68463, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.7665 - val_loss: 0.6846 - val_accuracy: 0.7579\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6841 - accuracy: 0.7923\n",
      "Epoch 28: val_loss improved from 0.68463 to 0.68428, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6841 - accuracy: 0.7923 - val_loss: 0.6843 - val_accuracy: 0.7629\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.7797\n",
      "Epoch 29: val_loss improved from 0.68428 to 0.68392, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6837 - accuracy: 0.7797 - val_loss: 0.6839 - val_accuracy: 0.7705\n",
      "Epoch 30/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6834 - accuracy: 0.7852\n",
      "Epoch 30: val_loss improved from 0.68392 to 0.68355, saving model to model_sgd_fold2.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6834 - accuracy: 0.7853 - val_loss: 0.6835 - val_accuracy: 0.7730\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 2 - TP: 526, TN: 400, FP: 202, FN: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4402\n",
      "Epoch 1: val_loss improved from inf to 0.69372, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.6937 - accuracy: 0.4401 - val_loss: 0.6937 - val_accuracy: 0.3731\n",
      "Epoch 2/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4533\n",
      "Epoch 2: val_loss improved from 0.69372 to 0.69349, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6936 - accuracy: 0.4538 - val_loss: 0.6935 - val_accuracy: 0.4282\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4800\n",
      "Epoch 3: val_loss improved from 0.69349 to 0.69326, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6933 - accuracy: 0.4800 - val_loss: 0.6933 - val_accuracy: 0.4816\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4927\n",
      "Epoch 4: val_loss improved from 0.69326 to 0.69303, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.4929 - val_loss: 0.6930 - val_accuracy: 0.5075\n",
      "Epoch 5/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5226\n",
      "Epoch 5: val_loss improved from 0.69303 to 0.69280, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6928 - accuracy: 0.5233 - val_loss: 0.6928 - val_accuracy: 0.5434\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5366\n",
      "Epoch 6: val_loss improved from 0.69280 to 0.69257, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6927 - accuracy: 0.5366 - val_loss: 0.6926 - val_accuracy: 0.5684\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5633\n",
      "Epoch 7: val_loss improved from 0.69257 to 0.69234, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.5633 - val_loss: 0.6923 - val_accuracy: 0.5743\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6922 - accuracy: 0.5863\n",
      "Epoch 8: val_loss improved from 0.69234 to 0.69211, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6922 - accuracy: 0.5863 - val_loss: 0.6921 - val_accuracy: 0.5860\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.5953\n",
      "Epoch 9: val_loss improved from 0.69211 to 0.69188, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6920 - accuracy: 0.5953 - val_loss: 0.6919 - val_accuracy: 0.5985\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5929\n",
      "Epoch 10: val_loss improved from 0.69188 to 0.69165, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.5929 - val_loss: 0.6917 - val_accuracy: 0.6119\n",
      "Epoch 11/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.6019\n",
      "Epoch 11: val_loss improved from 0.69165 to 0.69142, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.6026 - val_loss: 0.6914 - val_accuracy: 0.6160\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.6257\n",
      "Epoch 12: val_loss improved from 0.69142 to 0.69119, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.6257 - val_loss: 0.6912 - val_accuracy: 0.6244\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.6360\n",
      "Epoch 13: val_loss improved from 0.69119 to 0.69096, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.6359 - val_loss: 0.6910 - val_accuracy: 0.6336\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.6364\n",
      "Epoch 14: val_loss improved from 0.69096 to 0.69073, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.6364 - val_loss: 0.6907 - val_accuracy: 0.6427\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.6558\n",
      "Epoch 15: val_loss improved from 0.69073 to 0.69050, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6906 - accuracy: 0.6558 - val_loss: 0.6905 - val_accuracy: 0.6494\n",
      "Epoch 16/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6905 - accuracy: 0.6603\n",
      "Epoch 16: val_loss improved from 0.69050 to 0.69026, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6905 - accuracy: 0.6599 - val_loss: 0.6903 - val_accuracy: 0.6578\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.6778\n",
      "Epoch 17: val_loss improved from 0.69026 to 0.69003, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6902 - accuracy: 0.6778 - val_loss: 0.6900 - val_accuracy: 0.6661\n",
      "Epoch 18/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.6812\n",
      "Epoch 18: val_loss improved from 0.69003 to 0.68979, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6900 - accuracy: 0.6811 - val_loss: 0.6898 - val_accuracy: 0.6753\n",
      "Epoch 19/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6898 - accuracy: 0.6927\n",
      "Epoch 19: val_loss improved from 0.68979 to 0.68956, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6898 - accuracy: 0.6931 - val_loss: 0.6896 - val_accuracy: 0.6786\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.6979\n",
      "Epoch 20: val_loss improved from 0.68956 to 0.68931, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6895 - accuracy: 0.6979 - val_loss: 0.6893 - val_accuracy: 0.6912\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.6902\n",
      "Epoch 21: val_loss improved from 0.68931 to 0.68907, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6893 - accuracy: 0.6902 - val_loss: 0.6891 - val_accuracy: 0.6962\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.7121\n",
      "Epoch 22: val_loss improved from 0.68907 to 0.68883, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6890 - accuracy: 0.7121 - val_loss: 0.6888 - val_accuracy: 0.7104\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.7228\n",
      "Epoch 23: val_loss improved from 0.68883 to 0.68858, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6888 - accuracy: 0.7232 - val_loss: 0.6886 - val_accuracy: 0.7154\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.7299\n",
      "Epoch 24: val_loss improved from 0.68858 to 0.68833, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6886 - accuracy: 0.7299 - val_loss: 0.6883 - val_accuracy: 0.7195\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.7321\n",
      "Epoch 25: val_loss improved from 0.68833 to 0.68807, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.7321 - val_loss: 0.6881 - val_accuracy: 0.7254\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.7396\n",
      "Epoch 26: val_loss improved from 0.68807 to 0.68782, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6881 - accuracy: 0.7396 - val_loss: 0.6878 - val_accuracy: 0.7354\n",
      "Epoch 27/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.7516\n",
      "Epoch 27: val_loss improved from 0.68782 to 0.68756, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6878 - accuracy: 0.7518 - val_loss: 0.6876 - val_accuracy: 0.7362\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.7531\n",
      "Epoch 28: val_loss improved from 0.68756 to 0.68729, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6876 - accuracy: 0.7531 - val_loss: 0.6873 - val_accuracy: 0.7421\n",
      "Epoch 29/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.7558\n",
      "Epoch 29: val_loss improved from 0.68729 to 0.68703, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6873 - accuracy: 0.7558 - val_loss: 0.6870 - val_accuracy: 0.7454\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.7580\n",
      "Epoch 30: val_loss improved from 0.68703 to 0.68676, saving model to model_sgd_fold3.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6871 - accuracy: 0.7580 - val_loss: 0.6868 - val_accuracy: 0.7538\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 3 - TP: 503, TN: 400, FP: 189, FN: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6917 - accuracy: 0.6133\n",
      "Epoch 1: val_loss improved from inf to 0.69167, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 6s 11ms/step - loss: 0.6917 - accuracy: 0.6129 - val_loss: 0.6917 - val_accuracy: 0.6828\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.6407\n",
      "Epoch 2: val_loss improved from 0.69167 to 0.69144, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.6407 - val_loss: 0.6914 - val_accuracy: 0.6319\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.5989\n",
      "Epoch 3: val_loss improved from 0.69144 to 0.69121, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.5989 - val_loss: 0.6912 - val_accuracy: 0.6010\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.6274\n",
      "Epoch 4: val_loss improved from 0.69121 to 0.69097, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.6274 - val_loss: 0.6910 - val_accuracy: 0.5718\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.6068\n",
      "Epoch 5: val_loss improved from 0.69097 to 0.69073, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6906 - accuracy: 0.6068 - val_loss: 0.6907 - val_accuracy: 0.5518\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.6077\n",
      "Epoch 6: val_loss improved from 0.69073 to 0.69049, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6903 - accuracy: 0.6077 - val_loss: 0.6905 - val_accuracy: 0.5392\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.5903\n",
      "Epoch 7: val_loss improved from 0.69049 to 0.69024, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6900 - accuracy: 0.5903 - val_loss: 0.6902 - val_accuracy: 0.5334\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.6003\n",
      "Epoch 8: val_loss improved from 0.69024 to 0.68999, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6897 - accuracy: 0.6003 - val_loss: 0.6900 - val_accuracy: 0.5292\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.5824\n",
      "Epoch 9: val_loss improved from 0.68999 to 0.68974, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6894 - accuracy: 0.5824 - val_loss: 0.6897 - val_accuracy: 0.5259\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.5909\n",
      "Epoch 10: val_loss improved from 0.68974 to 0.68948, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6891 - accuracy: 0.5909 - val_loss: 0.6895 - val_accuracy: 0.5250\n",
      "Epoch 11/30\n",
      "161/169 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5738\n",
      "Epoch 11: val_loss improved from 0.68948 to 0.68922, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6889 - accuracy: 0.5739 - val_loss: 0.6892 - val_accuracy: 0.5259\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.5792\n",
      "Epoch 12: val_loss improved from 0.68922 to 0.68896, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6886 - accuracy: 0.5792 - val_loss: 0.6890 - val_accuracy: 0.5225\n",
      "Epoch 13/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.5831\n",
      "Epoch 13: val_loss improved from 0.68896 to 0.68870, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.5826 - val_loss: 0.6887 - val_accuracy: 0.5242\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.5957\n",
      "Epoch 14: val_loss improved from 0.68870 to 0.68842, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.5957 - val_loss: 0.6884 - val_accuracy: 0.5334\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.5953\n",
      "Epoch 15: val_loss improved from 0.68842 to 0.68815, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6877 - accuracy: 0.5953 - val_loss: 0.6881 - val_accuracy: 0.5417\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.6057\n",
      "Epoch 16: val_loss improved from 0.68815 to 0.68787, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.6057 - val_loss: 0.6879 - val_accuracy: 0.5534\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.5966\n",
      "Epoch 17: val_loss improved from 0.68787 to 0.68759, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6871 - accuracy: 0.5966 - val_loss: 0.6876 - val_accuracy: 0.5543\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.6009\n",
      "Epoch 18: val_loss improved from 0.68759 to 0.68729, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.6009 - val_loss: 0.6873 - val_accuracy: 0.5568\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6865 - accuracy: 0.6107\n",
      "Epoch 19: val_loss improved from 0.68729 to 0.68700, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6865 - accuracy: 0.6107 - val_loss: 0.6870 - val_accuracy: 0.5684\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.6272\n",
      "Epoch 20: val_loss improved from 0.68700 to 0.68670, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6862 - accuracy: 0.6272 - val_loss: 0.6867 - val_accuracy: 0.5701\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.6389\n",
      "Epoch 21: val_loss improved from 0.68670 to 0.68639, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.6389 - val_loss: 0.6864 - val_accuracy: 0.5943\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.6412\n",
      "Epoch 22: val_loss improved from 0.68639 to 0.68608, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.6412 - val_loss: 0.6861 - val_accuracy: 0.6294\n",
      "Epoch 23/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.6376\n",
      "Epoch 23: val_loss improved from 0.68608 to 0.68576, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6852 - accuracy: 0.6378 - val_loss: 0.6858 - val_accuracy: 0.6427\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.6597\n",
      "Epoch 24: val_loss improved from 0.68576 to 0.68543, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6848 - accuracy: 0.6597 - val_loss: 0.6854 - val_accuracy: 0.6519\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.6782\n",
      "Epoch 25: val_loss improved from 0.68543 to 0.68511, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.6782 - val_loss: 0.6851 - val_accuracy: 0.6644\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.6910\n",
      "Epoch 26: val_loss improved from 0.68511 to 0.68477, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6841 - accuracy: 0.6910 - val_loss: 0.6848 - val_accuracy: 0.6711\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.6874\n",
      "Epoch 27: val_loss improved from 0.68477 to 0.68443, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6838 - accuracy: 0.6874 - val_loss: 0.6844 - val_accuracy: 0.6786\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.6970\n",
      "Epoch 28: val_loss improved from 0.68443 to 0.68407, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6834 - accuracy: 0.6970 - val_loss: 0.6841 - val_accuracy: 0.6886\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.7156\n",
      "Epoch 29: val_loss improved from 0.68407 to 0.68372, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6830 - accuracy: 0.7156 - val_loss: 0.6837 - val_accuracy: 0.6928\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6826 - accuracy: 0.7269\n",
      "Epoch 30: val_loss improved from 0.68372 to 0.68335, saving model to model_sgd_fold4.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6826 - accuracy: 0.7269 - val_loss: 0.6833 - val_accuracy: 0.7003\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 4 - TP: 534, TN: 305, FP: 317, FN: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5538\n",
      "Epoch 1: val_loss improved from inf to 0.69273, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 6s 10ms/step - loss: 0.6929 - accuracy: 0.5538 - val_loss: 0.6927 - val_accuracy: 0.6169\n",
      "Epoch 2/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5779\n",
      "Epoch 2: val_loss improved from 0.69273 to 0.69246, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6927 - accuracy: 0.5778 - val_loss: 0.6925 - val_accuracy: 0.6511\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5912\n",
      "Epoch 3: val_loss improved from 0.69246 to 0.69219, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.5912 - val_loss: 0.6922 - val_accuracy: 0.6970\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.6244\n",
      "Epoch 4: val_loss improved from 0.69219 to 0.69192, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6922 - accuracy: 0.6243 - val_loss: 0.6919 - val_accuracy: 0.7245\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.6377\n",
      "Epoch 5: val_loss improved from 0.69192 to 0.69166, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6919 - accuracy: 0.6377 - val_loss: 0.6917 - val_accuracy: 0.7554\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.6646\n",
      "Epoch 6: val_loss improved from 0.69166 to 0.69139, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6916 - accuracy: 0.6647 - val_loss: 0.6914 - val_accuracy: 0.7796\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.6839\n",
      "Epoch 7: val_loss improved from 0.69139 to 0.69111, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.6839 - val_loss: 0.6911 - val_accuracy: 0.7980\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.6883\n",
      "Epoch 8: val_loss improved from 0.69111 to 0.69085, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6912 - accuracy: 0.6883 - val_loss: 0.6908 - val_accuracy: 0.8172\n",
      "Epoch 9/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.7223\n",
      "Epoch 9: val_loss improved from 0.69085 to 0.69058, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.7223 - val_loss: 0.6906 - val_accuracy: 0.8272\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.7347\n",
      "Epoch 10: val_loss improved from 0.69058 to 0.69030, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6906 - accuracy: 0.7347 - val_loss: 0.6903 - val_accuracy: 0.8289\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.7450\n",
      "Epoch 11: val_loss improved from 0.69030 to 0.69002, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6904 - accuracy: 0.7450 - val_loss: 0.6900 - val_accuracy: 0.8314\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.7612\n",
      "Epoch 12: val_loss improved from 0.69002 to 0.68975, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6901 - accuracy: 0.7612 - val_loss: 0.6898 - val_accuracy: 0.8331\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.7619\n",
      "Epoch 13: val_loss improved from 0.68975 to 0.68947, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.7619 - val_loss: 0.6895 - val_accuracy: 0.8381\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6896 - accuracy: 0.7683\n",
      "Epoch 14: val_loss improved from 0.68947 to 0.68919, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6896 - accuracy: 0.7683 - val_loss: 0.6892 - val_accuracy: 0.8431\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.7697\n",
      "Epoch 15: val_loss improved from 0.68919 to 0.68891, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6893 - accuracy: 0.7697 - val_loss: 0.6889 - val_accuracy: 0.8456\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.7814\n",
      "Epoch 16: val_loss improved from 0.68891 to 0.68863, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6891 - accuracy: 0.7814 - val_loss: 0.6886 - val_accuracy: 0.8489\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.7905\n",
      "Epoch 17: val_loss improved from 0.68863 to 0.68834, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6888 - accuracy: 0.7905 - val_loss: 0.6883 - val_accuracy: 0.8531\n",
      "Epoch 18/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.7959\n",
      "Epoch 18: val_loss improved from 0.68834 to 0.68805, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6885 - accuracy: 0.7960 - val_loss: 0.6881 - val_accuracy: 0.8548\n",
      "Epoch 19/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.8025\n",
      "Epoch 19: val_loss improved from 0.68805 to 0.68776, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6882 - accuracy: 0.8027 - val_loss: 0.6878 - val_accuracy: 0.8539\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.8041\n",
      "Epoch 20: val_loss improved from 0.68776 to 0.68746, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.8041 - val_loss: 0.6875 - val_accuracy: 0.8556\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.8097\n",
      "Epoch 21: val_loss improved from 0.68746 to 0.68716, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6877 - accuracy: 0.8097 - val_loss: 0.6872 - val_accuracy: 0.8581\n",
      "Epoch 22/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.8173\n",
      "Epoch 22: val_loss improved from 0.68716 to 0.68685, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.8171 - val_loss: 0.6869 - val_accuracy: 0.8564\n",
      "Epoch 23/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.8171\n",
      "Epoch 23: val_loss improved from 0.68685 to 0.68654, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6871 - accuracy: 0.8169 - val_loss: 0.6865 - val_accuracy: 0.8556\n",
      "Epoch 24/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6867 - accuracy: 0.8226\n",
      "Epoch 24: val_loss improved from 0.68654 to 0.68622, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6867 - accuracy: 0.8227 - val_loss: 0.6862 - val_accuracy: 0.8556\n",
      "Epoch 25/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.8220\n",
      "Epoch 25: val_loss improved from 0.68622 to 0.68590, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6865 - accuracy: 0.8219 - val_loss: 0.6859 - val_accuracy: 0.8556\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6861 - accuracy: 0.8239\n",
      "Epoch 26: val_loss improved from 0.68590 to 0.68557, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6861 - accuracy: 0.8239 - val_loss: 0.6856 - val_accuracy: 0.8548\n",
      "Epoch 27/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.8273\n",
      "Epoch 27: val_loss improved from 0.68557 to 0.68523, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6859 - accuracy: 0.8281 - val_loss: 0.6852 - val_accuracy: 0.8589\n",
      "Epoch 28/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.8274\n",
      "Epoch 28: val_loss improved from 0.68523 to 0.68489, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.8274 - val_loss: 0.6849 - val_accuracy: 0.8589\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.8296\n",
      "Epoch 29: val_loss improved from 0.68489 to 0.68454, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6852 - accuracy: 0.8296 - val_loss: 0.6845 - val_accuracy: 0.8598\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.8277\n",
      "Epoch 30: val_loss improved from 0.68454 to 0.68419, saving model to model_sgd_fold5.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6849 - accuracy: 0.8277 - val_loss: 0.6842 - val_accuracy: 0.8606\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 5 - TP: 526, TN: 505, FP: 75, FN: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4327\n",
      "Epoch 1: val_loss improved from inf to 0.69370, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.6939 - accuracy: 0.4330 - val_loss: 0.6937 - val_accuracy: 0.3860\n",
      "Epoch 2/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.6936 - accuracy: 0.4481\n",
      "Epoch 2: val_loss improved from 0.69370 to 0.69346, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6936 - accuracy: 0.4494 - val_loss: 0.6935 - val_accuracy: 0.4269\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4719\n",
      "Epoch 3: val_loss improved from 0.69346 to 0.69322, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6933 - accuracy: 0.4719 - val_loss: 0.6932 - val_accuracy: 0.4336\n",
      "Epoch 4/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4780\n",
      "Epoch 4: val_loss improved from 0.69322 to 0.69298, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6930 - accuracy: 0.4780 - val_loss: 0.6930 - val_accuracy: 0.4645\n",
      "Epoch 5/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.4865\n",
      "Epoch 5: val_loss improved from 0.69298 to 0.69274, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6928 - accuracy: 0.4865 - val_loss: 0.6927 - val_accuracy: 0.4754\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5096\n",
      "Epoch 6: val_loss improved from 0.69274 to 0.69249, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6925 - val_accuracy: 0.4812\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6922 - accuracy: 0.5221\n",
      "Epoch 7: val_loss improved from 0.69249 to 0.69225, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6922 - accuracy: 0.5221 - val_loss: 0.6923 - val_accuracy: 0.4871\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5126\n",
      "Epoch 8: val_loss improved from 0.69225 to 0.69200, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6920 - accuracy: 0.5122 - val_loss: 0.6920 - val_accuracy: 0.4871\n",
      "Epoch 9/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5167\n",
      "Epoch 9: val_loss improved from 0.69200 to 0.69175, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.5174 - val_loss: 0.6918 - val_accuracy: 0.4871\n",
      "Epoch 10/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5294\n",
      "Epoch 10: val_loss improved from 0.69175 to 0.69150, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.5294 - val_loss: 0.6915 - val_accuracy: 0.4854\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5225\n",
      "Epoch 11: val_loss improved from 0.69150 to 0.69124, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6912 - accuracy: 0.5225 - val_loss: 0.6912 - val_accuracy: 0.4862\n",
      "Epoch 12/30\n",
      "164/169 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5228\n",
      "Epoch 12: val_loss improved from 0.69124 to 0.69098, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6909 - accuracy: 0.5225 - val_loss: 0.6910 - val_accuracy: 0.4887\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.5252\n",
      "Epoch 13: val_loss improved from 0.69098 to 0.69072, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6906 - accuracy: 0.5254 - val_loss: 0.6907 - val_accuracy: 0.4887\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.5289\n",
      "Epoch 14: val_loss improved from 0.69072 to 0.69046, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6904 - accuracy: 0.5289 - val_loss: 0.6905 - val_accuracy: 0.4896\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.5273\n",
      "Epoch 15: val_loss improved from 0.69046 to 0.69019, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6900 - accuracy: 0.5273 - val_loss: 0.6902 - val_accuracy: 0.4896\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.5302\n",
      "Epoch 16: val_loss improved from 0.69019 to 0.68992, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.5302 - val_loss: 0.6899 - val_accuracy: 0.4904\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6896 - accuracy: 0.5374\n",
      "Epoch 17: val_loss improved from 0.68992 to 0.68965, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6896 - accuracy: 0.5374 - val_loss: 0.6896 - val_accuracy: 0.4904\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.5322\n",
      "Epoch 18: val_loss improved from 0.68965 to 0.68937, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6892 - accuracy: 0.5322 - val_loss: 0.6894 - val_accuracy: 0.4929\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.5402\n",
      "Epoch 19: val_loss improved from 0.68937 to 0.68909, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.5402 - val_loss: 0.6891 - val_accuracy: 0.4971\n",
      "Epoch 20/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5357\n",
      "Epoch 20: val_loss improved from 0.68909 to 0.68880, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6887 - accuracy: 0.5355 - val_loss: 0.6888 - val_accuracy: 0.5079\n",
      "Epoch 21/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6884 - accuracy: 0.5507\n",
      "Epoch 21: val_loss improved from 0.68880 to 0.68851, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6884 - accuracy: 0.5505 - val_loss: 0.6885 - val_accuracy: 0.5155\n",
      "Epoch 22/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5628\n",
      "Epoch 22: val_loss improved from 0.68851 to 0.68822, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6881 - accuracy: 0.5629 - val_loss: 0.6882 - val_accuracy: 0.5230\n",
      "Epoch 23/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.5639\n",
      "Epoch 23: val_loss improved from 0.68822 to 0.68792, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6878 - accuracy: 0.5637 - val_loss: 0.6879 - val_accuracy: 0.5347\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.5708\n",
      "Epoch 24: val_loss improved from 0.68792 to 0.68762, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6875 - accuracy: 0.5708 - val_loss: 0.6876 - val_accuracy: 0.5447\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.5797\n",
      "Epoch 25: val_loss improved from 0.68762 to 0.68731, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6872 - accuracy: 0.5797 - val_loss: 0.6873 - val_accuracy: 0.5505\n",
      "Epoch 26/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5896\n",
      "Epoch 26: val_loss improved from 0.68731 to 0.68701, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.5899 - val_loss: 0.6870 - val_accuracy: 0.5556\n",
      "Epoch 27/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.6040\n",
      "Epoch 27: val_loss improved from 0.68701 to 0.68669, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6865 - accuracy: 0.6038 - val_loss: 0.6867 - val_accuracy: 0.5656\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.6063\n",
      "Epoch 28: val_loss improved from 0.68669 to 0.68637, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6863 - accuracy: 0.6063 - val_loss: 0.6864 - val_accuracy: 0.5739\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.6144\n",
      "Epoch 29: val_loss improved from 0.68637 to 0.68604, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6859 - accuracy: 0.6144 - val_loss: 0.6860 - val_accuracy: 0.5906\n",
      "Epoch 30/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6856 - accuracy: 0.6110\n",
      "Epoch 30: val_loss improved from 0.68604 to 0.68570, saving model to model_sgd_fold6.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6856 - accuracy: 0.6110 - val_loss: 0.6857 - val_accuracy: 0.6307\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 6 - TP: 566, TN: 189, FP: 432, FN: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.4781\n",
      "Epoch 1: val_loss improved from inf to 0.69312, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.6931 - accuracy: 0.4778 - val_loss: 0.6931 - val_accuracy: 0.4486\n",
      "Epoch 2/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.4900\n",
      "Epoch 2: val_loss improved from 0.69312 to 0.69291, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6929 - accuracy: 0.4900 - val_loss: 0.6929 - val_accuracy: 0.4812\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5072\n",
      "Epoch 3: val_loss improved from 0.69291 to 0.69269, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6927 - accuracy: 0.5072 - val_loss: 0.6927 - val_accuracy: 0.4979\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.5189\n",
      "Epoch 4: val_loss improved from 0.69269 to 0.69247, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.5185 - val_loss: 0.6925 - val_accuracy: 0.5104\n",
      "Epoch 5/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5285\n",
      "Epoch 5: val_loss improved from 0.69247 to 0.69225, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6922 - accuracy: 0.5285 - val_loss: 0.6923 - val_accuracy: 0.5071\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5343\n",
      "Epoch 6: val_loss improved from 0.69225 to 0.69204, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6919 - accuracy: 0.5343 - val_loss: 0.6920 - val_accuracy: 0.5063\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.5345\n",
      "Epoch 7: val_loss improved from 0.69204 to 0.69181, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6917 - accuracy: 0.5345 - val_loss: 0.6918 - val_accuracy: 0.5021\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.5443\n",
      "Epoch 8: val_loss improved from 0.69181 to 0.69159, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.5443 - val_loss: 0.6916 - val_accuracy: 0.5013\n",
      "Epoch 9/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5404\n",
      "Epoch 9: val_loss improved from 0.69159 to 0.69136, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.5403 - val_loss: 0.6914 - val_accuracy: 0.4946\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5492\n",
      "Epoch 10: val_loss improved from 0.69136 to 0.69113, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.5497 - val_loss: 0.6911 - val_accuracy: 0.4937\n",
      "Epoch 11/30\n",
      "158/169 [===========================>..] - ETA: 0s - loss: 0.6905 - accuracy: 0.5361\n",
      "Epoch 11: val_loss improved from 0.69113 to 0.69089, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6906 - accuracy: 0.5333 - val_loss: 0.6909 - val_accuracy: 0.4929\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.5355\n",
      "Epoch 12: val_loss improved from 0.69089 to 0.69066, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6904 - accuracy: 0.5355 - val_loss: 0.6907 - val_accuracy: 0.4921\n",
      "Epoch 13/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.5406\n",
      "Epoch 13: val_loss improved from 0.69066 to 0.69042, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6901 - accuracy: 0.5403 - val_loss: 0.6904 - val_accuracy: 0.4929\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.5533\n",
      "Epoch 14: val_loss improved from 0.69042 to 0.69018, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6898 - accuracy: 0.5533 - val_loss: 0.6902 - val_accuracy: 0.4929\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6896 - accuracy: 0.5353\n",
      "Epoch 15: val_loss improved from 0.69018 to 0.68993, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6896 - accuracy: 0.5353 - val_loss: 0.6899 - val_accuracy: 0.4937\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.5403\n",
      "Epoch 16: val_loss improved from 0.68993 to 0.68968, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6893 - accuracy: 0.5403 - val_loss: 0.6897 - val_accuracy: 0.4921\n",
      "Epoch 17/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.5405\n",
      "Epoch 17: val_loss improved from 0.68968 to 0.68943, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6890 - accuracy: 0.5405 - val_loss: 0.6894 - val_accuracy: 0.4929\n",
      "Epoch 18/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5490\n",
      "Epoch 18: val_loss improved from 0.68943 to 0.68917, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6887 - accuracy: 0.5490 - val_loss: 0.6892 - val_accuracy: 0.4937\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.5501\n",
      "Epoch 19: val_loss improved from 0.68917 to 0.68892, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6885 - accuracy: 0.5501 - val_loss: 0.6889 - val_accuracy: 0.4979\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.5593\n",
      "Epoch 20: val_loss improved from 0.68892 to 0.68865, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6882 - accuracy: 0.5593 - val_loss: 0.6887 - val_accuracy: 0.5021\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6879 - accuracy: 0.5592\n",
      "Epoch 21: val_loss improved from 0.68865 to 0.68839, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6879 - accuracy: 0.5592 - val_loss: 0.6884 - val_accuracy: 0.5038\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.5632\n",
      "Epoch 22: val_loss improved from 0.68839 to 0.68812, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6877 - accuracy: 0.5632 - val_loss: 0.6881 - val_accuracy: 0.5054\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.5711\n",
      "Epoch 23: val_loss improved from 0.68812 to 0.68784, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.5711 - val_loss: 0.6878 - val_accuracy: 0.5113\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.5802\n",
      "Epoch 24: val_loss improved from 0.68784 to 0.68756, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.5802 - val_loss: 0.6876 - val_accuracy: 0.5180\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.5821\n",
      "Epoch 25: val_loss improved from 0.68756 to 0.68727, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6867 - accuracy: 0.5821 - val_loss: 0.6873 - val_accuracy: 0.5305\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6865 - accuracy: 0.6025\n",
      "Epoch 26: val_loss improved from 0.68727 to 0.68698, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6865 - accuracy: 0.6025 - val_loss: 0.6870 - val_accuracy: 0.5380\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6861 - accuracy: 0.6219\n",
      "Epoch 27: val_loss improved from 0.68698 to 0.68669, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6861 - accuracy: 0.6219 - val_loss: 0.6867 - val_accuracy: 0.5647\n",
      "Epoch 28/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.6291\n",
      "Epoch 28: val_loss improved from 0.68669 to 0.68638, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.6289 - val_loss: 0.6864 - val_accuracy: 0.5773\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.6266\n",
      "Epoch 29: val_loss improved from 0.68638 to 0.68608, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.6266 - val_loss: 0.6861 - val_accuracy: 0.5848\n",
      "Epoch 30/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.6489\n",
      "Epoch 30: val_loss improved from 0.68608 to 0.68578, saving model to model_sgd_fold7.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6853 - accuracy: 0.6491 - val_loss: 0.6858 - val_accuracy: 0.5931\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 7 - TP: 551, TN: 159, FP: 465, FN: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.69279, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.5263\n",
      "Epoch 2/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5225\n",
      "Epoch 2: val_loss improved from 0.69279 to 0.69251, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6926 - accuracy: 0.5225 - val_loss: 0.6925 - val_accuracy: 0.5556\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5378\n",
      "Epoch 3: val_loss improved from 0.69251 to 0.69224, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.5378 - val_loss: 0.6922 - val_accuracy: 0.5681\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5646\n",
      "Epoch 4: val_loss improved from 0.69224 to 0.69196, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6921 - accuracy: 0.5646 - val_loss: 0.6920 - val_accuracy: 0.5756\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5709\n",
      "Epoch 5: val_loss improved from 0.69196 to 0.69169, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.5709 - val_loss: 0.6917 - val_accuracy: 0.5848\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.5804\n",
      "Epoch 6: val_loss improved from 0.69169 to 0.69141, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.5803 - val_loss: 0.6914 - val_accuracy: 0.5973\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.6146\n",
      "Epoch 7: val_loss improved from 0.69141 to 0.69114, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6912 - accuracy: 0.6146 - val_loss: 0.6911 - val_accuracy: 0.6048\n",
      "Epoch 8/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.6134\n",
      "Epoch 8: val_loss improved from 0.69114 to 0.69086, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6910 - accuracy: 0.6134 - val_loss: 0.6909 - val_accuracy: 0.6065\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.6219\n",
      "Epoch 9: val_loss improved from 0.69086 to 0.69058, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6907 - accuracy: 0.6219 - val_loss: 0.6906 - val_accuracy: 0.6349\n",
      "Epoch 10/30\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.6904 - accuracy: 0.6325\n",
      "Epoch 10: val_loss improved from 0.69058 to 0.69030, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6904 - accuracy: 0.6317 - val_loss: 0.6903 - val_accuracy: 0.6642\n",
      "Epoch 11/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.6642\n",
      "Epoch 11: val_loss improved from 0.69030 to 0.69003, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6900 - accuracy: 0.6637 - val_loss: 0.6900 - val_accuracy: 0.6867\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.6677\n",
      "Epoch 12: val_loss improved from 0.69003 to 0.68974, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6898 - accuracy: 0.6677 - val_loss: 0.6897 - val_accuracy: 0.6942\n",
      "Epoch 13/30\n",
      "162/169 [===========================>..] - ETA: 0s - loss: 0.6895 - accuracy: 0.6985\n",
      "Epoch 13: val_loss improved from 0.68974 to 0.68946, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6895 - accuracy: 0.6973 - val_loss: 0.6895 - val_accuracy: 0.7018\n",
      "Epoch 14/30\n",
      "163/169 [===========================>..] - ETA: 0s - loss: 0.6892 - accuracy: 0.6830\n",
      "Epoch 14: val_loss improved from 0.68946 to 0.68917, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6892 - accuracy: 0.6846 - val_loss: 0.6892 - val_accuracy: 0.7151\n",
      "Epoch 15/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.7111\n",
      "Epoch 15: val_loss improved from 0.68917 to 0.68888, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6888 - accuracy: 0.7106 - val_loss: 0.6889 - val_accuracy: 0.7310\n",
      "Epoch 16/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.7088\n",
      "Epoch 16: val_loss improved from 0.68888 to 0.68859, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6886 - accuracy: 0.7088 - val_loss: 0.6886 - val_accuracy: 0.7343\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6883 - accuracy: 0.7165\n",
      "Epoch 17: val_loss improved from 0.68859 to 0.68830, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.7161 - val_loss: 0.6883 - val_accuracy: 0.7527\n",
      "Epoch 18/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.7280\n",
      "Epoch 18: val_loss improved from 0.68830 to 0.68800, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.7280 - val_loss: 0.6880 - val_accuracy: 0.7569\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.7298\n",
      "Epoch 19: val_loss improved from 0.68800 to 0.68769, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.6877 - accuracy: 0.7298 - val_loss: 0.6877 - val_accuracy: 0.7694\n",
      "Epoch 20/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.7495\n",
      "Epoch 20: val_loss improved from 0.68769 to 0.68739, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6874 - accuracy: 0.7497 - val_loss: 0.6874 - val_accuracy: 0.7753\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.7602\n",
      "Epoch 21: val_loss improved from 0.68739 to 0.68708, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.7602 - val_loss: 0.6871 - val_accuracy: 0.7778\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.7659\n",
      "Epoch 22: val_loss improved from 0.68708 to 0.68676, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6867 - accuracy: 0.7659 - val_loss: 0.6868 - val_accuracy: 0.7811\n",
      "Epoch 23/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.7565\n",
      "Epoch 23: val_loss improved from 0.68676 to 0.68644, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6864 - accuracy: 0.7566 - val_loss: 0.6864 - val_accuracy: 0.7836\n",
      "Epoch 24/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6860 - accuracy: 0.7719\n",
      "Epoch 24: val_loss improved from 0.68644 to 0.68612, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6860 - accuracy: 0.7719 - val_loss: 0.6861 - val_accuracy: 0.7920\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.7786\n",
      "Epoch 25: val_loss improved from 0.68612 to 0.68579, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.7786 - val_loss: 0.6858 - val_accuracy: 0.7978\n",
      "Epoch 26/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.7838\n",
      "Epoch 26: val_loss improved from 0.68579 to 0.68545, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6854 - accuracy: 0.7837 - val_loss: 0.6855 - val_accuracy: 0.8003\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.7837\n",
      "Epoch 27: val_loss improved from 0.68545 to 0.68511, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6850 - accuracy: 0.7837 - val_loss: 0.6851 - val_accuracy: 0.8037\n",
      "Epoch 28/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.7903\n",
      "Epoch 28: val_loss improved from 0.68511 to 0.68476, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6847 - accuracy: 0.7905 - val_loss: 0.6848 - val_accuracy: 0.8045\n",
      "Epoch 29/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.7910\n",
      "Epoch 29: val_loss improved from 0.68476 to 0.68440, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6843 - accuracy: 0.7911 - val_loss: 0.6844 - val_accuracy: 0.8104\n",
      "Epoch 30/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.8067\n",
      "Epoch 30: val_loss improved from 0.68440 to 0.68404, saving model to model_sgd_fold8.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6840 - accuracy: 0.8068 - val_loss: 0.6840 - val_accuracy: 0.8095\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 8 - TP: 531, TN: 438, FP: 157, FN: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.4319\n",
      "Epoch 1: val_loss improved from inf to 0.69346, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 7s 10ms/step - loss: 0.6936 - accuracy: 0.4326 - val_loss: 0.6935 - val_accuracy: 0.4578\n",
      "Epoch 2/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4549\n",
      "Epoch 2: val_loss improved from 0.69346 to 0.69315, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6933 - accuracy: 0.4551 - val_loss: 0.6932 - val_accuracy: 0.4703\n",
      "Epoch 3/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.4756\n",
      "Epoch 3: val_loss improved from 0.69315 to 0.69285, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6930 - accuracy: 0.4758 - val_loss: 0.6928 - val_accuracy: 0.4762\n",
      "Epoch 4/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.4981\n",
      "Epoch 4: val_loss improved from 0.69285 to 0.69254, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6928 - accuracy: 0.4981 - val_loss: 0.6925 - val_accuracy: 0.4845\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.5092\n",
      "Epoch 5: val_loss improved from 0.69254 to 0.69223, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6925 - accuracy: 0.5092 - val_loss: 0.6922 - val_accuracy: 0.5188\n",
      "Epoch 6/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5172\n",
      "Epoch 6: val_loss improved from 0.69223 to 0.69192, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6922 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5246\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5383\n",
      "Epoch 7: val_loss improved from 0.69192 to 0.69162, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6919 - accuracy: 0.5383 - val_loss: 0.6916 - val_accuracy: 0.5388\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5589\n",
      "Epoch 8: val_loss improved from 0.69162 to 0.69131, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6916 - accuracy: 0.5585 - val_loss: 0.6913 - val_accuracy: 0.5514\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.5745\n",
      "Epoch 9: val_loss improved from 0.69131 to 0.69101, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.5745 - val_loss: 0.6910 - val_accuracy: 0.5597\n",
      "Epoch 10/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5843\n",
      "Epoch 10: val_loss improved from 0.69101 to 0.69070, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6910 - accuracy: 0.5841 - val_loss: 0.6907 - val_accuracy: 0.5773\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.5868\n",
      "Epoch 11: val_loss improved from 0.69070 to 0.69039, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6907 - accuracy: 0.5868 - val_loss: 0.6904 - val_accuracy: 0.5990\n",
      "Epoch 12/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.6175\n",
      "Epoch 12: val_loss improved from 0.69039 to 0.69008, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6904 - accuracy: 0.6175 - val_loss: 0.6901 - val_accuracy: 0.6015\n",
      "Epoch 13/30\n",
      "165/169 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.6164\n",
      "Epoch 13: val_loss improved from 0.69008 to 0.68976, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6901 - accuracy: 0.6173 - val_loss: 0.6898 - val_accuracy: 0.6124\n",
      "Epoch 14/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.6311\n",
      "Epoch 14: val_loss improved from 0.68976 to 0.68945, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6898 - accuracy: 0.6311 - val_loss: 0.6895 - val_accuracy: 0.6316\n",
      "Epoch 15/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.6497\n",
      "Epoch 15: val_loss improved from 0.68945 to 0.68914, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6895 - accuracy: 0.6498 - val_loss: 0.6891 - val_accuracy: 0.6491\n",
      "Epoch 16/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6892 - accuracy: 0.6533\n",
      "Epoch 16: val_loss improved from 0.68914 to 0.68882, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6892 - accuracy: 0.6533 - val_loss: 0.6888 - val_accuracy: 0.6633\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.6638\n",
      "Epoch 17: val_loss improved from 0.68882 to 0.68850, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6890 - accuracy: 0.6635 - val_loss: 0.6885 - val_accuracy: 0.6825\n",
      "Epoch 18/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.6772\n",
      "Epoch 18: val_loss improved from 0.68850 to 0.68817, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6886 - accuracy: 0.6773 - val_loss: 0.6882 - val_accuracy: 0.6942\n",
      "Epoch 19/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.6815\n",
      "Epoch 19: val_loss improved from 0.68817 to 0.68784, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.6815 - val_loss: 0.6878 - val_accuracy: 0.7126\n",
      "Epoch 20/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.6941\n",
      "Epoch 20: val_loss improved from 0.68784 to 0.68751, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.6941 - val_loss: 0.6875 - val_accuracy: 0.7226\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.7078\n",
      "Epoch 21: val_loss improved from 0.68751 to 0.68718, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6877 - accuracy: 0.7078 - val_loss: 0.6872 - val_accuracy: 0.7318\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.7210\n",
      "Epoch 22: val_loss improved from 0.68718 to 0.68684, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.7210 - val_loss: 0.6868 - val_accuracy: 0.7385\n",
      "Epoch 23/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.7254\n",
      "Epoch 23: val_loss improved from 0.68684 to 0.68649, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.7254 - val_loss: 0.6865 - val_accuracy: 0.7477\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.7270\n",
      "Epoch 24: val_loss improved from 0.68649 to 0.68614, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.7270 - val_loss: 0.6861 - val_accuracy: 0.7536\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6864 - accuracy: 0.7318\n",
      "Epoch 25: val_loss improved from 0.68614 to 0.68579, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6864 - accuracy: 0.7318 - val_loss: 0.6858 - val_accuracy: 0.7577\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.7392\n",
      "Epoch 26: val_loss improved from 0.68579 to 0.68543, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6860 - accuracy: 0.7392 - val_loss: 0.6854 - val_accuracy: 0.7661\n",
      "Epoch 27/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.7436\n",
      "Epoch 27: val_loss improved from 0.68543 to 0.68506, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6857 - accuracy: 0.7436 - val_loss: 0.6851 - val_accuracy: 0.7703\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.7493\n",
      "Epoch 28: val_loss improved from 0.68506 to 0.68469, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6853 - accuracy: 0.7493 - val_loss: 0.6847 - val_accuracy: 0.7744\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.7574\n",
      "Epoch 29: val_loss improved from 0.68469 to 0.68431, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6849 - accuracy: 0.7574 - val_loss: 0.6843 - val_accuracy: 0.7803\n",
      "Epoch 30/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6846 - accuracy: 0.7545\n",
      "Epoch 30: val_loss improved from 0.68431 to 0.68392, saving model to model_sgd_fold9.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6846 - accuracy: 0.7545 - val_loss: 0.6839 - val_accuracy: 0.7811\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 9 - TP: 511, TN: 424, FP: 165, FN: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.4564\n",
      "Epoch 1: val_loss improved from inf to 0.69387, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 5s 10ms/step - loss: 0.6940 - accuracy: 0.4574 - val_loss: 0.6939 - val_accuracy: 0.4369\n",
      "Epoch 2/30\n",
      "167/169 [============================>.] - ETA: 0s - loss: 0.6938 - accuracy: 0.4730\n",
      "Epoch 2: val_loss improved from 0.69387 to 0.69358, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6938 - accuracy: 0.4724 - val_loss: 0.6936 - val_accuracy: 0.4403\n",
      "Epoch 3/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.4697\n",
      "Epoch 3: val_loss improved from 0.69358 to 0.69329, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6935 - accuracy: 0.4697 - val_loss: 0.6933 - val_accuracy: 0.4820\n",
      "Epoch 4/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5025\n",
      "Epoch 4: val_loss improved from 0.69329 to 0.69301, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.5025 - val_loss: 0.6930 - val_accuracy: 0.5046\n",
      "Epoch 5/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5161\n",
      "Epoch 5: val_loss improved from 0.69301 to 0.69272, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6929 - accuracy: 0.5161 - val_loss: 0.6927 - val_accuracy: 0.5539\n",
      "Epoch 6/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.5454\n",
      "Epoch 6: val_loss improved from 0.69272 to 0.69243, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6926 - accuracy: 0.5454 - val_loss: 0.6924 - val_accuracy: 0.5673\n",
      "Epoch 7/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.5604\n",
      "Epoch 7: val_loss improved from 0.69243 to 0.69215, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6923 - accuracy: 0.5604 - val_loss: 0.6922 - val_accuracy: 0.5965\n",
      "Epoch 8/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5751\n",
      "Epoch 8: val_loss improved from 0.69215 to 0.69187, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6921 - accuracy: 0.5750 - val_loss: 0.6919 - val_accuracy: 0.6099\n",
      "Epoch 9/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5975\n",
      "Epoch 9: val_loss improved from 0.69187 to 0.69158, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6918 - accuracy: 0.5975 - val_loss: 0.6916 - val_accuracy: 0.6366\n",
      "Epoch 10/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.5989\n",
      "Epoch 10: val_loss improved from 0.69158 to 0.69130, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6915 - accuracy: 0.5989 - val_loss: 0.6913 - val_accuracy: 0.6433\n",
      "Epoch 11/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.6044\n",
      "Epoch 11: val_loss improved from 0.69130 to 0.69102, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.6044 - val_loss: 0.6910 - val_accuracy: 0.6575\n",
      "Epoch 12/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.6319\n",
      "Epoch 12: val_loss improved from 0.69102 to 0.69073, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.6317 - val_loss: 0.6907 - val_accuracy: 0.6901\n",
      "Epoch 13/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.6373\n",
      "Epoch 13: val_loss improved from 0.69073 to 0.69045, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6907 - accuracy: 0.6373 - val_loss: 0.6904 - val_accuracy: 0.6942\n",
      "Epoch 14/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6904 - accuracy: 0.6822\n",
      "Epoch 14: val_loss improved from 0.69045 to 0.69016, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6904 - accuracy: 0.6821 - val_loss: 0.6902 - val_accuracy: 0.6926\n",
      "Epoch 15/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.6917\n",
      "Epoch 15: val_loss improved from 0.69016 to 0.68987, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6901 - accuracy: 0.6917 - val_loss: 0.6899 - val_accuracy: 0.6934\n",
      "Epoch 16/30\n",
      "161/169 [===========================>..] - ETA: 0s - loss: 0.6899 - accuracy: 0.6758\n",
      "Epoch 16: val_loss improved from 0.68987 to 0.68958, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 6ms/step - loss: 0.6899 - accuracy: 0.6756 - val_loss: 0.6896 - val_accuracy: 0.7185\n",
      "Epoch 17/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.6974\n",
      "Epoch 17: val_loss improved from 0.68958 to 0.68928, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6895 - accuracy: 0.6976 - val_loss: 0.6893 - val_accuracy: 0.7185\n",
      "Epoch 18/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.7000\n",
      "Epoch 18: val_loss improved from 0.68928 to 0.68899, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6892 - accuracy: 0.7000 - val_loss: 0.6890 - val_accuracy: 0.7260\n",
      "Epoch 19/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.6966\n",
      "Epoch 19: val_loss improved from 0.68899 to 0.68869, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.6963 - val_loss: 0.6887 - val_accuracy: 0.7402\n",
      "Epoch 20/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.7278\n",
      "Epoch 20: val_loss improved from 0.68869 to 0.68839, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6887 - accuracy: 0.7278 - val_loss: 0.6884 - val_accuracy: 0.7494\n",
      "Epoch 21/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.7353\n",
      "Epoch 21: val_loss improved from 0.68839 to 0.68809, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6883 - accuracy: 0.7353 - val_loss: 0.6881 - val_accuracy: 0.7536\n",
      "Epoch 22/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.7432\n",
      "Epoch 22: val_loss improved from 0.68809 to 0.68778, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.7432 - val_loss: 0.6878 - val_accuracy: 0.7619\n",
      "Epoch 23/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.7480\n",
      "Epoch 23: val_loss improved from 0.68778 to 0.68747, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6878 - accuracy: 0.7478 - val_loss: 0.6875 - val_accuracy: 0.7669\n",
      "Epoch 24/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.7477\n",
      "Epoch 24: val_loss improved from 0.68747 to 0.68715, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6875 - accuracy: 0.7477 - val_loss: 0.6872 - val_accuracy: 0.7744\n",
      "Epoch 25/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.7652\n",
      "Epoch 25: val_loss improved from 0.68715 to 0.68684, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6871 - accuracy: 0.7652 - val_loss: 0.6868 - val_accuracy: 0.7803\n",
      "Epoch 26/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.7681\n",
      "Epoch 26: val_loss improved from 0.68684 to 0.68651, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.7681 - val_loss: 0.6865 - val_accuracy: 0.7861\n",
      "Epoch 27/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.7769\n",
      "Epoch 27: val_loss improved from 0.68651 to 0.68618, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6864 - accuracy: 0.7771 - val_loss: 0.6862 - val_accuracy: 0.7870\n",
      "Epoch 28/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7747\n",
      "Epoch 28: val_loss improved from 0.68618 to 0.68585, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6862 - accuracy: 0.7747 - val_loss: 0.6858 - val_accuracy: 0.7928\n",
      "Epoch 29/30\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.7834\n",
      "Epoch 29: val_loss improved from 0.68585 to 0.68551, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.7834 - val_loss: 0.6855 - val_accuracy: 0.7970\n",
      "Epoch 30/30\n",
      "168/169 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.7860\n",
      "Epoch 30: val_loss improved from 0.68551 to 0.68517, saving model to model_sgd_fold10.hdf5\n",
      "169/169 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.7863 - val_loss: 0.6852 - val_accuracy: 0.8053\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "Fold 10 - TP: 548, TN: 416, FP: 177, FN: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/3282993093.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    }
   ],
   "source": [
    "for optimizer_name, optimizer in optimizers:\n",
    "    optimizer_histories = []\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_reshaped, y_array)):\n",
    "        X_train, X_val = X_reshaped[train_index], X_reshaped[val_index]\n",
    "        y_train, y_val = y_array[train_index], y_array[val_index]\n",
    "\n",
    "        model = create_model(input_shape)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Define the checkpoint for each fold\n",
    "        filepath = f\"model_{optimizer_name}_fold{fold+1}.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint]\n",
    "\n",
    "        start_time = time.time()\n",
    "        history = model.fit(X_train, y_train, epochs=30, batch_size=64, \n",
    "                            validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        # Evaluate the model on the training data\n",
    "        _, training_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "        training_loss = history.history['loss'][-1]\n",
    "\n",
    "        # Evaluate the model on the validation data\n",
    "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        precision = precision_score(y_val, y_pred, average='macro')\n",
    "        recall = recall_score(y_val, y_pred, average='macro')\n",
    "        f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "        # Calculate specificity\n",
    "        specificity = specificity_score(y_val, y_pred)\n",
    "\n",
    "        \n",
    "        # Calculate specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "        # Print TP, TN, FP, FN for the current fold\n",
    "        print(f\"Fold {fold+1} - TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "        # Update average values\n",
    "        avg_tp += tp\n",
    "        avg_tn += tn\n",
    "        avg_fp += fp\n",
    "        avg_fn += fn\n",
    "\n",
    "        \n",
    "        \n",
    "        # Save the performance metrics to the DataFrame\n",
    "        performance_df = performance_df.append({\n",
    "            'training_accuracy': training_accuracy,\n",
    "            'training_loss': training_loss,\n",
    "            'optimizer': optimizer_name,\n",
    "            'fold': fold+1,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'tp': tp,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'training_time': training_time\n",
    "        }, ignore_index=True)\n",
    "        optimizer_histories.append(history)\n",
    "        \n",
    "    histories.append(optimizer_histories)\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9715c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for (optimizer_name, _), optimizer_histories, model in zip(optimizers, histories, models):\n",
    "    best_fold_index = None\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    # Find the best fold based on val_accuracy\n",
    "    for fold, history in enumerate(optimizer_histories):\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_fold_index = fold\n",
    "            best_val_accuracy = val_accuracy\n",
    "\n",
    "    # Plot the best fold for the optimizer\n",
    "    best_history = optimizer_histories[best_fold_index]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{optimizer_name} - Fold {best_fold_index+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training loss for the best fold\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{optimizer_name} - Fold {best_fold_index+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "        \n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f'{optimizer_name}_fold{best_fold_index+1}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cdea5",
   "metadata": {},
   "source": [
    "# **Evaluasi Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80abb450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The best model is model_radam_fold1.hdf5 with a validation accuracy of 0.9273789525032043.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## masi perlu ditandai yang terbaik\n",
    "\n",
    "##BERDASARKAN VAL ACCCURACY\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adamax, Adam\n",
    "\n",
    "# Define a dictionary of custom_objects for non-radam optimizers\n",
    "custom_optimizers = {'adamax': Adamax, 'adam': Adam, 'sgd': SGD}\n",
    "\n",
    "# Load the best model with the appropriate optimizer\n",
    "best_model_info = performance_df.loc[performance_df['val_accuracy'].idxmax()]\n",
    "\n",
    "if best_model_info['optimizer'] == 'radam':\n",
    "    best_model = load_model(f\"model_radam_fold{best_model_info['fold']}.hdf5\", custom_objects={'RAdam': RAdam})\n",
    "else:\n",
    "    best_model = load_model(f\"model_{best_model_info['optimizer']}_fold{best_model_info['fold']}.hdf5\", \n",
    "                            custom_objects={best_model_info['optimizer']: custom_optimizers[best_model_info['optimizer']]})\n",
    "\n",
    "print(f\"The best model is model_{best_model_info['optimizer']}_fold{best_model_info['fold']}.hdf5 with a validation accuracy of {best_model_info['val_accuracy']}.\")\n",
    "\n",
    "# Save the performance metrics to a CSV file\n",
    "performance_df.to_csv('performance_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b232c7",
   "metadata": {},
   "source": [
    "# **SEMUA METRICS**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd8c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The model with the best recall is model_radam_fold1.hdf5 with a recall of 0.9289464037363198.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The model with the best precision is model_radam_fold1.hdf5 with a precision of 0.9272308189703227.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The model with the best f1_score is model_radam_fold1.hdf5 with a f1_score of 0.9272938069357799.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The model with the best specificity is model_radam_fold4.hdf5 with a specificity of 0.9642276422764228.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "The model with the best training_time is model_sgd_fold10.hdf5 with a training_time of 29.64009404182434.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Define the evaluation metrics you are interested in\n",
    "metrics = ['recall', 'precision', 'f1_score', 'specificity','training_time']\n",
    "custom_optimizers = {'adamax': Adamax, 'adam': Adam, 'sgd': SGD}\n",
    "\n",
    "# For each metric, find the model with the best performance and save it\n",
    "for metric in metrics:\n",
    "    if metric == 'training_time':\n",
    "        best_model_info = performance_df.loc[performance_df[metric].idxmin()]\n",
    "    else:\n",
    "        best_model_info = performance_df.loc[performance_df[metric].idxmax()]\n",
    "    if best_model_info['optimizer'] == 'radam':\n",
    "        best_model = load_model(f\"model_radam_fold{best_model_info['fold']}.hdf5\", custom_objects={'RAdam': RAdam})\n",
    "    else:\n",
    "        best_model = load_model(f\"model_{best_model_info['optimizer']}_fold{best_model_info['fold']}.hdf5\", \n",
    "                                custom_objects={best_model_info['optimizer']: custom_optimizers[best_model_info['optimizer']]})\n",
    "\n",
    "    print(f\"The model with the best {metric} is model_{best_model_info['optimizer']}_fold{best_model_info['fold']}.hdf5 with a {metric} of {best_model_info[metric]}.\")\n",
    "\n",
    "    # Save the model with the best performance in a separate directory\n",
    "    os.makedirs(f'best_{metric}_model', exist_ok=True)\n",
    "    best_model.save(f\"best_{metric}_model/model_{best_model_info['optimizer']}_fold{best_model_info['fold']}.hdf5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397afec",
   "metadata": {},
   "source": [
    "## **Grafik PLOT**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21ab72dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "AUC for adam: 0.9759598740270038\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "AUC for adamax: 0.9608274795349723\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "AUC for radam: 0.9702168232022604\n",
      "38/38 [==============================] - 1s 2ms/step\n",
      "AUC for sgd: 0.9095253118613403\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "# Set the number of optimizers\n",
    "n_classes = len(optimizers)\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define colors for each optimizer\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
    "\n",
    "# Group by optimizer and get the fold with the best validation accuracy\n",
    "best_folds = performance_df.sort_values('f1_score', ascending=False).groupby('optimizer').first()\n",
    "\n",
    "\n",
    "# For each optimizer, find the best fold and plot the ROC curve\n",
    "for optimizer, color in zip(best_folds.index, colors):\n",
    "    best_fold = best_folds.loc[optimizer]\n",
    "    \n",
    "    if optimizer == 'radam':\n",
    "        model = load_model(f\"dengan feature selection/model_radam_fold{int(best_fold['fold'])}.hdf5\", custom_objects={'RAdam': RAdam})\n",
    "    else:\n",
    "        model = load_model(f\"dengan feature selection/model_{optimizer}_fold{int(best_fold['fold'])}.hdf5\")\n",
    "    \n",
    "    # get the ground truth and prediction\n",
    "    y_test = y_array[val_index]\n",
    "    y_score = model.predict(X_reshaped[val_index])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    label = f'ROC curve of Optimizer {optimizer} (AUC = {roc_auc:0.2f})'\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=color, lw=2, label=label)\n",
    "\n",
    "    print(f\"AUC for {optimizer}: {roc_auc}\")\n",
    "\n",
    "# Define the random and perfect classifiers\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Each Optimizer')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# # Save the figure\n",
    "# fig.savefig('training.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5cbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614262c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
